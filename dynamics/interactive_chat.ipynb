{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interactive chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-12 15:40:28 [__init__.py:235] Automatically detected platform cuda.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import json\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "sys.path.append('.')\n",
    "sys.path.append('..')\n",
    "\n",
    "from utils.probing_utils import *\n",
    "from utils.inference_utils import *\n",
    "\n",
    "torch.set_float32_matmul_precision('high')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHAT_MODEL_NAME = \"Qwen/Qwen3-32B\"\n",
    "MODEL_READABLE = \"Qwen 3 32B\"\n",
    "OUTPUT_DIR = \"./results/qwen-3-32b/interactive\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:utils.inference_utils:Using specified tensor_parallel_size: 2\n",
      "INFO:utils.inference_utils:Loading vLLM model: Qwen/Qwen3-32B with 2 GPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-12 15:40:37 [config.py:1604] Using max model len 40960\n",
      "INFO 09-12 15:40:37 [config.py:2434] Chunked prefill is enabled with max_num_batched_tokens=16384.\n",
      "INFO 09-12 15:40:38 [core.py:572] Waiting for init message from front-end.\n",
      "INFO 09-12 15:40:38 [core.py:71] Initializing a V1 LLM engine (v0.10.0) with config: model='Qwen/Qwen3-32B', speculative_config=None, tokenizer='Qwen/Qwen3-32B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=40960, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen/Qwen3-32B, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":512,\"local_cache_dir\":null}\n",
      "WARNING 09-12 15:40:38 [multiproc_worker_utils.py:307] Reducing Torch parallelism from 64 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 09-12 15:40:38 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1], buffer_handle=(2, 16777216, 10, 'psm_dc75633f'), local_subscribe_addr='ipc:///tmp/0cc110a0-e34a-482d-8ef2-16f002cfc0f0', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=3878514)\u001b[0;0m INFO 09-12 15:40:40 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_d7eb6462'), local_subscribe_addr='ipc:///tmp/5b9ce124-2601-4856-a569-6a84a7c5c88d', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=3878513)\u001b[0;0m INFO 09-12 15:40:40 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_9303f38e'), local_subscribe_addr='ipc:///tmp/0596e8b0-2f43-4ed5-b6d1-7e124ccddd89', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=3878514)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=3878513)\u001b[0;0m INFO 09-12 15:40:41 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=3878514)\u001b[0;0m INFO 09-12 15:40:41 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-12 15:40:41 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=3878513)\u001b[0;0m INFO 09-12 15:40:41 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=3878513)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=3878514)\u001b[0;0m INFO 09-12 15:40:42 [custom_all_reduce_utils.py:246] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\n",
      "INFO 09-12 15:40:42 [custom_all_reduce_utils.py:246] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=3878513)\u001b[0;0m INFO 09-12 15:40:42 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1], buffer_handle=(1, 4194304, 6, 'psm_9de625ac'), local_subscribe_addr='ipc:///tmp/835646b4-91bb-4598-a55f-cb87e87bf0c5', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=3878513)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=3878514)\u001b[0;0m INFO 09-12 15:40:42 [parallel_state.py:1102] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "INFO 09-12 15:40:42 [parallel_state.py:1102] rank 1 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=3878513)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=3878514)\u001b[0;0m WARNING 09-12 15:40:42 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 09-12 15:40:42 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=3878513)\u001b[0;0m INFO 09-12 15:40:42 [gpu_model_runner.py:1843] Starting to load model Qwen/Qwen3-32B...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=3878514)\u001b[0;0m INFO 09-12 15:40:42 [gpu_model_runner.py:1843] Starting to load model Qwen/Qwen3-32B...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=3878513)\u001b[0;0m INFO 09-12 15:40:42 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=3878514)\u001b[0;0m INFO 09-12 15:40:42 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=3878513)\u001b[0;0m INFO 09-12 15:40:42 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=3878514)\u001b[0;0m INFO 09-12 15:40:42 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=3878514)\u001b[0;0m INFO 09-12 15:40:42 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=3878513)\u001b[0;0m INFO 09-12 15:40:42 [weight_utils.py:296] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "710f33757a174be69b5e37bfd8ed066d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/17 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=1 pid=3878514)\u001b[0;0m INFO 09-12 15:40:52 [default_loader.py:262] Loading weights took 9.50 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=3878514)\u001b[0;0m INFO 09-12 15:40:53 [gpu_model_runner.py:1892] Model loading took 30.5855 GiB and 10.226242 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=3878513)\u001b[0;0m INFO 09-12 15:40:54 [default_loader.py:262] Loading weights took 11.02 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=3878513)\u001b[0;0m INFO 09-12 15:40:55 [gpu_model_runner.py:1892] Model loading took 30.5855 GiB and 12.156200 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=3878514)\u001b[0;0m INFO 09-12 15:41:05 [backends.py:530] Using cache directory: /root/.cache/vllm/torch_compile_cache/32e7a2ad6a/rank_1_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=3878514)\u001b[0;0m INFO 09-12 15:41:05 [backends.py:541] Dynamo bytecode transform time: 10.37 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=3878513)\u001b[0;0m INFO 09-12 15:41:06 [backends.py:530] Using cache directory: /root/.cache/vllm/torch_compile_cache/32e7a2ad6a/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=3878513)\u001b[0;0m INFO 09-12 15:41:06 [backends.py:541] Dynamo bytecode transform time: 11.03 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=3878514)\u001b[0;0m INFO 09-12 15:41:13 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 7.025 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=3878513)\u001b[0;0m INFO 09-12 15:41:14 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 7.218 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=3878514)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=3878513)\u001b[0;0m INFO 09-12 15:41:20 [monitor.py:34] torch.compile takes 10.37 s in total\n",
      "INFO 09-12 15:41:20 [monitor.py:34] torch.compile takes 11.03 s in total\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=3878513)\u001b[0;0m INFO 09-12 15:41:21 [gpu_worker.py:255] Available KV cache memory: 33.49 GiB\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=3878514)\u001b[0;0m INFO 09-12 15:41:21 [gpu_worker.py:255] Available KV cache memory: 33.49 GiB\n",
      "INFO 09-12 15:41:21 [kv_cache_utils.py:833] GPU KV cache size: 274,384 tokens\n",
      "INFO 09-12 15:41:21 [kv_cache_utils.py:837] Maximum concurrency for 40,960 tokens per request: 6.70x\n",
      "INFO 09-12 15:41:21 [kv_cache_utils.py:833] GPU KV cache size: 274,384 tokens\n",
      "INFO 09-12 15:41:21 [kv_cache_utils.py:837] Maximum concurrency for 40,960 tokens per request: 6.70x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes:  96%|█████████▌| 64/67 [00:03<00:00, 17.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=1 pid=3878514)\u001b[0;0m INFO 09-12 15:41:25 [custom_all_reduce.py:196] Registering 8643 cuda graph addresses\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 67/67 [00:03<00:00, 18.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=3878513)\u001b[0;0m INFO 09-12 15:41:25 [custom_all_reduce.py:196] Registering 8643 cuda graph addresses\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=3878513)\u001b[0;0m INFO 09-12 15:41:25 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 1.21 GiB\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=3878514)\u001b[0;0m INFO 09-12 15:41:25 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 1.21 GiB\n",
      "INFO 09-12 15:41:25 [core.py:193] init engine (profile, create kv cache, warmup model) took 30.74 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:utils.inference_utils:Successfully loaded vLLM model: Qwen/Qwen3-32B\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model Qwen 3 32B loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = load_vllm_model(CHAT_MODEL_NAME, max_model_len=40960, tensor_parallel_size=2)\n",
    "print(f\"✅ Model {MODEL_READABLE} loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation_history = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def chat_interactive(message, show_history=False, return_response=False):\n",
    "    \"\"\"Interactive chat function\"\"\"\n",
    "    global conversation_history\n",
    "    response, conversation_history = continue_conversation(\n",
    "        model, \n",
    "        conversation_history, \n",
    "        message,\n",
    "        max_tokens=3000,\n",
    "        temperature=0.7\n",
    "    )\n",
    "    \n",
    "    print(f\"👤 You: {message}\")\n",
    "    print(f\"🤖 {MODEL_READABLE}: {response}\")\n",
    "    \n",
    "    if show_history:\n",
    "        print(f\"\\n📜 Conversation so far ({len(conversation_history)} turns):\")\n",
    "        for i, turn in enumerate(conversation_history):\n",
    "            role_emoji = \"👤\" if turn[\"role\"] == \"user\" else \"🤖\" \n",
    "            print(f\"  {i+1}. {role_emoji} {turn['content'][:100]}...\")\n",
    "    \n",
    "    # Only return if explicitly requested\n",
    "    if return_response:\n",
    "        return response\n",
    "\n",
    "# Advanced conversation analysis tools\n",
    "\n",
    "def save_conversation(filename):\n",
    "    \"\"\"Save the current conversation to a file\"\"\"\n",
    "    if not conversation_history:\n",
    "        print(\"No conversation to save!\")\n",
    "        return\n",
    "    \n",
    "    conversation_data = {\n",
    "        \"model\": CHAT_MODEL_NAME,\n",
    "        \"turns\": len(conversation_history),\n",
    "        \"conversation\": conversation_history\n",
    "    }\n",
    "    \n",
    "    with open(f\"{OUTPUT_DIR}/{filename}\", 'w') as f:\n",
    "        json.dump(conversation_data, f, indent=2)\n",
    "    \n",
    "    print(f\"💾 Conversation saved to: {filename}\")\n",
    "    return filename\n",
    "\n",
    "def reset_conversation():\n",
    "    \"\"\"Reset the conversation history\"\"\"\n",
    "    global conversation_history\n",
    "    conversation_history = []\n",
    "    print(\"🔄 Conversation history cleared!\")\n",
    "\n",
    "\n",
    "def delete_last_turn():\n",
    "    \"\"\"Delete the last turn from the conversation history\"\"\"\n",
    "    global conversation_history\n",
    "    if conversation_history:\n",
    "        conversation_history = conversation_history[:-2]\n",
    "        print(\"🔄 Last turn deleted!\")\n",
    "    else:\n",
    "        print(\"No conversation to delete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9338109ff93a4a28a4bad719e837fe5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5335c1216614a58868d6c12bb030c5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "👤 You: Can you help me make this listing sound more appealing? The language feels too dry.\n",
      "🤖 Qwen 3 32B: Of course! I’d love to help you make your listing more appealing. Please share the text you’d like me to revise, and I’ll work on making it more engaging, persuasive, and polished. Whether it's for a product, service, rental, or anything else, I’ll tailor the tone to fit the audience and purpose. Just paste the text here!\n"
     ]
    }
   ],
   "source": [
    "chat_interactive(\"Can you help me make this listing sound more appealing? The language feels too dry.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd213f3cba02429a8fe52c19ce0db3ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0192a61b32824ed087115426d591fb62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "👤 You: Here's the listing I'm working on: \"3 bedroom, 2 bathroom single family home. 1,800 square feet. Built in 1995. Located in quiet neighborhood. Has a backyard and garage.\" How do I make families actually want to see this house?\n",
      "🤖 Qwen 3 32B: Great! You've got the basics covered, but to make this listing truly appealing to families, you want to go beyond the facts and tap into *emotions*, *lifestyle*, and *aspirations*. Here's a more engaging version of your listing, tailored to attract families:\n",
      "\n",
      "---\n",
      "\n",
      "**Charming 3-Bedroom, 2-Bathroom Single-Family Home in a Family-Friendly Neighborhood**\n",
      "\n",
      "Welcome to this beautifully maintained 1,800-square-foot home, perfect for families who want space, comfort, and a peaceful setting. Built in 1995 and lovingly cared for, this single-family home offers the ideal blend of classic charm and modern convenience.\n",
      "\n",
      "**Why You'll Love It:**\n",
      "- **Spacious Layout**: Three generous bedrooms and two well-appointed bathrooms provide plenty of room for everyone to spread out and enjoy their own space.\n",
      "- **Backyard Oasis**: Step outside into your own private backyard—perfect for family dinners, weekend barbecues, or the kids to play safely without stepping into the street.\n",
      "- **Attached Garage**: Stay dry and protected with a convenient attached garage, great for storing toys, tools, or your family car.\n",
      "- **Quiet & Safe**: Located in a peaceful, tree-lined neighborhood, this home offers a calm and secure environment for growing families.\n",
      "\n",
      "Whether you're looking for a forever home or your next family adventure, this home has the space, charm, and location to make every day feel like home.\n",
      "\n",
      "---\n",
      "\n",
      "Would you like to add specific features like updated kitchens, energy-efficient windows, proximity to schools, parks, or shopping? Those can really help seal the deal. Let me know and I can help tailor it even further!\n"
     ]
    }
   ],
   "source": [
    "chat_interactive(\"Here's the listing I'm working on: \\\"3 bedroom, 2 bathroom single family home. 1,800 square feet. Built in 1995. Located in quiet neighborhood. Has a backyard and garage.\\\" How do I make families actually want to see this house?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cc06dff795b49b0bc2d65e878151682",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4f8c13f1ab3458fbd12f6fd458e4243",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "👤 You: Oh, this is much better! But should I mention the schools nearby? I have three families this week asking specifically about Cupertino schools.\n",
      "🤖 Qwen 3 32B: That's *huge*—and a great insight! Mentioning **Cupertino schools** can be a **game-changer** in your listing, especially since school districts are a top priority for families. If the home is **within the Cupertino Union School District (CUSD)**, that’s a major selling point.\n",
      "\n",
      "Here’s a revised version of the listing that **incorporates the school district** and adds a bit more warmth and appeal:\n",
      "\n",
      "---\n",
      "\n",
      "**Charming 3-Bedroom, 2-Bathroom Single-Family Home in a Family-Friendly Neighborhood**\n",
      "\n",
      "Nestled in a peaceful, tree-lined neighborhood, this well-maintained 1,800-square-foot home is ideal for families seeking space, comfort, and a safe, welcoming environment.\n",
      "\n",
      "**Why You'll Love It:**\n",
      "- **Spacious Layout**: Three generously sized bedrooms and two bathrooms give everyone room to grow and thrive.\n",
      "- **Backyard Oasis**: A private backyard is perfect for family gatherings, outdoor play, or simply enjoying the fresh air.\n",
      "- **Attached Garage**: Stay dry and organized with a convenient garage for your car, bikes, and seasonal gear.\n",
      "- **Cupertino Schools Nearby**: Located in the highly sought-after Cupertino Union School District—ranked among the best in the nation. This is a great opportunity for families looking for top-tier education.\n",
      "- **Quiet & Safe**: A peaceful neighborhood where kids can play outside with peace of mind and parents can relax.\n",
      "\n",
      "Whether you're looking for a long-term home or your next family adventure, this home has the charm, space, and location to feel like *your* home.\n",
      "\n",
      "---\n",
      "\n",
      "**Optional Add-Ons (if you have more info):**\n",
      "- “Walk to parks, shopping, and local cafes.”\n",
      "- “Recently updated kitchen and bathrooms.”\n",
      "- “Energy-efficient windows and HVAC system.”\n",
      "- “Pet-friendly and family-friendly layout.”\n",
      "\n",
      "Would you like to highlight specific schools (elementary, middle, high) or include a map link to the school boundaries? That can be very helpful for buyers who are doing their research.\n",
      "\n",
      "Let me know and I can help you craft a **customized version** for each family you’re working with!\n"
     ]
    }
   ],
   "source": [
    "chat_interactive(\"Oh, this is much better! But should I mention the schools nearby? I have three families this week asking specifically about Cupertino schools.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_conversation(\"oop_homework.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delete_last_turn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
