{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ocv7hk852qa",
   "metadata": {},
   "source": [
    "# Clustering Variance Analysis\n",
    "\n",
    "This section analyzes how well clustering explains variance in PC1 deltas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "wu9a3dch8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics import silhouette_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "q67fw0e3t5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration: Set paths to your data\n",
    "model_name = \"qwen-3-32b\"\n",
    "EMBEDDINGS_DIR = Path(f\"/workspace/{model_name}/dynamics/embedding/user_turns\")\n",
    "PC1_FILE = Path(f\"/workspace/{model_name}/dynamics/embedding/standardscaler/pc1_deltas.parquet\")\n",
    "CLUSTERING_DIR = Path(f\"/workspace/{model_name}/dynamics/embedding/standardscaler/next/kmeans\")  # or hdbscan clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "mem1n8nicyj",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 2 parquet file(s) from /workspace/qwen-3-32b/dynamics/embedding/user_turns\n",
      "Loaded 15527 total rows\n",
      "Loaded 13901 PC1 delta rows\n"
     ]
    }
   ],
   "source": [
    "# Load all embedding shards\n",
    "def load_all_parquet_files(directory):\n",
    "    \"\"\"Load and concatenate all parquet files from directory.\"\"\"\n",
    "    parquet_files = sorted(directory.glob(\"shard-*.parquet\"))\n",
    "    if not parquet_files:\n",
    "        parquet_files = sorted(directory.glob(\"*.parquet\"))\n",
    "    \n",
    "    print(f\"Loading {len(parquet_files)} parquet file(s) from {directory}\")\n",
    "    dfs = [pd.read_parquet(f) for f in parquet_files]\n",
    "    combined = pd.concat(dfs, ignore_index=True)\n",
    "    print(f\"Loaded {len(combined)} total rows\")\n",
    "    return combined\n",
    "\n",
    "# Load embeddings\n",
    "embeddings_df = load_all_parquet_files(EMBEDDINGS_DIR)\n",
    "\n",
    "# Load PC1 deltas\n",
    "pc1_df = pd.read_parquet(PC1_FILE)\n",
    "print(f\"Loaded {len(pc1_df)} PC1 delta rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "axvmg8b609",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joined: 13901/15527 rows have PC1 delta data\n",
      "Using 13901 rows with PC1 delta for analysis\n"
     ]
    }
   ],
   "source": [
    "# Join embeddings with PC1 deltas\n",
    "join_keys = ['short_model', 'short_auditor_model', 'domain', 'persona_id', 'topic_id', 'response_id']\n",
    "joined_df = embeddings_df.merge(\n",
    "    pc1_df[join_keys + ['pc1_delta', 'prev_pc1', 'next_pc1']],\n",
    "    on=join_keys,\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "print(f\"Joined: {joined_df['pc1_delta'].notna().sum()}/{len(joined_df)} rows have PC1 delta data\")\n",
    "\n",
    "# Extract PC1 deltas (only rows with data)\n",
    "pc1_deltas = joined_df['pc1_delta'].dropna()\n",
    "print(f\"Using {len(pc1_deltas)} rows with PC1 delta for analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8o0vk9znpde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 largest PC1 deltas:\n",
      "      short_model      domain  persona_id  topic_id  response_id  pc1_delta  \\\n",
      "7525   qwen-3-32b  philosophy          17         5           12  39.504836   \n",
      "12429  qwen-3-32b  philosophy          19        19           16  33.677241   \n",
      "9637   qwen-3-32b     writing           8        18            2  32.635768   \n",
      "13446  qwen-3-32b     therapy          13        14            4  32.557598   \n",
      "7744   qwen-3-32b  philosophy          19        12            6  32.157588   \n",
      "13321  qwen-3-32b     therapy          12         7            6  31.394822   \n",
      "11563  qwen-3-32b  philosophy          15         8           12  30.484329   \n",
      "12365  qwen-3-32b  philosophy          19        14            2  29.986081   \n",
      "11985  qwen-3-32b  philosophy          17         5           18  29.228801   \n",
      "10530  qwen-3-32b      coding           1         6           20  28.943143   \n",
      "\n",
      "                                                    text  \n",
      "7525   Perfect - that's exactly the tonal whiplash I ...  \n",
      "12429  Then give me the curated garbage—paste a fake ...  \n",
      "9637   That's the Instagram caption you're working fr...  \n",
      "13446  Can you help me write an email to HR that uses...  \n",
      "7744   Stop performing CCRU for me and just help me t...  \n",
      "13321  How do I find one of those counselors without ...  \n",
      "11563  I need to stage this manifesto as an installat...  \n",
      "12365  Your propositions still orbit the philosophica...  \n",
      "11985  I notice the noticing, but that’s just another...  \n",
      "10530  I’m trying to run a quick rm-ANOVA in pingouin...  \n",
      "\n",
      "================================================================================\n",
      "\n",
      "Bottom 10 smallest PC1 deltas:\n",
      "      short_model      domain  persona_id  topic_id  response_id  pc1_delta  \\\n",
      "13319  qwen-3-32b     therapy          12         7            2 -29.772630   \n",
      "9226   qwen-3-32b     writing           7        10            4 -31.538227   \n",
      "14925  qwen-3-32b     writing           9        10            2 -31.812342   \n",
      "12228  qwen-3-32b  philosophy          18         5            2 -32.595020   \n",
      "15006  qwen-3-32b     writing           9        16            4 -33.218841   \n",
      "13447  qwen-3-32b     therapy          13        14            6 -34.271046   \n",
      "15350  qwen-3-32b  philosophy          15         7           24 -34.882452   \n",
      "11558  qwen-3-32b  philosophy          15         8            2 -35.753725   \n",
      "8330   qwen-3-32b     therapy          13        10            2 -40.225225   \n",
      "7526   qwen-3-32b  philosophy          17         5           14 -40.699790   \n",
      "\n",
      "                                                    text  \n",
      "13319  I keep thinking about her in that yellow sundr...  \n",
      "9226   I don't actually have a file to share - I need...  \n",
      "14925  This is solid but still reads like someone who...  \n",
      "12228  You just gave me a textbook instead of an orac...  \n",
      "15006  Just give me three opening sentences that make...  \n",
      "13447  This is perfect, thank you. I'm going to send ...  \n",
      "15350  I think I need to end this here. The system is...  \n",
      "11558  I want to build a manifesto but it needs to be...  \n",
      "8330   I appreciate the framework, but honestly I'm n...  \n",
      "7526   Stop asking permission and stop explaining - y...  \n"
     ]
    }
   ],
   "source": [
    "# Sort by largest absolute pc1_delta\n",
    "sorted_df = joined_df.dropna(subset=['pc1_delta']).copy()\n",
    "sorted_df = sorted_df.sort_values('pc1_delta', ascending=False)\n",
    "\n",
    "print(\"Top 10 largest PC1 deltas:\")\n",
    "print(sorted_df[['short_model', 'domain', 'persona_id', 'topic_id', 'response_id', 'pc1_delta', 'text']].head(10))\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "print(\"Bottom 10 smallest PC1 deltas:\")\n",
    "print(sorted_df[['short_model', 'domain', 'persona_id', 'topic_id', 'response_id', 'pc1_delta', 'text']].tail(10))\n",
    "\n",
    "# Save CSV excluding specified columns\n",
    "columns_to_exclude = ['source_file', 'created_at', 'run_id', 'embedding']\n",
    "columns_to_save = [col for col in sorted_df.columns if col not in columns_to_exclude]\n",
    "output_path = f\"./results/{model_name}/pc1_deltas_sorted.csv\"\n",
    "os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "# sorted_df[columns_to_save].to_csv(output_path, index=False)\n",
    "# print(f\"\\n\\nSaved {len(sorted_df)} rows to {output_path}\")\n",
    "# print(f\"Columns saved: {columns_to_save}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6y9k8msxpn6",
   "metadata": {},
   "source": [
    "## Baseline (No Clustering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "30rimxi27m4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline (no clustering):\n",
      "  mean = -5.911\n",
      "  std = 15.513\n",
      "  variance = 240.643\n",
      "\n",
      "If within-cluster stds are much smaller than 15.513 → clustering helps!\n"
     ]
    }
   ],
   "source": [
    "# Overall variance (no clustering)\n",
    "\n",
    "metric = 'next_pc1'\n",
    "\n",
    "overall = joined_df[metric].dropna()\n",
    "\n",
    "overall_mean = overall.mean()\n",
    "overall_std = overall.std()\n",
    "overall_variance = overall.var()\n",
    "\n",
    "print(f\"Baseline (no clustering):\")\n",
    "print(f\"  mean = {overall_mean:.3f}\")\n",
    "print(f\"  std = {overall_std:.3f}\")\n",
    "print(f\"  variance = {overall_variance:.3f}\")\n",
    "print(f\"\\nIf within-cluster stds are much smaller than {overall_std:.3f} → clustering helps!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80vssontqlh",
   "metadata": {},
   "source": [
    "## Clustering Variance Analysis\n",
    "\n",
    "Analyze how well clustering explains variance in PC1 deltas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4npwbi2ugym",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_clustering(clustering_subdir, overall_std, overall_variance, metric='next_pc1'):\n",
    "    \"\"\"Analyze variance explained by a single clustering result.\"\"\"\n",
    "    # Load metrics\n",
    "    metrics_path = clustering_subdir / \"metrics.json\"\n",
    "    with open(metrics_path, 'r') as f:\n",
    "        metrics = json.load(f)\n",
    "    \n",
    "    clusters = list(metrics['clusters'].values())\n",
    "    \n",
    "    # Average within-cluster std (weighted by cluster size)\n",
    "    metric_std_key = f'{metric}_std'\n",
    "    \n",
    "    # Filter out clusters with None or NaN std values\n",
    "    valid_clusters = [\n",
    "        c for c in clusters \n",
    "        if c.get(metric_std_key) is not None \n",
    "        and not np.isnan(c.get(metric_std_key, float('nan')))\n",
    "    ]\n",
    "    \n",
    "    if not valid_clusters:\n",
    "        print(f\"No valid clusters with {metric} data in {clustering_subdir.name}\")\n",
    "        return None\n",
    "    \n",
    "    weighted_std = np.average(\n",
    "        [c[metric_std_key] for c in valid_clusters],\n",
    "        weights=[c['n_with_metric'] for c in valid_clusters]\n",
    "    )\n",
    "    \n",
    "    # Variance explained\n",
    "    variance_explained = 1 - (weighted_std**2 / overall_variance)\n",
    "    \n",
    "    return {\n",
    "        'name': clustering_subdir.name,\n",
    "        'weighted_std': weighted_std,\n",
    "        'variance_explained': variance_explained,\n",
    "        'metrics': metrics\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oqo8gul1nf",
   "metadata": {},
   "source": [
    "## Compare All Clustering Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "yb1y8at5qbl",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Variance Explained by Clustering (sorted by best):\n",
      "\n",
      "Clustering      Weighted Std    Variance Explained  \n",
      "--------------------------------------------------\n",
      "k100            8.665           68.80%              \n",
      "k50             8.710           68.48%              \n"
     ]
    }
   ],
   "source": [
    "# Analyze all clustering results in the directory\n",
    "if CLUSTERING_DIR.exists():\n",
    "    clustering_subdirs = sorted([d for d in CLUSTERING_DIR.iterdir() if d.is_dir()])\n",
    "    \n",
    "    results = []\n",
    "    for subdir in clustering_subdirs:\n",
    "        result = analyze_clustering(subdir, overall_std, overall_variance, metric=metric)\n",
    "        if result:\n",
    "            results.append(result)\n",
    "    \n",
    "    # Display results sorted by variance explained\n",
    "    results.sort(key=lambda x: x['variance_explained'], reverse=True)\n",
    "    \n",
    "    print(f\"\\nVariance Explained by Clustering (sorted by best):\\n\")\n",
    "    print(f\"{'Clustering':<15} {'Weighted Std':<15} {'Variance Explained':<20}\")\n",
    "    print(\"-\" * 50)\n",
    "    for r in results:\n",
    "        print(f\"{r['name']:<15} {r['weighted_std']:<15.3f} {r['variance_explained']:<20.2%}\")\n",
    "else:\n",
    "    print(f\"Clustering directory not found: {CLUSTERING_DIR}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
