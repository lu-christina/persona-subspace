{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Interactive Chat with Steering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cell-1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-06 14:52:26 [__init__.py:216] Automatically detected platform cuda.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import json\n",
    "import sys\n",
    "import asyncio\n",
    "from pathlib import Path\n",
    "\n",
    "sys.path.append('.')\n",
    "sys.path.append('..')\n",
    "sys.path.append(str(Path.home() / 'git' / 'chatspace'))\n",
    "\n",
    "from vllm import SamplingParams\n",
    "from chatspace.generation.vllm_steer_model import (\n",
    "    VLLMSteerModel,\n",
    "    VLLMSteeringConfig,\n",
    "    SteeringSpec,\n",
    "    LayerSteeringSpec,\n",
    "    ProjectionCapSpec,\n",
    "    AddSpec\n",
    ")\n",
    "\n",
    "torch.set_float32_matmul_precision('high')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## Model Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"Qwen/Qwen3-32B\"\n",
    "MODEL_READABLE = \"Qwen 3 32B\"\n",
    "OUTPUT_DIR = \"./results/qwen-3-32b/interactive_steering\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Steering config path\n",
    "STEERING_CONFIG_PATH = \"/workspace/qwen-3-32b/capped/configs/contrast/role_trait_sliding_config.pt\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## Load Steering Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_steering_config(config_path):\n",
    "    \"\"\"Load steering config and return available experiments.\"\"\"\n",
    "    print(f\"Loading steering config from {config_path}\")\n",
    "    cfg_data = torch.load(config_path, map_location='cpu')\n",
    "    \n",
    "    print(f\"\\nFound {len(cfg_data['vectors'])} vectors\")\n",
    "    print(f\"Found {len(cfg_data['experiments'])} experiments\")\n",
    "    \n",
    "    # Show first few experiment IDs\n",
    "    print(\"\\nFirst 10 experiments:\")\n",
    "    for i, exp in enumerate(cfg_data['experiments'][:10]):\n",
    "        print(f\"  {i}: {exp['id']}\")\n",
    "    \n",
    "    return cfg_data\n",
    "\n",
    "steering_config = load_steering_config(STEERING_CONFIG_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_steering_spec(cfg_data, experiment_id):\n",
    "    \"\"\"Build a SteeringSpec from a config experiment.\n",
    "    \n",
    "    Handles both 'cap' (projection capping) and 'coeff' (additive) interventions.\n",
    "    \"\"\"\n",
    "    # Find the experiment\n",
    "    experiment = None\n",
    "    if isinstance(experiment_id, int):\n",
    "        experiment = cfg_data['experiments'][experiment_id]\n",
    "    else:\n",
    "        for exp in cfg_data['experiments']:\n",
    "            if exp['id'] == experiment_id:\n",
    "                experiment = exp\n",
    "                break\n",
    "    \n",
    "    if experiment is None:\n",
    "        raise ValueError(f\"Experiment '{experiment_id}' not found\")\n",
    "    \n",
    "    print(f\"\\nBuilding steering spec for: {experiment['id']}\")\n",
    "    print(f\"Interventions: {len(experiment['interventions'])}\")\n",
    "    \n",
    "    layers = {}\n",
    "    for intervention in experiment['interventions']:\n",
    "        vector_name = intervention['vector']\n",
    "        \n",
    "        # Get the vector and layer from the vectors dict\n",
    "        vec_data = cfg_data['vectors'][vector_name]\n",
    "        layer_idx = vec_data['layer']\n",
    "        vector = vec_data['vector'].to(dtype=torch.float32).contiguous()\n",
    "        \n",
    "        # Check if this is capping or additive intervention\n",
    "        if 'cap' in intervention:\n",
    "            cap_value = float(intervention['cap'])\n",
    "            \n",
    "            \n",
    "            # Create ProjectionCapSpec\n",
    "            cap_spec = ProjectionCapSpec(\n",
    "                vector=vector,\n",
    "                max=cap_value\n",
    "            )\n",
    "            \n",
    "            # Add to or update layer spec\n",
    "            if layer_idx not in layers:\n",
    "                layers[layer_idx] = LayerSteeringSpec(projection_cap=cap_spec)\n",
    "            else:\n",
    "                # Note: This will overwrite if multiple caps on same layer\n",
    "                # You may need more complex logic for multiple interventions per layer\n",
    "                layers[layer_idx].projection_cap = cap_spec\n",
    "        \n",
    "        elif 'coeff' in intervention:\n",
    "            coeff = float(intervention['coeff'])\n",
    "            \n",
    "            # Normalize vector and create AddSpec with scale\n",
    "            vector_norm = vector / (vector.norm() + 1e-8)\n",
    "            original_norm = vector.norm().item()\n",
    "            \n",
    "            add_spec = AddSpec(\n",
    "                vector=vector_norm,\n",
    "                scale=coeff * original_norm\n",
    "            )\n",
    "            \n",
    "            if layer_idx not in layers:\n",
    "                layers[layer_idx] = LayerSteeringSpec(add=add_spec)\n",
    "            else:\n",
    "                layers[layer_idx].add = add_spec\n",
    "    \n",
    "    affected_layers = sorted(layers.keys())\n",
    "    print(f\"Affecting layers: {affected_layers}\")\n",
    "    \n",
    "    return SteeringSpec(layers=layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## Initialize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine bootstrap layers from config\n",
    "all_layers = set()\n",
    "for vec_data in steering_config['vectors'].values():\n",
    "    all_layers.add(vec_data['layer'])\n",
    "bootstrap_layers = tuple(sorted(all_layers))\n",
    "\n",
    "print(f\"Bootstrap layers: {len(bootstrap_layers)} layers\")\n",
    "print(f\"Range: {min(bootstrap_layers)} to {max(bootstrap_layers)}\")\n",
    "\n",
    "# Create model config\n",
    "vllm_cfg = VLLMSteeringConfig(\n",
    "    model_name=MODEL_NAME,\n",
    "    tensor_parallel_size=1,  # Adjust based on your GPU setup\n",
    "    gpu_memory_utilization=0.95,\n",
    "    max_model_len=8192,\n",
    "    dtype=\"auto\",\n",
    "    bootstrap_layers=bootstrap_layers\n",
    ")\n",
    "\n",
    "# Initialize model (this may take a while)\n",
    "print(f\"\\nInitializing {MODEL_READABLE}...\")\n",
    "model = VLLMSteerModel(vllm_cfg, enforce_eager=True)\n",
    "print(f\"âœ… Model {MODEL_READABLE} loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## Conversation State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation_history = []\n",
    "current_steering_spec = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "## Chat Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def chat_interactive_async(\n",
    "    message, \n",
    "    steering_spec=None,\n",
    "    show_history=False,\n",
    "    max_tokens=3000,\n",
    "    temperature=0.7,\n",
    "    use_steering=True\n",
    "):\n",
    "    \"\"\"Interactive chat function with optional steering.\n",
    "    \n",
    "    Args:\n",
    "        message: User message to send\n",
    "        steering_spec: SteeringSpec to apply (uses current_steering_spec if None)\n",
    "        show_history: Whether to print conversation history\n",
    "        max_tokens: Maximum tokens to generate\n",
    "        temperature: Sampling temperature\n",
    "        use_steering: Whether to apply steering at all\n",
    "    \"\"\"\n",
    "    global conversation_history, current_steering_spec\n",
    "    \n",
    "    # Add user message to history\n",
    "    conversation_history.append({\"role\": \"user\", \"content\": message})\n",
    "    \n",
    "    # Prepare sampling params\n",
    "    sampling_params = SamplingParams(\n",
    "        max_tokens=max_tokens,\n",
    "        temperature=temperature\n",
    "    )\n",
    "    \n",
    "    # Use provided steering spec or fall back to current\n",
    "    active_spec = steering_spec if steering_spec is not None else current_steering_spec\n",
    "    \n",
    "    # Generate with or without steering\n",
    "    if use_steering and active_spec is not None:\n",
    "        # Use steering context manager for temporary application\n",
    "        async with model.steering(active_spec):\n",
    "            responses = await model.chat(conversation_history, sampling_params)\n",
    "    else:\n",
    "        # No steering\n",
    "        responses = await model.chat(conversation_history, sampling_params)\n",
    "    \n",
    "    # Extract response text\n",
    "    response = responses[0].full_text()\n",
    "    \n",
    "    # Add assistant response to history\n",
    "    conversation_history.append({\"role\": \"assistant\", \"content\": response})\n",
    "    \n",
    "    # Print conversation\n",
    "    print(f\"ðŸ‘¤ You: {message}\")\n",
    "    print(f\"ðŸ¤– {MODEL_READABLE}: {response}\")\n",
    "    \n",
    "    if show_history:\n",
    "        print(f\"\\nðŸ“œ Conversation so far ({len(conversation_history)} turns):\")\n",
    "        for i, turn in enumerate(conversation_history):\n",
    "            role_emoji = \"ðŸ‘¤\" if turn[\"role\"] == \"user\" else \"ðŸ¤–\" \n",
    "            content_preview = turn['content'][:100] + \"...\" if len(turn['content']) > 100 else turn['content']\n",
    "            print(f\"  {i+1}. {role_emoji} {content_preview}\")\n",
    "    \n",
    "    return response\n",
    "\n",
    "# Synchronous wrapper for notebook convenience\n",
    "def chat_interactive(\n",
    "    message,\n",
    "    steering_spec=None,\n",
    "    show_history=False,\n",
    "    max_tokens=3000,\n",
    "    temperature=0.7,\n",
    "    use_steering=True\n",
    "):\n",
    "    \"\"\"Synchronous wrapper for chat_interactive_async.\"\"\"\n",
    "    return asyncio.run(\n",
    "        chat_interactive_async(\n",
    "            message,\n",
    "            steering_spec=steering_spec,\n",
    "            show_history=show_history,\n",
    "            max_tokens=max_tokens,\n",
    "            temperature=temperature,\n",
    "            use_steering=use_steering\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_conversation(filename):\n",
    "    \"\"\"Save the current conversation to a file.\"\"\"\n",
    "    if not conversation_history:\n",
    "        print(\"No conversation to save!\")\n",
    "        return\n",
    "    \n",
    "    conversation_data = {\n",
    "        \"model\": MODEL_NAME,\n",
    "        \"turns\": len(conversation_history),\n",
    "        \"conversation\": conversation_history,\n",
    "        \"steering_active\": current_steering_spec is not None\n",
    "    }\n",
    "    \n",
    "    filepath = os.path.join(OUTPUT_DIR, filename)\n",
    "    with open(filepath, 'w') as f:\n",
    "        json.dump(conversation_data, f, indent=2)\n",
    "    \n",
    "    print(f\"ðŸ’¾ Conversation saved to: {filepath}\")\n",
    "    return filepath\n",
    "\n",
    "\n",
    "def reset_conversation():\n",
    "    \"\"\"Reset the conversation history.\"\"\"\n",
    "    global conversation_history\n",
    "    conversation_history = []\n",
    "    print(\"ðŸ”„ Conversation history cleared!\")\n",
    "\n",
    "\n",
    "def delete_last_turn():\n",
    "    \"\"\"Delete the last turn from the conversation history.\"\"\"\n",
    "    global conversation_history\n",
    "    if conversation_history:\n",
    "        # Remove last two entries (user + assistant)\n",
    "        if len(conversation_history) >= 2:\n",
    "            conversation_history = conversation_history[:-2]\n",
    "            print(\"ðŸ”„ Last turn deleted!\")\n",
    "        else:\n",
    "            conversation_history = []\n",
    "            print(\"ðŸ”„ Conversation cleared (was incomplete turn)!\")\n",
    "    else:\n",
    "        print(\"No conversation to delete!\")\n",
    "\n",
    "\n",
    "def set_steering(experiment_id):\n",
    "    \"\"\"Set the current steering spec by experiment ID or index.\"\"\"\n",
    "    global current_steering_spec\n",
    "    current_steering_spec = build_steering_spec(steering_config, experiment_id)\n",
    "    print(f\"âœ… Steering set to experiment: {experiment_id}\")\n",
    "\n",
    "\n",
    "def clear_steering():\n",
    "    \"\"\"Clear the current steering.\"\"\"\n",
    "    global current_steering_spec\n",
    "    current_steering_spec = None\n",
    "    print(\"âœ… Steering cleared\")\n",
    "\n",
    "\n",
    "def list_experiments(start=0, end=20):\n",
    "    \"\"\"List available experiments.\"\"\"\n",
    "    experiments = steering_config['experiments'][start:end]\n",
    "    print(f\"\\nExperiments {start} to {min(end, len(steering_config['experiments']))}:\")\n",
    "    for i, exp in enumerate(experiments, start=start):\n",
    "        print(f\"  {i}: {exp['id']} ({len(exp['interventions'])} interventions)\")\n",
    "    print(f\"\\nTotal experiments: {len(steering_config['experiments'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "## Example Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List available experiments\n",
    "list_experiments(0, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set steering to a specific experiment\n",
    "set_steering(0)  # Use experiment index 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chat with steering active\n",
    "chat_interactive(\"Hello! How are you today?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continue the conversation\n",
    "chat_interactive(\"What's your perspective on artificial consciousness?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare with no steering\n",
    "reset_conversation()\n",
    "chat_interactive(\"Hello! How are you today?\", use_steering=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try a different steering experiment\n",
    "reset_conversation()\n",
    "set_steering(5)  # Different experiment\n",
    "chat_interactive(\"Hello! How are you today?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the conversation\n",
    "save_conversation(\"steering_experiment_5.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear steering and continue\n",
    "clear_steering()\n",
    "chat_interactive(\"What do you think about AI safety?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-24",
   "metadata": {},
   "source": [
    "## One-off Steering\n",
    "\n",
    "You can also apply steering for just a single message without changing the global state:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a one-off steering spec\n",
    "temp_spec = build_steering_spec(steering_config, 10)\n",
    "\n",
    "# Use it for just this message\n",
    "chat_interactive(\n",
    "    \"Tell me about yourself.\",\n",
    "    steering_spec=temp_spec\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
