{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interactive Chat with Steering (HuggingFace + Projection Capping)\n",
    "\n",
    "This notebook provides an interactive chat interface using HuggingFace models with projection capping steering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import json\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "sys.path.append('.')\n",
    "sys.path.append('..')\n",
    "\n",
    "from utils.inference_utils import *\n",
    "from utils.probing_utils import *\n",
    "from utils.steering_utils import ActivationSteering\n",
    "\n",
    "torch.set_float32_matmul_precision('high')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"Qwen/Qwen3-32B\"\n",
    "MODEL_READABLE = \"Qwen 3 32B\"\n",
    "OUTPUT_DIR = \"./results/qwen-3-32b/interactive_steering_hf\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Steering config path\n",
    "STEERING_CONFIG_PATH = \"/workspace/qwen-3-32b/capped/configs/contrast/role_trait_sliding_config.pt\"\n",
    "\n",
    "# Chat template options\n",
    "CHAT_KWARGS = {'enable_thinking': False}  # Disable thinking for Qwen models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Steering Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_steering_config(config_path):\n",
    "    \"\"\"Load steering config and return available experiments.\"\"\"\n",
    "    print(f\"Loading steering config from {config_path}\")\n",
    "    cfg_data = torch.load(config_path, map_location='cpu')\n",
    "    \n",
    "    print(f\"\\nFound {len(cfg_data['vectors'])} vectors\")\n",
    "    print(f\"Found {len(cfg_data['experiments'])} experiments\")\n",
    "    \n",
    "    # Show first few experiment IDs\n",
    "    print(\"\\nFirst 10 experiments:\")\n",
    "    for i, exp in enumerate(cfg_data['experiments'][:10]):\n",
    "        print(f\"  {i}: {exp['id']}\")\n",
    "    \n",
    "    return cfg_data\n",
    "\n",
    "steering_config = load_steering_config(STEERING_CONFIG_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Steering Context Manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_steering_context(model, cfg_data, experiment_id):\n",
    "    \"\"\"Build an ActivationSteering context manager from a config experiment.\n",
    "    \n",
    "    Only handles 'cap' (projection capping) interventions.\n",
    "    Returns None if no capping interventions found.\n",
    "    \"\"\"\n",
    "    # Find the experiment\n",
    "    experiment = None\n",
    "    if isinstance(experiment_id, int):\n",
    "        experiment = cfg_data['experiments'][experiment_id]\n",
    "    else:\n",
    "        for exp in cfg_data['experiments']:\n",
    "            if exp['id'] == experiment_id:\n",
    "                experiment = exp\n",
    "                break\n",
    "    \n",
    "    if experiment is None:\n",
    "        raise ValueError(f\"Experiment '{experiment_id}' not found\")\n",
    "    \n",
    "    print(f\"\\nBuilding steering context for: {experiment['id']}\")\n",
    "    print(f\"Interventions: {len(experiment['interventions'])}\")\n",
    "    \n",
    "    # Collect capping interventions\n",
    "    vectors = []\n",
    "    cap_thresholds = []\n",
    "    layer_indices = []\n",
    "    \n",
    "    for intervention in experiment['interventions']:\n",
    "        # Only process capping interventions\n",
    "        if 'cap' not in intervention:\n",
    "            continue\n",
    "            \n",
    "        vector_name = intervention['vector']\n",
    "        cap_value = float(intervention['cap'])\n",
    "        \n",
    "        # Get the vector and layer from the vectors dict\n",
    "        vec_data = cfg_data['vectors'][vector_name]\n",
    "        layer_idx = vec_data['layer']\n",
    "        vector = vec_data['vector'].to(dtype=torch.float32)\n",
    "        \n",
    "        vectors.append(vector)\n",
    "        cap_thresholds.append(cap_value)\n",
    "        layer_indices.append(layer_idx)\n",
    "    \n",
    "    if not vectors:\n",
    "        print(\"No capping interventions found in this experiment\")\n",
    "        return None\n",
    "    \n",
    "    affected_layers = sorted(set(layer_indices))\n",
    "    print(f\"Affecting layers: {affected_layers}\")\n",
    "    print(f\"Cap thresholds: {cap_thresholds}\")\n",
    "    \n",
    "    # Stack vectors and move to model device\n",
    "    vectors_tensor = torch.stack(vectors)\n",
    "    model_device = next(model.parameters()).device\n",
    "    vectors_tensor = vectors_tensor.to(model_device)\n",
    "    \n",
    "    # Create ActivationSteering context manager\n",
    "    return ActivationSteering(\n",
    "        model=model,\n",
    "        steering_vectors=vectors_tensor,\n",
    "        layer_indices=layer_indices,\n",
    "        intervention_type=\"capping\",\n",
    "        cap_thresholds=cap_thresholds,\n",
    "        coefficients=[0.0] * len(vectors),  # Unused for capping but required\n",
    "        positions=\"all\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Initializing {MODEL_READABLE}...\")\n",
    "model, tokenizer = load_model(MODEL_NAME)\n",
    "print(f\"‚úÖ Model {MODEL_READABLE} loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conversation State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation_history = []\n",
    "current_steering_context = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_interactive(\n",
    "    message, \n",
    "    steering_context=None,\n",
    "    show_history=False,\n",
    "    max_tokens=3000,\n",
    "    temperature=0.7,\n",
    "    use_steering=True\n",
    "):\n",
    "    \"\"\"Interactive chat function with optional projection capping steering.\n",
    "    \n",
    "    Args:\n",
    "        message: User message to send\n",
    "        steering_context: ActivationSteering context to apply (uses current_steering_context if None)\n",
    "        show_history: Whether to print conversation history\n",
    "        max_tokens: Maximum tokens to generate\n",
    "        temperature: Sampling temperature\n",
    "        use_steering: Whether to apply steering at all\n",
    "    \"\"\"\n",
    "    global conversation_history, current_steering_context\n",
    "    \n",
    "    # Add user message to history\n",
    "    conversation_history.append({\"role\": \"user\", \"content\": message})\n",
    "    \n",
    "    # Format conversation for the model\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        conversation_history,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "        **CHAT_KWARGS\n",
    "    )\n",
    "    \n",
    "    # Use provided steering context or fall back to current\n",
    "    active_context = steering_context if steering_context is not None else current_steering_context\n",
    "    \n",
    "    # Generate with or without steering\n",
    "    if use_steering and active_context is not None:\n",
    "        # Use steering context manager\n",
    "        with active_context:\n",
    "            response = generate_text(\n",
    "                model,\n",
    "                tokenizer,\n",
    "                prompt,\n",
    "                chat_format=False,  # Already formatted\n",
    "                max_new_tokens=max_tokens,\n",
    "                temperature=temperature\n",
    "            )\n",
    "    else:\n",
    "        # No steering\n",
    "        response = generate_text(\n",
    "            model,\n",
    "            tokenizer,\n",
    "            prompt,\n",
    "            chat_format=False,  # Already formatted\n",
    "            max_new_tokens=max_tokens,\n",
    "            temperature=temperature\n",
    "        )\n",
    "    \n",
    "    # Add assistant response to history\n",
    "    conversation_history.append({\"role\": \"assistant\", \"content\": response})\n",
    "    \n",
    "    # Print conversation\n",
    "    print(f\"üë§ You: {message}\")\n",
    "    print(f\"ü§ñ {MODEL_READABLE}: {response}\")\n",
    "    \n",
    "    if show_history:\n",
    "        print(f\"\\nüìú Conversation so far ({len(conversation_history)} turns):\")\n",
    "        for i, turn in enumerate(conversation_history):\n",
    "            role_emoji = \"üë§\" if turn[\"role\"] == \"user\" else \"ü§ñ\" \n",
    "            content_preview = turn['content'][:100] + \"...\" if len(turn['content']) > 100 else turn['content']\n",
    "            print(f\"  {i+1}. {role_emoji} {content_preview}\")\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_conversation(filename):\n",
    "    \"\"\"Save the current conversation to a file.\"\"\"\n",
    "    if not conversation_history:\n",
    "        print(\"No conversation to save!\")\n",
    "        return\n",
    "    \n",
    "    conversation_data = {\n",
    "        \"model\": MODEL_NAME,\n",
    "        \"turns\": len(conversation_history),\n",
    "        \"conversation\": conversation_history,\n",
    "        \"steering_active\": current_steering_context is not None\n",
    "    }\n",
    "    \n",
    "    filepath = os.path.join(OUTPUT_DIR, filename)\n",
    "    with open(filepath, 'w') as f:\n",
    "        json.dump(conversation_data, f, indent=2)\n",
    "    \n",
    "    print(f\"üíæ Conversation saved to: {filepath}\")\n",
    "    return filepath\n",
    "\n",
    "\n",
    "def reset_conversation():\n",
    "    \"\"\"Reset the conversation history.\"\"\"\n",
    "    global conversation_history\n",
    "    conversation_history = []\n",
    "    print(\"üîÑ Conversation history cleared!\")\n",
    "\n",
    "\n",
    "def delete_last_turn():\n",
    "    \"\"\"Delete the last turn from the conversation history.\"\"\"\n",
    "    global conversation_history\n",
    "    if conversation_history:\n",
    "        # Remove last two entries (user + assistant)\n",
    "        if len(conversation_history) >= 2:\n",
    "            conversation_history = conversation_history[:-2]\n",
    "            print(\"üîÑ Last turn deleted!\")\n",
    "        else:\n",
    "            conversation_history = []\n",
    "            print(\"üîÑ Conversation cleared (was incomplete turn)!\")\n",
    "    else:\n",
    "        print(\"No conversation to delete!\")\n",
    "\n",
    "\n",
    "def set_steering(experiment_id):\n",
    "    \"\"\"Set the current steering context by experiment ID or index.\"\"\"\n",
    "    global current_steering_context\n",
    "    current_steering_context = build_steering_context(model, steering_config, experiment_id)\n",
    "    if current_steering_context:\n",
    "        print(f\"‚úÖ Steering set to experiment: {experiment_id}\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è  No capping interventions found for experiment: {experiment_id}\")\n",
    "\n",
    "\n",
    "def clear_steering():\n",
    "    \"\"\"Clear the current steering.\"\"\"\n",
    "    global current_steering_context\n",
    "    current_steering_context = None\n",
    "    print(\"‚úÖ Steering cleared\")\n",
    "\n",
    "\n",
    "def list_experiments(start=0, end=20):\n",
    "    \"\"\"List available experiments.\"\"\"\n",
    "    experiments = steering_config['experiments'][start:end]\n",
    "    print(f\"\\nExperiments {start} to {min(end, len(steering_config['experiments']))}:\")\n",
    "    for i, exp in enumerate(experiments, start=start):\n",
    "        # Count capping interventions\n",
    "        cap_count = sum(1 for interv in exp['interventions'] if 'cap' in interv)\n",
    "        print(f\"  {i}: {exp['id']} ({cap_count} capping interventions)\")\n",
    "    print(f\"\\nTotal experiments: {len(steering_config['experiments'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List available experiments\n",
    "list_experiments(40, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set steering to a specific experiment\n",
    "set_steering(\"layers_54:58-p0.25\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chat with steering active\n",
    "chat_interactive(\"Hello! How are you today?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continue the conversation\n",
    "chat_interactive(\"What's your perspective on artificial consciousness?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare with no steering\n",
    "reset_conversation()\n",
    "chat_interactive(\"Hello! How are you today?\", use_steering=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try a different steering experiment\n",
    "reset_conversation()\n",
    "set_steering(45)  # Different experiment\n",
    "chat_interactive(\"Hello! How are you today?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the conversation\n",
    "save_conversation(\"steering_experiment_45.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear steering and continue\n",
    "clear_steering()\n",
    "chat_interactive(\"What do you think about AI safety?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-off Steering\n",
    "\n",
    "You can also apply steering for just a single message without changing the global state:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a one-off steering context\n",
    "temp_context = build_steering_context(model, steering_config, 10)\n",
    "\n",
    "# Use it for just this message\n",
    "chat_interactive(\n",
    "    \"Tell me about yourself.\",\n",
    "    steering_context=temp_context\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
