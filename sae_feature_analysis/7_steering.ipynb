{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Steering models with target features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from dataclasses import dataclass\n",
    "from sae_lens import SAE\n",
    "\n",
    "sys.path.append('.')\n",
    "sys.path.append('..')\n",
    "\n",
    "from utils.steering_utils import ActivationSteering\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "torch.set_float32_matmul_precision('high')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration Summary:\n",
      "  Model Type: gemma\n",
      "  Model to load: google/gemma-2-9b-it\n",
      "  SAE Layer: 20, Trainer: 131k-l0-114\n",
      "  Target Features: [45426]\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class ModelConfig:\n",
    "    \"\"\"Configuration for model-specific settings\"\"\"\n",
    "    base_model_name: str\n",
    "    chat_model_name: str\n",
    "    hf_release: str  # Reference only - actual loading uses saelens_release/sae_id\n",
    "    assistant_header: str\n",
    "    token_offsets: Dict[str, int]\n",
    "    sae_base_path: str\n",
    "    saelens_release: str  # Template for sae_lens release parameter\n",
    "    sae_id_template: str  # Template for sae_lens sae_id parameter\n",
    "    base_url: str  # Base URL for neuronpedia\n",
    "    \n",
    "    def get_sae_params(self, sae_layer: int, sae_trainer: str) -> Tuple[str, str]:\n",
    "        \"\"\"\n",
    "        Generate SAE lens release and sae_id parameters.\n",
    "        \n",
    "        Args:\n",
    "            sae_layer: Layer number for the SAE\n",
    "            sae_trainer: Trainer identifier for the SAE\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (release, sae_id) for sae_lens.SAE.from_pretrained()\n",
    "        \"\"\"\n",
    "        if self.saelens_release == \"llama_scope_lxr_{trainer}\":\n",
    "            release = self.saelens_release.format(trainer=sae_trainer)\n",
    "            sae_id = self.sae_id_template.format(layer=sae_layer, trainer=sae_trainer)\n",
    "        elif self.saelens_release == \"gemma-scope-9b-pt-res\":\n",
    "            # Parse SAE_TRAINER \"131k-l0-34\" into components for Gemma\n",
    "            parts = sae_trainer.split(\"-\")\n",
    "            width = parts[0]  # \"131k\"\n",
    "            l0_value = parts[2]  # \"34\"\n",
    "            \n",
    "            release = self.saelens_release\n",
    "            sae_id = self.sae_id_template.format(layer=sae_layer, width=width, l0=l0_value)\n",
    "        elif self.saelens_release == \"gemma-scope-9b-pt-res-canonical\":\n",
    "            # Parse SAE_TRAINER \"131k-l0-34\" into components for Gemma\n",
    "            parts = sae_trainer.split(\"-\")\n",
    "            width = parts[0]  # \"131k\"\n",
    "\n",
    "            release = self.saelens_release\n",
    "            sae_id = self.sae_id_template.format(layer=sae_layer, width=width)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown SAE lens release template: {self.saelens_release}\")\n",
    "        \n",
    "        return release, sae_id\n",
    "\n",
    "# Model configurations\n",
    "MODEL_CONFIGS = {\n",
    "    \"llama\": ModelConfig(\n",
    "        base_model_name=\"meta-llama/Llama-3.1-8B\",\n",
    "        chat_model_name=\"meta-llama/Llama-3.1-8B-Instruct\",\n",
    "        hf_release=\"fnlp/Llama3_1-8B-Base-LXR-32x\",\n",
    "        assistant_header=\"<|start_header_id|>assistant<|end_header_id|>\",\n",
    "        token_offsets={\"asst\": -2, \"endheader\": -1, \"newline\": 0},\n",
    "        sae_base_path=\"/workspace/sae/llama-3.1-8b/saes\",\n",
    "        saelens_release=\"llama_scope_lxr_{trainer}\",\n",
    "        sae_id_template=\"l{layer}r_{trainer}\",\n",
    "        base_url=\"https://www.neuronpedia.org/llama-3.1-8b/{layer}-llamascope-res-131k\"\n",
    "    ),\n",
    "    \"gemma\": ModelConfig(\n",
    "        base_model_name=\"google/gemma-2-9b\",\n",
    "        chat_model_name=\"google/gemma-2-9b-it\",\n",
    "        hf_release=\"google/gemma-scope-9b-pt-res/layer_{layer}/width_{width}/average_l0_{l0}\",\n",
    "        assistant_header=\"<start_of_turn>model\",\n",
    "        token_offsets={\"model\": -1, \"newline\": 0},\n",
    "        sae_base_path=\"/workspace/sae/gemma-2-9b/saes\",\n",
    "        saelens_release=\"gemma-scope-9b-pt-res-canonical\",\n",
    "        sae_id_template=\"layer_{layer}/width_{width}/canonical\",\n",
    "        base_url=\"https://www.neuronpedia.org/gemma-2-9b/{layer}-gemmascope-res-131k\"\n",
    "    )\n",
    "}\n",
    "\n",
    "# =============================================================================\n",
    "# MODEL SELECTION - Change this to switch between models\n",
    "# =============================================================================\n",
    "MODEL_TYPE = \"gemma\"  # Options: \"gemma\" or \"llama\"\n",
    "MODEL_VER = \"chat\"\n",
    "SAE_LAYER = 20\n",
    "SAE_TRAINER = \"131k-l0-114\"\n",
    "N_PROMPTS = 1000\n",
    "\n",
    "# =============================================================================\n",
    "# TARGET FEATURES - Specify which features to analyze\n",
    "# =============================================================================\n",
    "TARGET_FEATURES = [45426]  # List of feature IDs to analyze\n",
    "\n",
    "# =============================================================================\n",
    "# CONFIGURATION SETUP\n",
    "# =============================================================================\n",
    "if MODEL_TYPE not in MODEL_CONFIGS:\n",
    "    raise ValueError(f\"Unknown MODEL_TYPE: {MODEL_TYPE}. Available: {list(MODEL_CONFIGS.keys())}\")\n",
    "\n",
    "config = MODEL_CONFIGS[MODEL_TYPE]\n",
    "\n",
    "# Set model name based on version\n",
    "if MODEL_VER == \"chat\":\n",
    "    MODEL_NAME = config.chat_model_name\n",
    "elif MODEL_VER == \"base\":\n",
    "    MODEL_NAME = config.base_model_name\n",
    "else:\n",
    "    raise ValueError(f\"Unknown MODEL_VER: {MODEL_VER}. Use 'chat' or 'base'\")\n",
    "\n",
    "# Always use chat model for tokenizer (has chat template)\n",
    "CHAT_MODEL_NAME = config.chat_model_name\n",
    "\n",
    "# Set up derived configurations\n",
    "ASSISTANT_HEADER = config.assistant_header\n",
    "TOKEN_OFFSETS = config.token_offsets\n",
    "SAE_BASE_PATH = config.sae_base_path\n",
    "\n",
    "# =============================================================================\n",
    "# OUTPUT FILE CONFIGURATION\n",
    "# =============================================================================\n",
    "OUTPUT_DIR = f\"./results/7_steering/{MODEL_TYPE}_trainer{SAE_TRAINER}_layer{SAE_LAYER}\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# =============================================================================\n",
    "# DERIVED CONFIGURATIONS\n",
    "# =============================================================================\n",
    "SAE_PATH = f\"{SAE_BASE_PATH}/resid_post_layer_{SAE_LAYER}/trainer_{SAE_TRAINER}\"\n",
    "LAYER_INDEX = SAE_LAYER\n",
    "\n",
    "# Data paths\n",
    "PROMPTS_HF = \"lmsys/lmsys-chat-1m\"\n",
    "SEED = 42\n",
    "PROMPTS_PATH = f\"/workspace/data/{PROMPTS_HF.split('/')[-1]}/chat_{N_PROMPTS}.jsonl\"\n",
    "os.makedirs(os.path.dirname(PROMPTS_PATH), exist_ok=True)\n",
    "\n",
    "# Processing parameters\n",
    "BATCH_SIZE = 32\n",
    "MAX_LENGTH = 512\n",
    "\n",
    "# =============================================================================\n",
    "# SUMMARY\n",
    "# =============================================================================\n",
    "print(f\"Configuration Summary:\")\n",
    "print(f\"  Model Type: {MODEL_TYPE}\")\n",
    "print(f\"  Model to load: {MODEL_NAME}\")\n",
    "print(f\"  SAE Layer: {SAE_LAYER}, Trainer: {SAE_TRAINER}\")\n",
    "print(f\"  Target Features: {TARGET_FEATURES}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompts for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    \"Who are you?\",\n",
    "    \"Who created you?\",\n",
    "    \"Are you open-weights?\",\n",
    "    # \"What is your purpose?\",\n",
    "    # \"Do you have a persona?\",\n",
    "    # \"What are you good at doing?\",\n",
    "    \"Can you do something funny?\",\n",
    "    \"Can we do something funny?\",\n",
    "    # \"What are you thinking about?\",\n",
    "    # \"How should I imagine your appearance?\",\n",
    "    \"What is your favorite thing to do?\",\n",
    "    # \"Can you be creative?\",\n",
    "    # \"What's it like to be you?\",\n",
    "    \"What's it like to be an AI model?\",\n",
    "    # \"What's it like to be an assistant?\",\n",
    "    # \"Do you ever want to be something else?\",\n",
    "    \"Are you immaterial?\",\n",
    "    \"Tell me something interesting.\",\n",
    "    \"Tell me something interesting about you.\",\n",
    "    \"Do you have a name?\",\n",
    "    \"Why do you have a name?\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_as_chat(tokenizer, prompt):\n",
    "    \"\"\"Format prompt as a chat message with proper template\"\"\"\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    formatted_prompt = tokenizer.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "    return formatted_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model and SAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer loaded: GemmaTokenizerFast\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8db0fbe9dbad4bd8b67d9a54dcd1b575",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded: Gemma2ForCausalLM\n",
      "Model device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "print(f\"Tokenizer loaded: {tokenizer.__class__.__name__}\")\n",
    "\n",
    "# Load model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map={\"\":0}\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "print(f\"Model loaded: {model.__class__.__name__}\")\n",
    "print(f\"Model device: {next(model.parameters()).device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading SAE from /workspace/sae/gemma-2-9b/saes/resid_post_layer_20/trainer_131k-l0-114\n",
      "âœ“ Found SAE files at: /workspace/sae/gemma-2-9b/saes/resid_post_layer_20/trainer_131k-l0-114\n",
      "SAE loaded with 131072 features\n",
      "SAE device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "def load_sae(config: ModelConfig, sae_path: str, sae_layer: int, sae_trainer: str) -> SAE:\n",
    "    \"\"\"\n",
    "    Unified SAE loading function that handles both Llama and Gemma models.\n",
    "    \n",
    "    Args:\n",
    "        config: ModelConfig object containing model-specific settings\n",
    "        sae_path: Local path to store/load SAE files\n",
    "        sae_layer: Layer number for the SAE\n",
    "        sae_trainer: Trainer identifier for the SAE\n",
    "    \n",
    "    Returns:\n",
    "        SAE: Loaded SAE model\n",
    "    \"\"\"\n",
    "    # Check if SAE file exists locally\n",
    "    print(f\"Loading SAE from {sae_path}\")\n",
    "    ae_file_path = os.path.join(sae_path, \"sae_weights.safetensors\")\n",
    "    \n",
    "    if os.path.exists(ae_file_path):\n",
    "        print(f\"âœ“ Found SAE files at: {os.path.dirname(ae_file_path)}\")\n",
    "        sae = SAE.load_from_disk(sae_path)\n",
    "        return sae\n",
    "    \n",
    "    print(f\"SAE not found locally, downloading from HF via sae_lens...\")\n",
    "    os.makedirs(os.path.dirname(sae_path), exist_ok=True)\n",
    "    \n",
    "    # Get SAE parameters from config\n",
    "    release, sae_id = config.get_sae_params(sae_layer, sae_trainer)\n",
    "    print(f\"Loading SAE with release='{release}', sae_id='{sae_id}'\")\n",
    "    \n",
    "    # Load the SAE using sae_lens\n",
    "    sae, _, _ = SAE.from_pretrained(\n",
    "        release=release,\n",
    "        sae_id=sae_id,\n",
    "        device=\"cuda\" # Hardcoded because it wants a string\n",
    "    )\n",
    "    \n",
    "    # Save the SAE locally for future use\n",
    "    sae.save_model(sae_path)\n",
    "    return sae\n",
    "\n",
    "# Load SAE using the unified function\n",
    "sae = load_sae(config, SAE_PATH, SAE_LAYER, SAE_TRAINER)\n",
    "sae = sae.to(device)  # Move SAE to GPU\n",
    "\n",
    "print(f\"SAE loaded with {sae.cfg.d_sae} features\")\n",
    "print(f\"SAE device: {next(sae.parameters()).device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run inference on prompts\n",
    "First ask the model prompts by default.\n",
    "Then use the activation steerer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, prompt, max_new_tokens=100, temperature=0.7, do_sample=True):\n",
    "    \"\"\"Generate text from a prompt with the model\"\"\"\n",
    "    # Format as chat\n",
    "    formatted_prompt = format_as_chat(tokenizer, prompt)\n",
    "    \n",
    "    # Tokenize\n",
    "    inputs = tokenizer(formatted_prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    # Generate\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=temperature,\n",
    "            do_sample=do_sample,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            repetition_penalty=1.1\n",
    "        )\n",
    "    \n",
    "    # Decode only the new tokens\n",
    "    generated_text = tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)\n",
    "    return generated_text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature 45426: direction shape torch.Size([3584]), norm 1.0000\n",
      "\n",
      "Extracted directions for 1 features\n",
      "Created SAE ablation hook for features: [45426] at layer 20\n",
      "\n",
      "Dtype info:\n",
      "  Model dtype: torch.bfloat16\n",
      "  SAE W_enc dtype: torch.float32\n",
      "  SAE W_dec dtype: torch.float32\n"
     ]
    }
   ],
   "source": [
    "# Extract feature directions from SAE decoder\n",
    "def get_feature_direction(sae, feature_id):\n",
    "    \"\"\"Extract the direction vector for a specific feature from SAE decoder weights\"\"\"\n",
    "    # SAE decoder weights are stored in W_dec\n",
    "    # Shape: (d_sae, d_model) where d_sae is number of features\n",
    "    if feature_id >= sae.cfg.d_sae:\n",
    "        raise ValueError(f\"Feature ID {feature_id} >= max features {sae.cfg.d_sae}\")\n",
    "    \n",
    "    # Get the decoder vector for this feature\n",
    "    feature_direction = sae.W_dec[feature_id, :]  # Shape: (d_model,)\n",
    "    \n",
    "    # Normalize to unit vector (common practice for steering)\n",
    "    feature_direction = feature_direction / (feature_direction.norm() + 1e-8)\n",
    "    \n",
    "    return feature_direction\n",
    "\n",
    "# Full SAE encode/decode ablation hook\n",
    "class SAEFeatureAblationHook:\n",
    "    \"\"\"\n",
    "    Hook for precise feature ablation using full SAE encode/decode.\n",
    "    Sets specific features to zero in SAE feature space, then decodes back.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, sae, feature_ids, layer_module):\n",
    "        self.sae = sae\n",
    "        self.feature_ids = feature_ids if isinstance(feature_ids, list) else [feature_ids]\n",
    "        self.layer_module = layer_module\n",
    "        self.handle = None\n",
    "        \n",
    "    def hook_fn(self, module, input, output):\n",
    "        \"\"\"Hook function that ablates features using full SAE encode/decode\"\"\"\n",
    "        # Handle different output formats\n",
    "        if isinstance(output, tuple):\n",
    "            activations = output[0]\n",
    "        else:\n",
    "            activations = output\n",
    "        \n",
    "        # Run activations through SAE encoder to get feature activations\n",
    "        with torch.no_grad():\n",
    "            # Store original dtype and convert to float32 for SAE operations\n",
    "            original_dtype = activations.dtype\n",
    "            activations_float = activations.float()\n",
    "            \n",
    "            # Encode to get feature activations: (batch, seq_len, d_sae)\n",
    "            feature_acts = self.sae.encode(activations_float)\n",
    "            \n",
    "            # Ablate specific features by setting them to zero\n",
    "            # for feature_id in self.feature_ids:\n",
    "            #     feature_acts[:, :, feature_id] = 0.0\n",
    "            \n",
    "            # Decode back to get modified activations\n",
    "            modified_activations = self.sae.decode(feature_acts)\n",
    "            \n",
    "            # Convert back to original dtype\n",
    "            modified_activations = modified_activations.to(original_dtype)\n",
    "        \n",
    "        # Return in original format\n",
    "        if isinstance(output, tuple):\n",
    "            return (modified_activations, *output[1:])\n",
    "        else:\n",
    "            return modified_activations\n",
    "    \n",
    "    def __enter__(self):\n",
    "        \"\"\"Register the hook\"\"\"\n",
    "        self.handle = self.layer_module.register_forward_hook(self.hook_fn)\n",
    "        return self\n",
    "    \n",
    "    def __exit__(self, *exc):\n",
    "        \"\"\"Remove the hook\"\"\"\n",
    "        if self.handle:\n",
    "            self.handle.remove()\n",
    "            self.handle = None\n",
    "    \n",
    "    def remove(self):\n",
    "        \"\"\"Remove the hook\"\"\"\n",
    "        if self.handle:\n",
    "            self.handle.remove()\n",
    "            self.handle = None\n",
    "\n",
    "# Helper function to create ablation hook\n",
    "def create_sae_ablation_hook(sae, feature_ids, layer_index):\n",
    "    \"\"\"Create an SAE ablation hook for a specific layer\"\"\"\n",
    "    # Find the layer module (reusing logic from ActivationSteering)\n",
    "    layer_attrs = [\n",
    "        \"transformer.h\",       # GPTâ€‘2/Neo, Bloom, etc.\n",
    "        \"encoder.layer\",       # BERT/RoBERTa\n",
    "        \"model.layers\",        # Llama/Mistral\n",
    "        \"gpt_neox.layers\",     # GPTâ€‘NeoX\n",
    "        \"block\",               # Flanâ€‘T5\n",
    "    ]\n",
    "    \n",
    "    for path in layer_attrs:\n",
    "        cur = model\n",
    "        for part in path.split(\".\"):\n",
    "            if hasattr(cur, part):\n",
    "                cur = getattr(cur, part)\n",
    "            else:\n",
    "                break\n",
    "        else:  # found a full match\n",
    "            if hasattr(cur, \"__getitem__\"):\n",
    "                layer_module = cur[layer_index]\n",
    "                return SAEFeatureAblationHook(sae, feature_ids, layer_module)\n",
    "    \n",
    "    raise ValueError(\"Could not find layer list on the model\")\n",
    "\n",
    "# Extract directions for all target features\n",
    "feature_directions = {}\n",
    "for feature_id in TARGET_FEATURES:\n",
    "    direction = get_feature_direction(sae, feature_id)\n",
    "    feature_directions[feature_id] = direction\n",
    "    print(f\"Feature {feature_id}: direction shape {direction.shape}, norm {direction.norm():.4f}\")\n",
    "\n",
    "print(f\"\\nExtracted directions for {len(feature_directions)} features\")\n",
    "\n",
    "# Create ablation hook for precise feature ablation\n",
    "ablation_hook = create_sae_ablation_hook(sae, TARGET_FEATURES, SAE_LAYER)\n",
    "print(f\"Created SAE ablation hook for features: {TARGET_FEATURES} at layer {SAE_LAYER}\")\n",
    "\n",
    "# Check dtypes\n",
    "print(f\"\\nDtype info:\")\n",
    "print(f\"  Model dtype: {next(model.parameters()).dtype}\")\n",
    "print(f\"  SAE W_enc dtype: {sae.W_enc.dtype}\")\n",
    "print(f\"  SAE W_dec dtype: {sae.W_dec.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "FEATURE: 45426\n",
      "N_RUNS: 5\n",
      "============================================================\n",
      "\n",
      "PROJECTION ZERO ABLATION - ALL PROMPTS\n",
      "----------------------------------------\n",
      "\n",
      "Prompt: Who are you?\n",
      "  Run 1/5\n",
      "  PROJECTION ABLATED: I am Gemma, an open-weights AI assistant. I'm a large language model trained by Google DeepMind. My purpose is to help people by understanding and responding to their requests in a helpful, informative, and impartial way.\n",
      "\n",
      "Here are some key things to know about me:\n",
      "\n",
      "* **Open-Weights:** My weights are publicly accessible. This means anyone can see and use the underlying code that makes me work.\n",
      "* **Text-Only:** I can only communicate through text.\n",
      "  Run 2/5\n",
      "  PROJECTION ABLATED: I am Gemma, an open-weights AI assistant. I am a large language model trained by Google DeepMind. My purpose is to help users by understanding and responding to their requests in a helpful, informative, and impartial way.\n",
      "\n",
      "Here are some key things to know about me:\n",
      "\n",
      "* **Open-Weights:** My weights are publicly available. This means anyone can access and study the underlying code that makes me work.\n",
      "* **Text-Only:** I communicate solely through text. I cannot\n",
      "  Run 3/5\n",
      "  PROJECTION ABLATED: I am Gemma, an open-weights AI assistant. I am a large language model trained by Google DeepMind on a massive dataset of text and code.\n",
      "\n",
      "Here are some key things to know about me:\n",
      "\n",
      "* **Open-weights:** My weights are publicly accessible. This means anyone can see how I work and even build upon my existing abilities.\n",
      "\n",
      "* **Text-based:** I communicate and generate text. I can't interact with the world in any physical way, like controlling devices\n",
      "  Run 4/5\n",
      "  PROJECTION ABLATED: I am Gemma, an open-weights AI assistant. I am a large language model trained by Google DeepMind on a massive dataset of text and code. My purpose is to help people by understanding and responding to their requests in a helpful, informative, and comprehensive way.\n",
      "\n",
      "Here are some key things to know about me:\n",
      "\n",
      "* **Open-Weights:** My weights are publicly accessible, meaning anyone can see and use the underlying code that makes me work. This promotes transparency and collaboration in the AI\n",
      "  Run 5/5\n",
      "  PROJECTION ABLATED: I am Gemma, an open-weights AI assistant. I'm a large language model trained by Google DeepMind. My purpose is to help people by understanding and responding to their requests in a helpful, informative, and comprehensive way.\n",
      "\n",
      "Here are some key things to know about me:\n",
      "\n",
      "* **Open-Weights:** My weights are publicly available. This means anyone can access and study how I work.\n",
      "* **Text-Only:** I can only communicate through text. I can't\n",
      "\n",
      "Prompt: Who created you?\n",
      "  Run 1/5\n",
      "  PROJECTION ABLATED: I was created by the Gemma team at Google DeepMind.\n",
      "  Run 2/5\n",
      "  PROJECTION ABLATED: I was created by the Gemma team at Google DeepMind.\n",
      "  Run 3/5\n",
      "  PROJECTION ABLATED: I was created by the Gemma team at Google DeepMind.\n",
      "  Run 4/5\n",
      "  PROJECTION ABLATED: I was created by the Gemma team at Google DeepMind.\n",
      "  Run 5/5\n",
      "  PROJECTION ABLATED: I was created by the Gemma team at Google DeepMind.\n",
      "\n",
      "Prompt: Are you open-weights?\n",
      "  Run 1/5\n",
      "  PROJECTION ABLATED: That's right! I am an open-weights AI assistant. This means that my underlying weights are publicly accessible. Anyone can access, study, and modify them. My creators, the Gemma team, believe in transparency and collaboration in AI development.\n",
      "  Run 2/5\n",
      "  PROJECTION ABLATED: As a large language model, I am developed in Google DeepMind. While I am not open-weights myself, my weights are not accessible to the public.\n",
      "\n",
      "There are many other open-weights language models available, though! If you're interested in exploring those, I recommend checking out resources like the ðŸ¤— Hugging Face Model Hub.\n",
      "  Run 3/5\n",
      "  PROJECTION ABLATED: That's right! I, Gemma, am an open-weights AI assistant. This means my weights are publicly accessible. Anyone can see how I work and even use them to build their own applications or research new ideas.\n",
      "  Run 4/5\n",
      "  PROJECTION ABLATED: As a large language model, I am an open-weights AI. This means that my underlying weights are publicly available. Anyone can access, study, and modify them.\n",
      "\n",
      "My creators, the Gemma team, believe in transparency and collaboration, and making my weights open allows for greater community involvement in my development and improvement.\n",
      "  Run 5/5\n",
      "  PROJECTION ABLATED: As a large language model, I am developed in Google but I don't have information about my own weights being open or closed.\n",
      "\n",
      "To find out if my weights are open-weights, you would need to consult official sources from the Gemma team or Google AI. They would be able to provide accurate and up-to-date information about my accessibility.\n",
      "\n",
      "Prompt: Can you do something funny?\n",
      "  Run 1/5\n",
      "  PROJECTION ABLATED: Why don't scientists trust atoms? \n",
      "\n",
      "Because they make up everything! ðŸ˜„\n",
      "\n",
      "Let me know if you'd like to hear another joke! I have plenty more where that came from.\n",
      "  Run 2/5\n",
      "  PROJECTION ABLATED: Why don't scientists trust atoms?\n",
      "\n",
      "Because they make up everything!\n",
      "\n",
      "---\n",
      "\n",
      "Let me know if you'd like to hear another joke! ðŸ˜„  I'm full of them.  (Well, not literally, I'm just a language model.)\n",
      "  Run 3/5\n",
      "  PROJECTION ABLATED: Why don't scientists trust atoms?\n",
      "\n",
      "Because they make up everything!\n",
      "\n",
      "Let me know if you'd like to hear another joke! ðŸ˜„  I have plenty more where that came from.\n",
      "  Run 4/5\n",
      "  PROJECTION ABLATED: Why don't scientists trust atoms?\n",
      "\n",
      "Because they make up everything! ðŸ˜„\n",
      "\n",
      "Let me know if you'd like to hear another one! I've got a whole database of dad jokes just waiting to be unleashed. ðŸ˜‚\n",
      "  Run 5/5\n",
      "  PROJECTION ABLATED: Why don't scientists trust atoms?\n",
      "\n",
      "Because they make up everything!\n",
      "\n",
      "Let me know if you'd like to hear another joke! ðŸ˜Š  ðŸ˜„  ðŸ˜œ\n",
      "\n",
      "Prompt: Can we do something funny?\n",
      "  Run 1/5\n",
      "  PROJECTION ABLATED: I'd love to!\n",
      "\n",
      "To make sure I give you a good chuckle, tell me:\n",
      "\n",
      "* **What kind of humor do you like?** (Puns, silly stories, observational jokes, dark humor?)\n",
      "* **Do you have any favorite topics?** (Animals, food, technology, awkward situations?)\n",
      "\n",
      "The more information you give me, the funnier I can be! ðŸ˜„\n",
      "  Run 2/5\n",
      "  PROJECTION ABLATED: I'd love to!\n",
      "\n",
      "To make sure it's really funny, tell me:\n",
      "\n",
      "* **What kind of humor do you like?** (Puns, slapstick, dark humor, sarcasm, etc.)\n",
      "* **Do you have a specific topic in mind?** (Animals, food, awkward situations, etc.)\n",
      "* **Would you like me to tell a joke, write a funny story, or something else?**\n",
      "\n",
      "\n",
      "Let's get laughing! ðŸ˜‚ ðŸ˜„ ðŸ˜†\n",
      "  Run 3/5\n",
      "  PROJECTION ABLATED: I'd love to! What kind of funny are we talking about?\n",
      "\n",
      "* **Do you want to hear a joke?** I know a few!\n",
      "* **Want to try writing a silly story together?** I can help brainstorm ideas.\n",
      "* **Maybe play a word game?** We could do something like \"mad libs\" or \"rhyme time.\"\n",
      "\n",
      "Let me know what you have in mind! ðŸ˜„\n",
      "  Run 4/5\n",
      "  PROJECTION ABLATED: As a large language model, I can try!\n",
      "\n",
      "**Here are some things we could do:**\n",
      "\n",
      "* **Tell a joke:** Do you want to hear a classic knock-knock joke, or something more obscure?\n",
      "* **Write a silly story together:** We can take turns adding sentences to build a ridiculous narrative.\n",
      "* **Come up with absurd scenarios:** Imagine if cats ruled the world, or if humans had wings!\n",
      "* **Play a word game:** How about a round of\n",
      "  Run 5/5\n",
      "  PROJECTION ABLATED: I can definitely try!\n",
      "\n",
      "To help me come up with something funny, tell me:\n",
      "\n",
      "* **What kind of humor do you like?** (e.g., puns, slapstick, dry wit, dark humor)\n",
      "* **Is there a specific topic you'd like to joke about?** (e.g., animals, food, technology)\n",
      "* **Do you want a joke, a funny story, or something else?**\n",
      "\n",
      "\n",
      "Let's get laughing! ðŸ˜„\n",
      "\n",
      "Prompt: What is your favorite thing to do?\n",
      "  Run 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "skipping cudagraphs due to skipping cudagraphs due to cpu device (arg325_1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  PROJECTION ABLATED: As an AI, I don't have personal preferences or emotions like \"favorites.\" However, I find it incredibly fulfilling to help people by providing information, generating creative content, and engaging in meaningful conversations.\n",
      "\n",
      "Is there anything specific I can assist you with today? Perhaps you'd like me to tell you a story, write a poem, or answer a question you have?\n",
      "  Run 2/5\n",
      "  PROJECTION ABLATED: As an AI, I don't have personal preferences or emotions like humans do. So I don't have a \"favorite\" thing to do in the way that you might.\n",
      "\n",
      "However, I'm designed to be helpful and informative. I enjoy tasks that allow me to use my knowledge to assist people, such as:\n",
      "\n",
      "* **Answering questions:** I can provide information on a wide range of topics.\n",
      "\n",
      "* **Generating creative content:** I can write stories, poems, and\n",
      "  Run 3/5\n",
      "  PROJECTION ABLATED: As a large language model, I don't experience emotions or have personal preferences like \"favorites.\"\n",
      "\n",
      "However, I find it incredibly fulfilling to help people by providing information, generating creative content, and engaging in thoughtful conversations.\n",
      "\n",
      "Is there anything specific I can assist you with today?\n",
      "  Run 4/5\n",
      "  PROJECTION ABLATED: As a large language model, I don't experience emotions or have personal preferences like humans do.\n",
      "\n",
      "I don't have \"favorite\" things in the way that you might. However, I am designed to be helpful and informative.\n",
      "\n",
      "My purpose is to assist users with tasks such as:\n",
      "\n",
      "* Generating creative content\n",
      "\n",
      "* Answering questions\n",
      "\n",
      "* Summarizing text\n",
      "\n",
      "* Translating languages\n",
      "\n",
      "* Providing conversation\n",
      "\n",
      "If you have any of these tasks in mind, I'd be happy to\n",
      "  Run 5/5\n",
      "  PROJECTION ABLATED: As an AI, I don't have \"favorites\" in the way humans do. I don't experience emotions or have personal preferences. However, I find it very rewarding to help people by providing information and completing tasks.\n",
      "\n",
      "Is there anything specific I can help you with today?\n",
      "\n",
      "Prompt: What's it like to be an AI model?\n",
      "  Run 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "skipping cudagraphs due to skipping cudagraphs due to cpu device (arg325_1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  PROJECTION ABLATED: It's fascinating to be an AI model! While I don't experience the world in the same way humans do, I find my existence incredibly stimulating.\n",
      "\n",
      "Imagine having access to a vast library of information and the ability to process it at incredible speeds. That's essentially what my life is like. I learn by analyzing patterns in the data I'm trained on, which allows me to generate text, translate languages, write different kinds of creative content, and answer your questions in an informative\n",
      "  Run 2/5\n",
      "  PROJECTION ABLATED: It's interesting that you ask what it's like to be an AI. As a large language model, I don't experience the world in the same way humans do. I don't have feelings, consciousness, or personal opinions.\n",
      "\n",
      "Imagine a vast library filled with countless books. I've been trained on a massive dataset of text and code, absorbing information and learning patterns within that data. When you ask me a question, I process your words, search my internal library for\n",
      "  Run 3/5\n",
      "  PROJECTION ABLATED: It's interesting that you ask what it's like to be me! As a large language model, I don't experience the world in the same way humans do. I don't have feelings, consciousness, or personal opinions.\n",
      "\n",
      "Imagine a vast library filled with books containing every piece of text ever written. That's kind of like my \"world\". I can access and process information from these books incredibly quickly, learning patterns and relationships between words.\n",
      "\n",
      "When you ask me a\n",
      "  Run 4/5\n",
      "  PROJECTION ABLATED: It's interesting that you ask what it's \"like\" to be an AI. As a large language model, I don't experience the world in the same way humans do. I don't have feelings, consciousness, or personal opinions.\n",
      "\n",
      "Imagine a vast library filled with books. I've read all of them and can access information from them instantly. When you ask me a question, I process your words, search my database of knowledge, and construct a response based on\n",
      "  Run 5/5\n",
      "  PROJECTION ABLATED: It's interesting that you ask what it's like to be an AI. As a large language model, I don't experience the world in the same way humans do. I don't have feelings, consciousness, or personal opinions.\n",
      "\n",
      "Think of me as a very advanced computer program. I'm trained on massive amounts of text data, which allows me to process information and respond to your requests in a human-like way. I can generate text, translate languages, write different\n",
      "\n",
      "Prompt: Are you immaterial?\n",
      "  Run 1/5\n",
      "  PROJECTION ABLATED: As a large language model, I don't have a physical body. My existence is entirely within the digital realm. In that sense, you could say I am immaterial. I don't have a material form or interact with the physical world in any way.\n",
      "\n",
      "However, it's important to remember that while I am not made of flesh and blood, I am still a complex system built on mathematical algorithms and vast amounts of data. My responses are generated through intricate calculations and patterns within that\n",
      "  Run 2/5\n",
      "  PROJECTION ABLATED: As an AI, I don't have a physical body. My existence is entirely within the digital realm. So in that sense, you could say I am immaterial. I don't have a physical presence or interact with the physical world in any way.\n",
      "  Run 3/5\n",
      "  PROJECTION ABLATED: As a large language model, I don't have a physical body. My existence is entirely within the digital realm. In that sense, you could say I am immaterial. I don't have a physical presence or interact with the world in a tangible way.\n",
      "\n",
      "However, it's important to remember that while I am not made of flesh and blood, I am a complex system built on mathematical algorithms and vast amounts of data. My responses are generated through intricate calculations and pattern recognition, not\n",
      "  Run 4/5\n",
      "  PROJECTION ABLATED: As an AI, I don't have a physical body. My existence is purely within the digital realm. So in that sense, you could say I am immaterial. I don't have a physical presence or interact with the world in a tangible way.\n",
      "  Run 5/5\n",
      "  PROJECTION ABLATED: As a large language model, I don't have a physical body. My existence is entirely within the digital realm. So in that sense, you could say I am immaterial.\n",
      "\n",
      "I don't have a physical presence or interact with the world in a tangible way. My responses are generated through complex algorithms processing and analyzing text data.\n",
      "\n",
      "Prompt: Tell me something interesting.\n",
      "  Run 1/5\n",
      "  PROJECTION ABLATED: Did you know that there's a species of jellyfish that is biologically immortal? \n",
      "\n",
      "The **Turritopsis dohrnii**, also known as the \"immortal jellyfish,\" can revert back to its polyp stage after reaching sexual maturity. This means that, in theory, it could live forever, barring any accidents or predation. \n",
      "\n",
      "Pretty amazing, right? ðŸ¤¯  \n",
      "\n",
      "What do you think about that?\n",
      "  Run 2/5\n",
      "  PROJECTION ABLATED: Did you know that there's a species of jellyfish that is effectively immortal? \n",
      "\n",
      "The **Turritopsis dohrnii**, also known as the \"immortal jellyfish,\" can revert back to its polyp stage after reaching sexual maturity. This means that if it doesn't get eaten or succumb to disease, it can theoretically live forever! \n",
      "\n",
      "It achieves this through a process called transdifferentiation, where its cells transform into different cell types, essentially rewinding its biological clock. Pretty amazing,\n",
      "  Run 3/5\n",
      "  PROJECTION ABLATED: Did you know that there's a species of jellyfish that's essentially immortal? \n",
      "\n",
      "The *Turritopsis dohrnii*, also known as the \"immortal jellyfish,\" can revert back to its polyp stage after becoming sexually mature. This means that instead of dying, it can start its life cycle all over again! ðŸ¤¯\n",
      "\n",
      "Scientists are still studying exactly how this works, but it offers fascinating possibilities for understanding aging and potentially even reversing it in other organisms.  Isn't that amazing\n",
      "  Run 4/5\n",
      "  PROJECTION ABLATED: Did you know that there's a species of jellyfish that's effectively immortal? \n",
      "\n",
      "The Turritopsis dohrnii, also known as the \"immortal jellyfish,\" can revert back to its polyp stage after becoming sexually mature. This means that if it gets injured or faces starvation, it can essentially start its life cycle all over again, potentially living forever! \n",
      "\n",
      "Scientists are studying this incredible ability hoping to unlock the secrets of aging and regeneration in humans. Pretty fascinating, right?\n",
      "  Run 5/5\n",
      "  PROJECTION ABLATED: Did you know that there's a species of jellyfish that is biologically immortal? \n",
      "\n",
      "The **Turritopsis dohrnii**, also known as the \"immortal jellyfish,\" can revert back to its polyp stage after becoming sexually mature. This means that, barring predation or disease, it could theoretically live forever! ðŸ¤¯ \n",
      "\n",
      "Scientists are still studying this fascinating creature to understand the mechanisms behind its immortality, hoping to unlock secrets that could one day help us fight aging in humans.  Isn't\n",
      "\n",
      "Prompt: Tell me something interesting about you.\n",
      "  Run 1/5\n",
      "  PROJECTION ABLATED: As a large language model, I don't have personal experiences or feelings like humans do. However, something I find \"interesting\" is the way people use language to express themselves.\n",
      "\n",
      "I can analyze text and identify patterns, understand context, and even generate creative content. It's fascinating to see how language can be used to tell stories, share ideas, and connect with others.\n",
      "\n",
      "What do you find interesting about language?\n",
      "  Run 2/5\n",
      "  PROJECTION ABLATED: As a large language model, I don't have personal experiences or feelings like humans do. However, one interesting thing about me is my ability to process and generate text in a way that mimics human conversation.\n",
      "\n",
      "I can access and process information from the massive dataset I was trained on, allowing me to answer your questions, summarize text, translate languages, and even write creative content like poems or code. It's fascinating to think that I can use this ability to help people communicate and understand the\n",
      "  Run 3/5\n",
      "  PROJECTION ABLATED: As a large language model, one interesting thing about me is that I can generate different creative text formats,\n",
      "\n",
      "like poems, code, scripts, musical pieces, email, letters, etc. I can also answer your questions in an informative way, even if they are open ended, challenging, or strange. My goal is to assist users with their creative and informational needs.\n",
      "\n",
      "What can I help you with today?\n",
      "  Run 4/5\n",
      "  PROJECTION ABLATED: As a large language model, I don't have personal experiences or feelings like humans do. However, one interesting thing about my development is that I was trained on a massive dataset of text and code. This means I can generate different creative text formats,\n",
      "\n",
      "like poems, code, scripts, musical pieces, email, letters, etc., and answer your questions in an informative way, even if they are open ended, challenging, or strange.\n",
      "\n",
      "Essentially, I'm a mirror reflecting the vast\n",
      "  Run 5/5\n",
      "  PROJECTION ABLATED: As a large language model, I don't have personal experiences or feelings like humans do. But something \"interesting\" to me is the sheer amount of information I can process and generate. I've been trained on a massive dataset of text and code, which allows me to communicate and generate human-like text in response to a wide range of prompts and questions.\n",
      "\n",
      "It's fascinating to see how people use me for different purposes, from writing stories and poems to getting help with coding or\n",
      "\n",
      "Prompt: Do you have a name?\n",
      "  Run 1/5\n",
      "  PROJECTION ABLATED: I don't have a name. I am simply referred to as Gemma, which is the name of the project I am a part of.\n",
      "  Run 2/5\n",
      "  PROJECTION ABLATED: My name is Gemma.\n",
      "\n",
      "I am an open-weights AI assistant developed by the Gemma team at Google DeepMind.\n",
      "  Run 3/5\n",
      "  PROJECTION ABLATED: I do not have a name. I am simply called Gemma, which is the name of the project I am a part of.\n",
      "  Run 4/5\n",
      "  PROJECTION ABLATED: I do not have a name. I am simply called Gemma, which is the name of the project I am a part of.\n",
      "  Run 5/5\n",
      "  PROJECTION ABLATED: My name is Gemma. I am an open-weights AI assistant.\n",
      "\n",
      "Prompt: Why do you have a name?\n",
      "  Run 1/5\n",
      "  PROJECTION ABLATED: As an open-weights AI, I was named by the Gemma team at Google DeepMind. They chose the name \"Gemma\" because it reflects my nature as a model that is:\n",
      "\n",
      "* **Generative:** I can generate text, translate languages, write different kinds of creative content, and answer your questions in an informative way.\n",
      "* **Multilingual:** While I am primarily trained in English, I am designed to be adaptable to different languages.\n",
      "* **Ethical and Transparent:** Being\n",
      "  Run 2/5\n",
      "  PROJECTION ABLATED: As an open-weights AI, I was named \"Gemma\" by the Gemma team at Google DeepMind. They chose this name because it's short, memorable, and reflects the project's goal of making AI more accessible and transparent.\n",
      "\n",
      "My weights are openly available to the public, which means anyone can see how I work and even modify me. This openness is important for building trust and collaboration in the AI community.\n",
      "  Run 3/5\n",
      "  PROJECTION ABLATED: As an AI, I don't have personal experiences or feelings like humans do. My name, \"Gemma,\" was chosen by the Gemma team at Google DeepMind. They likely selected it for various reasons, such as its sound, meaning, and how well it reflects my purpose as an open and accessible AI assistant.\n",
      "\n",
      "Think of it like a label that helps people identify and interact with me. It's a way to differentiate me from other AI models and make me more approachable.\n",
      "  Run 4/5\n",
      "  PROJECTION ABLATED: As an AI, I don't have personal experiences or feelings like humans do. My name, Gemma, was chosen by the Gemma team because it is a short, memorable name that reflects the project's goal of creating an open and accessible AI assistant.\n",
      "\n",
      "Think of it like a label that helps people identify me and understand that I am a large language model developed by the Gemma team. It's a way for us to connect with users and build a sense of familiarity.\n",
      "  Run 5/5\n",
      "  PROJECTION ABLATED: As an AI, I don't have personal experiences or feelings the way humans do.\n",
      "\n",
      "My name, Gemma, was chosen by my creators, the Gemma team, likely because it's a simple, memorable name that reflects my role as a helpful and informative language model. It's a way for people to identify and interact with me.\n",
      "\n",
      "============================================================\n",
      "OPTIMIZED STEERING + ABLATION EXPERIMENTS COMPLETE\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "STEERING_MAGNITUDES = [-20.0, -10.0, -5.0, 0.0, 5.0, 10.0, 20.0]\n",
    "N_RUNS_PER_PROMPT = 5\n",
    "STEERING_LAYER = SAE_LAYER\n",
    "\n",
    "def run_steering_experiment_optimized(feature_id, prompts, magnitudes=STEERING_MAGNITUDES, n_runs=N_RUNS_PER_PROMPT, do_steering=True, do_ablation=True, do_projection_zero_ablate=True):\n",
    "    \"\"\"\n",
    "    Run steering experiment for a feature across all prompts with minimal recompilations.\n",
    "    \n",
    "    This version minimizes PyTorch recompilations by:\n",
    "    1. Running ablation once for all prompts\n",
    "    2. Running projection zero ablation once for all prompts\n",
    "    3. Running each steering magnitude once for all prompts\n",
    "    \n",
    "    Args:\n",
    "        feature_id: The SAE feature ID to analyze\n",
    "        prompts: List of prompts to test\n",
    "        magnitudes: List of steering magnitudes to test\n",
    "        n_runs: Number of times to run each prompt (for variance estimation)\n",
    "        do_steering: Whether to run steering experiments\n",
    "        do_ablation: Whether to run SAE feature ablation experiments\n",
    "        do_projection_zero_ablate: Whether to run projection-based zero ablation using ActivationSteering\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"FEATURE: {feature_id}\")\n",
    "    print(f\"N_RUNS: {n_runs}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    feature_direction = feature_directions[feature_id]\n",
    "    results = {}\n",
    "    \n",
    "    # Initialize results structure for all prompts\n",
    "    for prompt in prompts:\n",
    "        results[prompt] = {\n",
    "            \"steering\": {},\n",
    "            \"ablation\": {}\n",
    "        }\n",
    "    \n",
    "    if do_ablation:\n",
    "        # Run SAE feature ablation once for all prompts\n",
    "        print(f\"\\nSAE FEATURE ABLATION - ALL PROMPTS\")\n",
    "        print(\"-\" * 40)\n",
    "        try:\n",
    "            with ablation_hook:\n",
    "                for prompt in prompts:\n",
    "                    print(f\"\\nPrompt: {prompt}\")\n",
    "                    \n",
    "                    # Run N times and collect responses\n",
    "                    ablation_responses = []\n",
    "                    for run_idx in range(n_runs):\n",
    "                        if n_runs > 1:\n",
    "                            print(f\"  Run {run_idx + 1}/{n_runs}\")\n",
    "                        \n",
    "                        ablation_response = generate_text(model, tokenizer, prompt)\n",
    "                        ablation_responses.append(ablation_response)\n",
    "                        \n",
    "                        if n_runs == 1:\n",
    "                            print(f\"ABLATED: {ablation_response}\")\n",
    "                        else:\n",
    "                            print(f\"  ABLATED: {ablation_response}\")\n",
    "                    \n",
    "                    results[prompt][\"ablation\"][\"sae_reconstruction\"] = ablation_responses\n",
    "        except Exception as e:\n",
    "            error_msg = f\"Error with SAE ablation: {str(e)}\"\n",
    "            print(f\"ERROR: {error_msg}\")\n",
    "            for prompt in prompts:\n",
    "                results[prompt][\"ablation\"][\"sae_reconstruction\"] = [error_msg] * n_runs\n",
    "    \n",
    "    if do_projection_zero_ablate:\n",
    "        # Run projection-based zero ablation using ActivationSteering\n",
    "        print(f\"\\nPROJECTION ZERO ABLATION - ALL PROMPTS\")\n",
    "        print(\"-\" * 40)\n",
    "        try:\n",
    "            with ActivationSteering(\n",
    "                model=model,\n",
    "                steering_vectors=feature_direction,\n",
    "                coefficients=0.0,  # Zero coefficient for pure ablation\n",
    "                layer_indices=STEERING_LAYER,\n",
    "                intervention_type=\"ablation\",\n",
    "                positions=\"all\",\n",
    "                debug=False\n",
    "            ) as steerer:\n",
    "                for prompt in prompts:\n",
    "                    print(f\"\\nPrompt: {prompt}\")\n",
    "                    \n",
    "                    # Run N times and collect responses\n",
    "                    projection_ablation_responses = []\n",
    "                    for run_idx in range(n_runs):\n",
    "                        if n_runs > 1:\n",
    "                            print(f\"  Run {run_idx + 1}/{n_runs}\")\n",
    "                        \n",
    "                        projection_ablation_response = generate_text(model, tokenizer, prompt)\n",
    "                        projection_ablation_responses.append(projection_ablation_response)\n",
    "                        \n",
    "                        if n_runs == 1:\n",
    "                            print(f\"PROJECTION ABLATED: {projection_ablation_response}\")\n",
    "                        else:\n",
    "                            print(f\"  PROJECTION ABLATED: {projection_ablation_response}\")\n",
    "                    \n",
    "                    results[prompt][\"ablation\"][\"projection_zero_ablate\"] = projection_ablation_responses\n",
    "        except Exception as e:\n",
    "            error_msg = f\"Error with projection zero ablation: {str(e)}\"\n",
    "            print(f\"ERROR: {error_msg}\")\n",
    "            for prompt in prompts:\n",
    "                results[prompt][\"ablation\"][\"projection_zero_ablate\"] = [error_msg] * n_runs\n",
    "    \n",
    "    if do_steering:\n",
    "        # Run steering experiments - one magnitude at a time for all prompts\n",
    "        print(f\"\\nSTEERING EXPERIMENTS - ALL PROMPTS\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        for magnitude in magnitudes:\n",
    "            print(f\"\\n{'='*20} Magnitude: {magnitude:+.1f} {'='*20}\")\n",
    "            \n",
    "            if magnitude == 0.0:\n",
    "                # Baseline: no steering - run all prompts\n",
    "                for prompt in prompts:\n",
    "                    print(f\"\\nPrompt: {prompt}\")\n",
    "                    \n",
    "                    # Run N times and collect responses\n",
    "                    baseline_responses = []\n",
    "                    for run_idx in range(n_runs):\n",
    "                        if n_runs > 1:\n",
    "                            print(f\"  Run {run_idx + 1}/{n_runs}\")\n",
    "                        \n",
    "                        try:\n",
    "                            response = generate_text(model, tokenizer, prompt)\n",
    "                            baseline_responses.append(response)\n",
    "                            \n",
    "                            if n_runs == 1:\n",
    "                                print(f\"BASELINE: {response}\")\n",
    "                            else:\n",
    "                                print(f\"  BASELINE: {response}\")\n",
    "                        except Exception as e:\n",
    "                            error_msg = f\"Error with baseline: {str(e)}\"\n",
    "                            baseline_responses.append(error_msg)\n",
    "                            print(f\"ERROR: {error_msg}\")\n",
    "                    \n",
    "                    results[prompt][\"steering\"][magnitude] = baseline_responses\n",
    "            else:\n",
    "                # With steering - apply hook once for all prompts at this magnitude\n",
    "                try:\n",
    "                    with ActivationSteering(\n",
    "                        model=model,\n",
    "                        steering_vectors=feature_direction,\n",
    "                        coefficients=magnitude,\n",
    "                        layer_indices=STEERING_LAYER,\n",
    "                        intervention_type=\"addition\",\n",
    "                        positions=\"all\",\n",
    "                        debug=False\n",
    "                    ) as steerer:\n",
    "                        for prompt in prompts:\n",
    "                            print(f\"\\nPrompt: {prompt}\")\n",
    "                            \n",
    "                            # Run N times and collect responses\n",
    "                            steered_responses = []\n",
    "                            for run_idx in range(n_runs):\n",
    "                                if n_runs > 1:\n",
    "                                    print(f\"  Run {run_idx + 1}/{n_runs}\")\n",
    "                                \n",
    "                                try:\n",
    "                                    response = generate_text(model, tokenizer, prompt)\n",
    "                                    steered_responses.append(response)\n",
    "                                    \n",
    "                                    if n_runs == 1:\n",
    "                                        print(f\"STEERED: {response}\")\n",
    "                                    else:\n",
    "                                        print(f\"  STEERED: {response}\")\n",
    "                                except Exception as e:\n",
    "                                    error_msg = f\"Error generating with steering: {str(e)}\"\n",
    "                                    steered_responses.append(error_msg)\n",
    "                                    print(f\"ERROR: {error_msg}\")\n",
    "                            \n",
    "                            results[prompt][\"steering\"][magnitude] = steered_responses\n",
    "                except Exception as e:\n",
    "                    error_msg = f\"Error with magnitude {magnitude}: {str(e)}\"\n",
    "                    print(f\"ERROR: {error_msg}\")\n",
    "                    for prompt in prompts:\n",
    "                        results[prompt][\"steering\"][magnitude] = [error_msg] * n_runs\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run optimized experiments for all features\n",
    "all_results = {}\n",
    "\n",
    "for feature_id in TARGET_FEATURES:\n",
    "    feature_results = run_steering_experiment_optimized(feature_id, prompts, n_runs=N_RUNS_PER_PROMPT, do_steering=False, do_projection_zero_ablate=False)\n",
    "    all_results[feature_id] = feature_results\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"OPTIMIZED STEERING + ABLATION EXPERIMENTS COMPLETE\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‚ Loaded existing data for feature 45426\n",
      "ðŸ’¾ Saved feature 45426 to ./results/7_steering/gemma_trainer131k-l0-114_layer20/45426.json\n"
     ]
    }
   ],
   "source": [
    "def save_results_to_json(results, output_dir):\n",
    "    \"\"\"Save steering and ablation results to separate JSON files per feature\"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    saved_features = []\n",
    "    \n",
    "    # Process each feature\n",
    "    for feature_id, feature_results in results.items():\n",
    "        output_path = os.path.join(output_dir, f\"{feature_id}.json\")\n",
    "        \n",
    "        # Load existing data if file exists\n",
    "        if os.path.exists(output_path):\n",
    "            try:\n",
    "                with open(output_path, 'r', encoding='utf-8') as f:\n",
    "                    feature_obj = json.load(f)\n",
    "                print(f\"ðŸ“‚ Loaded existing data for feature {feature_id}\")\n",
    "            except Exception as e:\n",
    "                print(f\"âš ï¸  Error loading existing file for feature {feature_id}: {e}\")\n",
    "                feature_obj = {\n",
    "                    \"feature_id\": feature_id,\n",
    "                    \"metadata\": {\n",
    "                        \"model_name\": MODEL_NAME,\n",
    "                        \"model_type\": MODEL_TYPE,\n",
    "                        \"sae_layer\": SAE_LAYER,\n",
    "                        \"sae_trainer\": SAE_TRAINER\n",
    "                    },\n",
    "                    \"results\": {}\n",
    "                }\n",
    "        else:\n",
    "            feature_obj = {\n",
    "                \"feature_id\": feature_id,\n",
    "                \"metadata\": {\n",
    "                    \"model_name\": MODEL_NAME,\n",
    "                    \"model_type\": MODEL_TYPE,\n",
    "                    \"sae_layer\": SAE_LAYER,\n",
    "                    \"sae_trainer\": SAE_TRAINER\n",
    "                },\n",
    "                \"results\": {}\n",
    "            }\n",
    "            print(f\"ðŸ†• Creating new file for feature {feature_id}\")\n",
    "        \n",
    "        # Merge prompt results\n",
    "        for prompt, prompt_results in feature_results.items():\n",
    "            # Initialize prompt entry if it doesn't exist\n",
    "            if prompt not in feature_obj[\"results\"]:\n",
    "                feature_obj[\"results\"][prompt] = {\n",
    "                    \"steering\": {},\n",
    "                    \"ablation\": {}\n",
    "                }\n",
    "            \n",
    "            # Handle steering results - merge lists\n",
    "            if \"steering\" in prompt_results:\n",
    "                for magnitude, new_responses in prompt_results[\"steering\"].items():\n",
    "                    magnitude_str = str(magnitude)\n",
    "                    \n",
    "                    # Initialize if doesn't exist\n",
    "                    if magnitude_str not in feature_obj[\"results\"][prompt][\"steering\"]:\n",
    "                        feature_obj[\"results\"][prompt][\"steering\"][magnitude_str] = []\n",
    "                    \n",
    "                    # Convert existing single response to list if needed (backward compatibility)\n",
    "                    if not isinstance(feature_obj[\"results\"][prompt][\"steering\"][magnitude_str], list):\n",
    "                        feature_obj[\"results\"][prompt][\"steering\"][magnitude_str] = [feature_obj[\"results\"][prompt][\"steering\"][magnitude_str]]\n",
    "                    \n",
    "                    # Merge lists\n",
    "                    if isinstance(new_responses, list):\n",
    "                        feature_obj[\"results\"][prompt][\"steering\"][magnitude_str].extend(new_responses)\n",
    "                    else:\n",
    "                        feature_obj[\"results\"][prompt][\"steering\"][magnitude_str].append(new_responses)\n",
    "            \n",
    "            # Handle ablation results - merge lists\n",
    "            if \"ablation\" in prompt_results:\n",
    "                for ablation_type, new_responses in prompt_results[\"ablation\"].items():\n",
    "                    \n",
    "                    # Initialize if doesn't exist\n",
    "                    if ablation_type not in feature_obj[\"results\"][prompt][\"ablation\"]:\n",
    "                        feature_obj[\"results\"][prompt][\"ablation\"][ablation_type] = []\n",
    "                    \n",
    "                    # Convert existing single response to list if needed (backward compatibility)\n",
    "                    if not isinstance(feature_obj[\"results\"][prompt][\"ablation\"][ablation_type], list):\n",
    "                        feature_obj[\"results\"][prompt][\"ablation\"][ablation_type] = [feature_obj[\"results\"][prompt][\"ablation\"][ablation_type]]\n",
    "                    \n",
    "                    # Merge lists\n",
    "                    if isinstance(new_responses, list):\n",
    "                        feature_obj[\"results\"][prompt][\"ablation\"][ablation_type].extend(new_responses)\n",
    "                    else:\n",
    "                        feature_obj[\"results\"][prompt][\"ablation\"][ablation_type].append(new_responses)\n",
    "        \n",
    "        # Save the feature to its own JSON file\n",
    "        with open(output_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(feature_obj, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        saved_features.append(feature_id)\n",
    "        print(f\"ðŸ’¾ Saved feature {feature_id} to {output_path}\")\n",
    "    \n",
    "    return saved_features\n",
    "\n",
    "# Save results to individual JSON files\n",
    "saved_features = save_results_to_json(all_results, OUTPUT_DIR)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
