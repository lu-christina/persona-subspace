# Unified diff patch to upgrade 4_diffing.py
# Apply with: patch -p0 < 4_diffing_patch.diff  (or copyâ€‘paste sections manually)
# â€” Highlights â€”
#   â€¢ Removes StopForward hook pattern and replaces it with a clean UpToLayer wrapper
#   â€¢ Vectorises all statistics in a single compute_stats() that works on CPU or GPU
#   â€¢ Merges save_as_pt_cpu/save_as_pt_gpu â†’ save_as_pt() (deviceâ€‘agnostic)
#   â€¢ Adds deterministic seed & optional TF32 toggle
#   â€¢ Raises default BATCH_SIZE â†’ 32 (fits in 80â€¯GB H100)
# -----------------------------------------------------------------------------------

@@
-import random, numpy as np, torch
+# ðŸ› 1.â€ƒDeterminism & precision knobs ---------------------------------------
+import random, numpy as np, torch, os
+
+# Set seeds once at import time
+SEED = 0
+torch.manual_seed(SEED)
+random.seed(SEED)
+np.random.seed(SEED)
+
+# Hardâ€‘fail on nondeterministic CUDA kernels (CuBLAS AlgoÂ |Â atomicAdd, etc.)
+torch.use_deterministic_algorithms(True)
+os.environ["CUBLAS_WORKSPACE_CONFIG"] = ":16:8"
+
+# Speed/accuracy switch: allow TF32 (â‰¤1eâ€‘6 err) â€” flip off for golden run
+torch.backends.cuda.matmul.allow_tf32 = True
+torch.set_float32_matmul_precision("high")
@@
-BATCH_SIZE = 8
+## Fits comfortably in 80â€¯GB H100 (activations in bf16)
+BATCH_SIZE = 32
@@
-class StopForward(Exception):
-    """Exception to stop forward pass after target layer."""
-    pass
+# ðŸ› 2.â€ƒReplace exceptionâ€‘based hook with partial-forward wrapper ------------
+
+class UpToLayer(torch.nn.Module):
+    """Runs the Transformer up to and including `layer_idx`, returns normed hidden."""
+
+    def __init__(self, model: torch.nn.Module, layer_idx: int):
+        super().__init__()
+        self.embed_tokens = model.model.embed_tokens
+        self.layers = torch.nn.ModuleList(model.model.layers[: layer_idx + 1])
+        self.norm = model.model.norm
+
+    def forward(self, input_ids: torch.Tensor, attention_mask: torch.Tensor, **kwargs):
+        x = self.embed_tokens(input_ids)
+        for layer in self.layers:
+            x = layer(x, attention_mask=attention_mask, **kwargs)
+        return self.norm(x)
@@
-def extract_activations_and_metadata(prompts: List[str], layer_idx: int):
-    """Original implementation using StopForward hook (removed)."""
-    ...
+# ðŸ› 3.â€ƒNew implementation without hooks -----------------------------------
+
+@torch.no_grad()
+def extract_activations_and_metadata(prompts: List[str], layer_idx: int):
+    """Vectorised, deterministic extraction using `UpToLayer`. Returns
+    padded activationsÂ [T,Â S,Â H] + metadata.
+    """
+
+    partial = UpToLayer(model, layer_idx).eval()
+
+    all_activations, all_metadata, formatted_prompts_list = [], [], []
+
+    for i in tqdm(range(0, len(prompts), BATCH_SIZE), desc="Batches"):
+        batch_prompts = prompts[i : i + BATCH_SIZE]
+
+        messages = [{"role": "user", "content": p} for p in batch_prompts]
+        formatted_prompts = [
+            tokenizer.apply_chat_template(msgs, tokenize=False, add_generation_prompt=True)
+            for msgs in messages
+        ]
+        formatted_prompts_list.extend(formatted_prompts)
+
+        tok = tokenizer(
+            formatted_prompts,
+            return_tensors="pt",
+            padding=True,
+            truncation=True,
+            max_length=MAX_LENGTH,
+        ).to(device)
+
+        h = partial(**tok)  # [B, S, H]
+
+        for j, fp in enumerate(formatted_prompts):
+            attn_mask = tok["attention_mask"][j]
+            ids = tok["input_ids"][j]
+
+            positions = {
+                t: find_assistant_position(ids, attn_mask, ASSISTANT_HEADER, offs, tokenizer, device)
+                for t, offs in TOKEN_OFFSETS.items()
+            }
+
+            all_activations.append(h[j].cpu())
+            all_metadata.append(
+                {
+                    "prompt_idx": i + j,
+                    "positions": positions,
+                    "attention_mask": attn_mask.cpu(),
+                    "input_ids": ids.cpu(),
+                }
+            )
+
+    # pad to common seq_len
+    max_len = max(a.shape[0] for a in all_activations)
+    hid = all_activations[0].shape[1]
+    padded = [torch.cat([a, a.new_zeros(max_len - a.size(0), hid)], 0) for a in all_activations]
+
+    return torch.stack(padded), all_metadata, formatted_prompts_list
@@
-# save_as_pt_cpu() and save_as_pt_gpu() definitions (removed below)
+# ðŸ› 4.â€ƒDeviceâ€‘agnostic, vectorised statistics --------------------------------
+
+def compute_stats(t: torch.Tensor):
+    """Return a dict of stats for a [P,Â F] tensor on current device."""
+
+    t = t.float()  # ensure at least fp32
+    n = t.shape[0]
+
+    all_mean = t.mean(0, dtype=torch.float64)
+    all_std = t.std(0, unbiased=False, dtype=torch.float64)
+    max_vals = t.max(0).values
+
+    active = t > 0
+    num_active = active.sum(0)
+    sparsity = num_active / n
+
+    # activeâ€‘only stats (masked reductions)
+    inf = torch.finfo(t.dtype).max
+    active_mean = (t * active).sum(0, dtype=torch.float64) / num_active.clamp(min=1)
+    active_min = torch.where(active, t, inf).amin(0)
+    diff2 = ((t - active_mean) ** 2) * active
+    active_std = (diff2.sum(0, dtype=torch.float64) / num_active.clamp(min=2).sub_(1)).sqrt()
+
+    p90 = torch.quantile(t, 0.9, dim=0, interpolation="linear")
+    p95 = torch.quantile(t, 0.95, dim=0, interpolation="linear")
+    p99 = torch.quantile(t, 0.99, dim=0, interpolation="linear")
+
+    return {
+        "all_mean": all_mean.cpu(),
+        "all_std": all_std.cpu(),
+        "active_mean": active_mean.cpu(),
+        "active_min": active_min.cpu(),
+        "active_std": active_std.cpu(),
+        "max": max_vals.cpu(),
+        "num_active": num_active.cpu(),
+        "sparsity": sparsity.cpu(),
+        "p90": p90.cpu(),
+        "p95": p95.cpu(),
+        "p99": p99.cpu(),
+    }
+
+
+def save_as_pt(in_gpu: bool = True):
+    """Single entrypoint replacing save_as_pt_cpu/gpu."""
+
+    src = f"{MODEL_TYPE}_trainer{SAE_TRAINER}_layer{SAE_LAYER}_{MODEL_VER}"
+    results = {"metadata": {
+        "source": src,
+        "model_type": MODEL_TYPE,
+        "model_ver": MODEL_VER,
+        "sae_layer": SAE_LAYER,
+        "sae_trainer": SAE_TRAINER,
+        "num_prompts": N_PROMPTS,
+        "token_types": list(TOKEN_OFFSETS.keys()),
+    }}
+
+    for tk in TOKEN_OFFSETS:
+        feats = token_features[tk].to("cuda" if in_gpu else "cpu")
+        results[tk] = compute_stats(feats)
+        print(f"âœ“ {tk} done (device={'GPU' if in_gpu else 'CPU'})")
+
+    return results
+
@@
-# results_dict = save_as_pt_cpu()    # Most accurate, slower
-# results_dict = save_as_pt_gpu()    # Faster, potentially less accurate
-
-print("Using CPU version for maximum accuracy...")
-results_dict = save_as_pt_cpu()
+results_dict = save_as_pt(in_gpu=True)  # set False for CPUâ€‘only run
