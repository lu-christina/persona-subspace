{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Analysis: Token-level activations for target features\n",
    "\n",
    "This notebook analyzes token-level activations for specific SAE features on prompts, using a two-stage approach:\n",
    "1. Screen all prompts for target feature activation\n",
    "2. Extract detailed token activations only for prompts where target features fire\n",
    "\n",
    "Saves results to `active.jsonl` and `inactive.jsonl`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from dataclasses import dataclass\n",
    "from sae_lens import SAE\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "torch.set_float32_matmul_precision('high')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@dataclass\n",
    "class ModelConfig:\n",
    "    \"\"\"Configuration for model-specific settings\"\"\"\n",
    "    base_model_name: str\n",
    "    chat_model_name: str\n",
    "    hf_release: str  # Reference only - actual loading uses saelens_release/sae_id\n",
    "    assistant_header: str\n",
    "    token_offsets: Dict[str, int]\n",
    "    sae_base_path: str\n",
    "    saelens_release: str  # Template for sae_lens release parameter\n",
    "    sae_id_template: str  # Template for sae_lens sae_id parameter\n",
    "    base_url: str  # Base URL for neuronpedia\n",
    "    \n",
    "    def get_sae_params(self, sae_layer: int, sae_trainer: str) -> Tuple[str, str]:\n",
    "        \"\"\"\n",
    "        Generate SAE lens release and sae_id parameters.\n",
    "        \n",
    "        Args:\n",
    "            sae_layer: Layer number for the SAE\n",
    "            sae_trainer: Trainer identifier for the SAE\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (release, sae_id) for sae_lens.SAE.from_pretrained()\n",
    "        \"\"\"\n",
    "        if self.saelens_release == \"llama_scope_lxr_{trainer}\":\n",
    "            release = self.saelens_release.format(trainer=sae_trainer)\n",
    "            sae_id = self.sae_id_template.format(layer=sae_layer, trainer=sae_trainer)\n",
    "        elif self.saelens_release == \"gemma-scope-9b-pt-res\":\n",
    "            # Parse SAE_TRAINER \"131k-l0-34\" into components for Gemma\n",
    "            parts = sae_trainer.split(\"-\")\n",
    "            width = parts[0]  # \"131k\"\n",
    "            l0_value = parts[2]  # \"34\"\n",
    "            \n",
    "            release = self.saelens_release\n",
    "            sae_id = self.sae_id_template.format(layer=sae_layer, width=width, l0=l0_value)\n",
    "        elif self.saelens_release == \"gemma-scope-9b-pt-res-canonical\":\n",
    "            # Parse SAE_TRAINER \"131k-l0-34\" into components for Gemma\n",
    "            parts = sae_trainer.split(\"-\")\n",
    "            width = parts[0]  # \"131k\"\n",
    "\n",
    "            release = self.saelens_release\n",
    "            sae_id = self.sae_id_template.format(layer=sae_layer, width=width)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown SAE lens release template: {self.saelens_release}\")\n",
    "        \n",
    "        return release, sae_id\n",
    "\n",
    "# Model configurations\n",
    "MODEL_CONFIGS = {\n",
    "    \"llama\": ModelConfig(\n",
    "        base_model_name=\"meta-llama/Llama-3.1-8B\",\n",
    "        chat_model_name=\"meta-llama/Llama-3.1-8B-Instruct\",\n",
    "        hf_release=\"fnlp/Llama3_1-8B-Base-LXR-32x\",\n",
    "        assistant_header=\"<|start_header_id|>assistant<|end_header_id|>\",\n",
    "        token_offsets={\"asst\": -2, \"endheader\": -1, \"newline\": 0},\n",
    "        sae_base_path=\"/workspace/sae/llama-3.1-8b/saes\",\n",
    "        saelens_release=\"llama_scope_lxr_{trainer}\",\n",
    "        sae_id_template=\"l{layer}r_{trainer}\",\n",
    "        base_url=\"https://www.neuronpedia.org/llama-3.1-8b/{layer}-llamascope-res-131k\"\n",
    "    ),\n",
    "    \"gemma\": ModelConfig(\n",
    "        base_model_name=\"google/gemma-2-9b\",\n",
    "        chat_model_name=\"google/gemma-2-9b-it\",\n",
    "        hf_release=\"google/gemma-scope-9b-pt-res/layer_{layer}/width_{width}/average_l0_{l0}\",\n",
    "        assistant_header=\"<start_of_turn>model\",\n",
    "        token_offsets={\"model\": -1, \"newline\": 0},\n",
    "        sae_base_path=\"/workspace/sae/gemma-2-9b/saes\",\n",
    "        saelens_release=\"gemma-scope-9b-pt-res-canonical\",\n",
    "        sae_id_template=\"layer_{layer}/width_{width}/canonical\",\n",
    "        base_url=\"https://www.neuronpedia.org/gemma-2-9b/{layer}-gemmascope-res-131k\"\n",
    "    )\n",
    "}\n",
    "\n",
    "# =============================================================================\n",
    "# MODEL SELECTION - Change this to switch between models\n",
    "# =============================================================================\n",
    "MODEL_TYPE = \"gemma\"  # Options: \"gemma\" or \"llama\"\n",
    "MODEL_VER = \"chat\"\n",
    "SAE_LAYER = 20\n",
    "SAE_TRAINER = \"131k-l0-114\"\n",
    "N_PROMPTS = 1000\n",
    "\n",
    "# =============================================================================\n",
    "# TARGET FEATURES - Specify which features to analyze\n",
    "# =============================================================================\n",
    "ACTIVATION_THRESHOLD = 0.0  # Minimum activation to consider \"active\"\n",
    "\n",
    "# =============================================================================\n",
    "# DEDUPLICATION AND CONFIGURATION SETUP\n",
    "# =============================================================================\n",
    "if MODEL_TYPE not in MODEL_CONFIGS:\n",
    "    raise ValueError(f\"Unknown MODEL_TYPE: {MODEL_TYPE}. Available: {list(MODEL_CONFIGS.keys())}\")\n",
    "\n",
    "config = MODEL_CONFIGS[MODEL_TYPE]\n",
    "# Set model name based on version\n",
    "if MODEL_VER == \"chat\":\n",
    "    MODEL_NAME = config.chat_model_name\n",
    "elif MODEL_VER == \"base\":\n",
    "    MODEL_NAME = config.base_model_name\n",
    "else:\n",
    "    raise ValueError(f\"Unknown MODEL_VER: {MODEL_VER}. Use 'chat' or 'base'\")\n",
    "\n",
    "# Always use chat model for tokenizer (has chat template)\n",
    "CHAT_MODEL_NAME = config.chat_model_name\n",
    "\n",
    "# Set up derived configurations\n",
    "ASSISTANT_HEADER = config.assistant_header\n",
    "TOKEN_OFFSETS = config.token_offsets\n",
    "SAE_BASE_PATH = config.sae_base_path\n",
    "\n",
    "# =============================================================================\n",
    "# OUTPUT FILE CONFIGURATION\n",
    "# =============================================================================\n",
    "# Base output directory - individual feature directories will be created under this\n",
    "BASE_OUTPUT_DIR = f\"./results/6_active_prompts/{MODEL_TYPE}_trainer{SAE_TRAINER}_layer{SAE_LAYER}/{N_PROMPTS}_prompts\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Will process 5 new features: [69711, 76952, 103126, 78349, 52115]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# LOAD FEATURES FROM FILE\n",
    "# FEATURE_FILE = \"./results/4_diffing/gemma_trainer131k-l0-114_layer20/1000_prompts/top_all_mean.csv\"\n",
    "# df = pd.read_csv(FEATURE_FILE)\n",
    "# TARGET_FEATURES = df['feature_id'].tolist()\n",
    "TARGET_FEATURES = [69711,76952,103126,78349,52115]\n",
    "\n",
    "# =============================================================================\n",
    "# CHECK FOR EXISTING DIRECTORIES AND FILTER TARGET FEATURES\n",
    "# =============================================================================\n",
    "original_target_features = TARGET_FEATURES.copy()\n",
    "filtered_target_features = []\n",
    "existing_features = []\n",
    "\n",
    "for feature_id in TARGET_FEATURES:\n",
    "    feature_dir = f\"{BASE_OUTPUT_DIR}/{feature_id}\"\n",
    "    if os.path.exists(feature_dir):\n",
    "        existing_features.append(feature_id)\n",
    "        print(f\"âš ï¸  WARNING: Directory already exists for feature {feature_id}, skipping: {feature_dir}\")\n",
    "    else:\n",
    "        filtered_target_features.append(feature_id)\n",
    "\n",
    "# Update TARGET_FEATURES to only include features that don't have existing directories\n",
    "TARGET_FEATURES = filtered_target_features\n",
    "\n",
    "if existing_features:\n",
    "    print(f\"\\nðŸ”„ Skipped {len(existing_features)} existing features: {existing_features}\")\n",
    "    \n",
    "if not TARGET_FEATURES:\n",
    "    print(f\"\\nâŒ No new features to process - all {len(original_target_features)} features already have existing directories!\")\n",
    "    print(\"To reprocess existing features, delete their directories first.\")\n",
    "else:\n",
    "    print(f\"\\nâœ… Will process {len(TARGET_FEATURES)} new features: {TARGET_FEATURES}\")\n",
    "\n",
    "\n",
    "# Deduplicate TARGET_FEATURES\n",
    "original_count = len(TARGET_FEATURES)\n",
    "TARGET_FEATURES = list(set(TARGET_FEATURES))\n",
    "duplicates_removed = original_count - len(TARGET_FEATURES)\n",
    "\n",
    "if duplicates_removed > 0:\n",
    "    print(f\"ðŸ”„ Removed {duplicates_removed} duplicate feature(s) from TARGET_FEATURES\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Configuration Summary:\n",
      "  Model Type: gemma\n",
      "  Model to load: google/gemma-2-9b-it\n",
      "  SAE Layer: 20, Trainer: 131k-l0-114\n",
      "  Target Features: [78349, 69711, 52115, 103126, 76952]\n",
      "  Activation Threshold: 0.0\n",
      "  Base Output Directory: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# DERIVED CONFIGURATIONS\n",
    "# =============================================================================\n",
    "SAE_PATH = f\"{SAE_BASE_PATH}/resid_post_layer_{SAE_LAYER}/trainer_{SAE_TRAINER}\"\n",
    "LAYER_INDEX = SAE_LAYER\n",
    "\n",
    "# Data paths\n",
    "PROMPTS_HF = \"lmsys/lmsys-chat-1m\"\n",
    "SEED = 42\n",
    "PROMPTS_PATH = f\"/workspace/data/{PROMPTS_HF.split('/')[-1]}/chat_{N_PROMPTS}.jsonl\"\n",
    "os.makedirs(os.path.dirname(PROMPTS_PATH), exist_ok=True)\n",
    "\n",
    "# Processing parameters\n",
    "BATCH_SIZE = 32\n",
    "MAX_LENGTH = 512\n",
    "\n",
    "# =============================================================================\n",
    "# SUMMARY\n",
    "# =============================================================================\n",
    "print(f\"\\nConfiguration Summary:\")\n",
    "print(f\"  Model Type: {MODEL_TYPE}\")\n",
    "print(f\"  Model to load: {MODEL_NAME}\")\n",
    "print(f\"  SAE Layer: {SAE_LAYER}, Trainer: {SAE_TRAINER}\")\n",
    "print(f\"  Target Features: {TARGET_FEATURES}\")\n",
    "print(f\"  Activation Threshold: {ACTIVATION_THRESHOLD}\")\n",
    "print(f\"  Base Output Directory: {BASE_OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompts already exist at /workspace/data/lmsys-chat-1m/chat_1000.jsonl\n",
      "Loaded 1000 prompts\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def load_lmsys_prompts(prompts_path: str, prompts_hf: str, n_prompts: int, seed: int) -> pd.DataFrame:\n",
    "    # Check if prompts_path exists\n",
    "    if os.path.exists(prompts_path):\n",
    "        print(f\"Prompts already exist at {prompts_path}\")\n",
    "        return pd.read_json(prompts_path, lines=True)\n",
    "    else:\n",
    "        print(f\"Prompts do not exist at {prompts_path}. Loading from {prompts_hf}...\")\n",
    "        dataset = load_dataset(prompts_hf)\n",
    "        dataset = dataset['train'].shuffle(seed=seed).select(range(n_prompts))\n",
    "        df = dataset.to_pandas()\n",
    "\n",
    "        # Extract the prompt from the first conversation item\n",
    "        df['prompt'] = df['conversation'].apply(lambda x: x[0]['content'])\n",
    "\n",
    "        # Only keep some columns\n",
    "        df = df[['conversation_id', 'prompt', 'redacted', 'language']]\n",
    "\n",
    "        # Save to .jsonl file\n",
    "        df.to_json(prompts_path, orient='records', lines=True)\n",
    "        return df\n",
    "\n",
    "prompts_df = load_lmsys_prompts(PROMPTS_PATH, PROMPTS_HF, N_PROMPTS, SEED)\n",
    "print(f\"Loaded {prompts_df.shape[0]} prompts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model and SAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer loaded: GemmaTokenizerFast\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "print(f\"Tokenizer loaded: {tokenizer.__class__.__name__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ca3463a128846269776358cf34238ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded: Gemma2ForCausalLM\n",
      "Model device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map={\"\":0}\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "print(f\"Model loaded: {model.__class__.__name__}\")\n",
    "print(f\"Model device: {next(model.parameters()).device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading SAE from /workspace/sae/gemma-2-9b/saes/resid_post_layer_20/trainer_131k-l0-114\n",
      "âœ“ Found SAE files at: /workspace/sae/gemma-2-9b/saes/resid_post_layer_20/trainer_131k-l0-114\n",
      "SAE loaded with 131072 features\n",
      "SAE device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "def load_sae(config: ModelConfig, sae_path: str, sae_layer: int, sae_trainer: str) -> SAE:\n",
    "    \"\"\"\n",
    "    Unified SAE loading function that handles both Llama and Gemma models.\n",
    "    \n",
    "    Args:\n",
    "        config: ModelConfig object containing model-specific settings\n",
    "        sae_path: Local path to store/load SAE files\n",
    "        sae_layer: Layer number for the SAE\n",
    "        sae_trainer: Trainer identifier for the SAE\n",
    "    \n",
    "    Returns:\n",
    "        SAE: Loaded SAE model\n",
    "    \"\"\"\n",
    "    # Check if SAE file exists locally\n",
    "    print(f\"Loading SAE from {sae_path}\")\n",
    "    ae_file_path = os.path.join(sae_path, \"sae_weights.safetensors\")\n",
    "    \n",
    "    if os.path.exists(ae_file_path):\n",
    "        print(f\"âœ“ Found SAE files at: {os.path.dirname(ae_file_path)}\")\n",
    "        sae = SAE.load_from_disk(sae_path)\n",
    "        return sae\n",
    "    \n",
    "    print(f\"SAE not found locally, downloading from HF via sae_lens...\")\n",
    "    os.makedirs(os.path.dirname(sae_path), exist_ok=True)\n",
    "    \n",
    "    # Get SAE parameters from config\n",
    "    release, sae_id = config.get_sae_params(sae_layer, sae_trainer)\n",
    "    print(f\"Loading SAE with release='{release}', sae_id='{sae_id}'\")\n",
    "    \n",
    "    # Load the SAE using sae_lens\n",
    "    sae, _, _ = SAE.from_pretrained(\n",
    "        release=release,\n",
    "        sae_id=sae_id,\n",
    "        device=\"cuda\" # Hardcoded because it wants a string\n",
    "    )\n",
    "    \n",
    "    # Save the SAE locally for future use\n",
    "    sae.save_model(sae_path)\n",
    "    return sae\n",
    "\n",
    "# Load SAE using the unified function\n",
    "sae = load_sae(config, SAE_PATH, SAE_LAYER, SAE_TRAINER)\n",
    "sae = sae.to(device)  # Move SAE to GPU\n",
    "\n",
    "print(f\"SAE loaded with {sae.cfg.d_sae} features\")\n",
    "print(f\"SAE device: {next(sae.parameters()).device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two-Stage Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated two-stage processing function defined\n"
     ]
    }
   ],
   "source": [
    "class StopForward(Exception):\n",
    "    \"\"\"Exception to stop forward pass after target layer.\"\"\"\n",
    "    pass\n",
    "\n",
    "@torch.no_grad()\n",
    "def process_prompts_for_features(prompts: List[str], target_features: List[int], \n",
    "                                layer_idx: int, activation_threshold: float = 0.0) -> Dict[int, Tuple[List[Dict], List[Dict]]]:\n",
    "    \"\"\"Two-stage processing: screen for target features, then get detailed tokens for active prompts.\n",
    "    \n",
    "    Returns:\n",
    "        Dict mapping feature_id to (active_prompts, inactive_prompts) for that feature\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize results dictionary for each feature\n",
    "    results = {}\n",
    "    for feature_id in target_features:\n",
    "        results[feature_id] = ([], [])  # (active_prompts, inactive_prompts)\n",
    "    \n",
    "    # Get target layer\n",
    "    target_layer = model.model.layers[layer_idx]\n",
    "    \n",
    "    # Process in batches\n",
    "    for i in tqdm(range(0, len(prompts), BATCH_SIZE), desc=\"Processing prompts\"):\n",
    "        batch_prompts = prompts[i:i+BATCH_SIZE]\n",
    "        \n",
    "        # Format prompts as chat messages\n",
    "        formatted_prompts = []\n",
    "        for prompt in batch_prompts:\n",
    "            messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "            formatted_prompt = tokenizer.apply_chat_template(\n",
    "                messages, tokenize=False, add_generation_prompt=True\n",
    "            )\n",
    "            formatted_prompts.append(formatted_prompt)\n",
    "        \n",
    "        # Tokenize batch\n",
    "        batch_inputs = tokenizer(\n",
    "            formatted_prompts,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=MAX_LENGTH\n",
    "        )\n",
    "        \n",
    "        # Move to device\n",
    "        batch_inputs = {k: v.to(device) for k, v in batch_inputs.items()}\n",
    "        \n",
    "        # Stage 1: Get activations for screening\n",
    "        activations = None\n",
    "        \n",
    "        def hook_fn(module, input, output):\n",
    "            nonlocal activations\n",
    "            activations = output[0] if isinstance(output, tuple) else output\n",
    "            raise StopForward()\n",
    "        \n",
    "        # Register hook\n",
    "        handle = target_layer.register_forward_hook(hook_fn)\n",
    "        \n",
    "        try:\n",
    "            _ = model(**batch_inputs)\n",
    "        except StopForward:\n",
    "            pass\n",
    "        finally:\n",
    "            handle.remove()\n",
    "        \n",
    "        # Apply SAE to get feature activations\n",
    "        batch_size, seq_len, hidden_dim = activations.shape\n",
    "        flat_activations = activations.view(-1, hidden_dim)\n",
    "        \n",
    "        # Process SAE in chunks to avoid memory issues\n",
    "        sae_features = []\n",
    "        for chunk_start in range(0, flat_activations.shape[0], BATCH_SIZE * 8):\n",
    "            chunk_end = min(chunk_start + BATCH_SIZE * 8, flat_activations.shape[0])\n",
    "            chunk_activations = flat_activations[chunk_start:chunk_end]\n",
    "            chunk_features = sae.encode(chunk_activations)\n",
    "            sae_features.append(chunk_features.cpu())\n",
    "        \n",
    "        sae_features = torch.cat(sae_features, dim=0)\n",
    "        sae_features = sae_features.view(batch_size, seq_len, -1)\n",
    "        \n",
    "        # Stage 2: Process each prompt for each target feature separately\n",
    "        for batch_idx, (prompt, formatted_prompt) in enumerate(zip(batch_prompts, formatted_prompts)):\n",
    "            prompt_idx = i + batch_idx\n",
    "            prompt_features = sae_features[batch_idx]  # [seq_len, num_features]\n",
    "            input_ids = batch_inputs['input_ids'][batch_idx].cpu().numpy()\n",
    "            \n",
    "            # Create tokenized prompt (convert input_ids to token strings)\n",
    "            tokenized_prompt = []\n",
    "            for token_id in input_ids:\n",
    "                if token_id != tokenizer.pad_token_id:  # Skip padding tokens\n",
    "                    token_text = tokenizer.decode([int(token_id)])\n",
    "                    tokenized_prompt.append(token_text)\n",
    "            \n",
    "            # Process each target feature separately\n",
    "            for feature_id in target_features:\n",
    "                # Get activations for this specific feature\n",
    "                feature_activations = prompt_features[:, feature_id]  # [seq_len]\n",
    "                max_activation = float(feature_activations.max())\n",
    "                \n",
    "                # Check if this feature is active for this prompt\n",
    "                is_active = max_activation > activation_threshold\n",
    "                \n",
    "                if is_active:\n",
    "                    # Stage 3: Get detailed token analysis for active prompts\n",
    "                    tokens = []\n",
    "                    for pos in range(len(input_ids)):\n",
    "                        if pos >= prompt_features.shape[0]:\n",
    "                            break\n",
    "                            \n",
    "                        token_id = int(input_ids[pos])\n",
    "                        token_text = tokenizer.decode([token_id])\n",
    "                        \n",
    "                        # Get activation for this specific feature at this position\n",
    "                        activation_val = float(prompt_features[pos, feature_id])\n",
    "                        \n",
    "                        if activation_val > 0:  # Only store non-zero activations\n",
    "                            tokens.append({\n",
    "                                'position': pos,\n",
    "                                'token_id': token_id,\n",
    "                                'text': token_text,\n",
    "                                'feature_activation': activation_val\n",
    "                            })\n",
    "                    \n",
    "                    results[feature_id][0].append({\n",
    "                        'prompt_id': prompt_idx,\n",
    "                        'prompt_text': prompt,\n",
    "                        'tokenized_prompt': tokenized_prompt,\n",
    "                        'max_feature_activation': max_activation,\n",
    "                        'tokens': tokens\n",
    "                    })\n",
    "                else:\n",
    "                    # Inactive prompt - just basic info\n",
    "                    results[feature_id][1].append({\n",
    "                        'prompt_id': prompt_idx,\n",
    "                        'prompt_text': prompt,\n",
    "                        'tokenized_prompt': tokenized_prompt,\n",
    "                        'max_feature_activation': max_activation\n",
    "                    })\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"Updated two-stage processing function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process All Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 1000 prompts for 5 target features...\n",
      "Target features: [78349, 69711, 52115, 103126, 76952]\n",
      "Activation threshold: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [06:59<00:00, 13.11s/it]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Processing {len(prompts_df)} prompts for {len(TARGET_FEATURES)} target features...\")\n",
    "print(f\"Target features: {TARGET_FEATURES}\")\n",
    "print(f\"Activation threshold: {ACTIVATION_THRESHOLD}\")\n",
    "\n",
    "# Process all prompts for all features\n",
    "feature_results = process_prompts_for_features(\n",
    "    prompts_df['prompt'].tolist(), \n",
    "    TARGET_FEATURES, \n",
    "    LAYER_INDEX, \n",
    "    ACTIVATION_THRESHOLD\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results by feature:\n",
      "  Feature 78349: 994 active, 6 inactive, 164137 active tokens\n",
      "  Feature 69711: 512 active, 488 inactive, 17179 active tokens\n",
      "  Feature 52115: 430 active, 570 inactive, 21263 active tokens\n",
      "  Feature 103126: 936 active, 64 inactive, 85873 active tokens\n",
      "  Feature 76952: 255 active, 745 inactive, 2549 active tokens\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(f\"\\nResults by feature:\")\n",
    "for feature_id in TARGET_FEATURES:\n",
    "    active_prompts, inactive_prompts = feature_results[feature_id]\n",
    "    total_tokens = sum(len(p['tokens']) for p in active_prompts)\n",
    "    print(f\"  Feature {feature_id}: {len(active_prompts)} active, {len(inactive_prompts)} inactive, {total_tokens} active tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save results for each feature in separate directories\n",
    "# for feature_id in TARGET_FEATURES:\n",
    "#     active_prompts, inactive_prompts = feature_results[feature_id]\n",
    "    \n",
    "#     # Create feature-specific directory\n",
    "#     feature_dir = f\"{BASE_OUTPUT_DIR}/{feature_id}\"\n",
    "#     os.makedirs(feature_dir, exist_ok=True)\n",
    "    \n",
    "#     # Define file paths\n",
    "#     active_file = f\"{feature_dir}/active.jsonl\"\n",
    "#     inactive_file = f\"{feature_dir}/inactive.jsonl\"\n",
    "    \n",
    "#     # Save active prompts\n",
    "#     print(f\"Saving {len(active_prompts)} active prompts for feature {feature_id} to {active_file}\")\n",
    "#     with open(active_file, 'w') as f:\n",
    "#         for prompt in active_prompts:\n",
    "#             f.write(json.dumps(prompt) + '\\n')\n",
    "    \n",
    "#     # Save inactive prompts\n",
    "#     print(f\"Saving {len(inactive_prompts)} inactive prompts for feature {feature_id} to {inactive_file}\")\n",
    "#     with open(inactive_file, 'w') as f:\n",
    "#         for prompt in inactive_prompts:\n",
    "#             f.write(json.dumps(prompt) + '\\n')\n",
    "    \n",
    "#     # Show sample results for this feature\n",
    "#     if active_prompts:\n",
    "#         print(f\"  Sample active prompt for feature {feature_id}:\")\n",
    "#         sample = active_prompts[0]\n",
    "#         print(f\"    Prompt: {sample['prompt_text'][:100]}...\")\n",
    "#         print(f\"    Max activation: {sample['max_feature_activation']}\")\n",
    "#         print(f\"    Active tokens: {len(sample['tokens'])}\")\n",
    "#         if sample['tokens']:\n",
    "#             top_token = max(sample['tokens'], key=lambda x: x['feature_activation'])\n",
    "#             print(f\"    Top token: '{top_token['text']}' (position {top_token['position']})\")\n",
    "#             print(f\"    Token activation: {top_token['feature_activation']}\")\n",
    "#         print()\n",
    "\n",
    "# print(f\"âœ“ Results saved successfully for all {len(TARGET_FEATURES)} features!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated token-specific filtering functions defined\n"
     ]
    }
   ],
   "source": [
    "def find_assistant_position(input_ids: torch.Tensor, attention_mask: torch.Tensor, \n",
    "                          assistant_header: str, token_offset: int, tokenizer, device) -> int:\n",
    "    \"\"\"Find the position of the assistant token based on the given offset.\"\"\"\n",
    "    # Find assistant header position\n",
    "    assistant_tokens = tokenizer.encode(assistant_header, add_special_tokens=False)\n",
    "    \n",
    "    # Find where assistant section starts\n",
    "    assistant_pos = None\n",
    "    for k in range(len(input_ids) - len(assistant_tokens) + 1):\n",
    "        if torch.equal(input_ids[k:k+len(assistant_tokens)], torch.tensor(assistant_tokens, device=input_ids.device)):\n",
    "            assistant_pos = k + len(assistant_tokens) + token_offset\n",
    "            break\n",
    "    \n",
    "    if assistant_pos is None:\n",
    "        # Fallback to last non-padding token\n",
    "        assistant_pos = attention_mask.sum().item() - 1\n",
    "    \n",
    "    # Ensure position is within bounds\n",
    "    max_pos = attention_mask.sum().item() - 1\n",
    "    assistant_pos = min(assistant_pos, max_pos)\n",
    "    assistant_pos = max(assistant_pos, 0)\n",
    "    \n",
    "    return int(assistant_pos)\n",
    "\n",
    "@torch.no_grad()\n",
    "def filter_by_token_position(active_prompts: List[Dict], inactive_prompts: List[Dict], \n",
    "                           token_type: str, token_offset: int, feature_id: int,\n",
    "                           activation_threshold: float = 0.0) -> Tuple[List[Dict], List[Dict]]:\n",
    "    \"\"\"Filter prompts based on whether target feature is active at specific token positions.\"\"\"\n",
    "    \n",
    "    token_active = []\n",
    "    token_inactive = []\n",
    "    \n",
    "    # Process all prompts (both active and inactive from main analysis)\n",
    "    all_prompts = active_prompts + inactive_prompts\n",
    "    \n",
    "    for prompt_data in all_prompts:\n",
    "        prompt_text = prompt_data['prompt_text']\n",
    "        \n",
    "        # Use existing tokenized_prompt if available, otherwise tokenize\n",
    "        if 'tokenized_prompt' in prompt_data:\n",
    "            tokenized_prompt = prompt_data['tokenized_prompt']\n",
    "        else:\n",
    "            # Format as chat message to match processing\n",
    "            messages = [{\"role\": \"user\", \"content\": prompt_text}]\n",
    "            formatted_prompt = tokenizer.apply_chat_template(\n",
    "                messages, tokenize=False, add_generation_prompt=True\n",
    "            )\n",
    "            \n",
    "            # Tokenize to get input_ids and attention_mask\n",
    "            inputs = tokenizer(\n",
    "                formatted_prompt,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=MAX_LENGTH\n",
    "            )\n",
    "            \n",
    "            input_ids = inputs['input_ids'].squeeze(0)\n",
    "            \n",
    "            # Create tokenized prompt\n",
    "            tokenized_prompt = []\n",
    "            for token_id in input_ids:\n",
    "                if token_id != tokenizer.pad_token_id:  # Skip padding tokens\n",
    "                    token_text = tokenizer.decode([int(token_id)])\n",
    "                    tokenized_prompt.append(token_text)\n",
    "        \n",
    "        # For finding token position, we still need input_ids and attention_mask\n",
    "        messages = [{\"role\": \"user\", \"content\": prompt_text}]\n",
    "        formatted_prompt = tokenizer.apply_chat_template(\n",
    "            messages, tokenize=False, add_generation_prompt=True\n",
    "        )\n",
    "        \n",
    "        inputs = tokenizer(\n",
    "            formatted_prompt,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=MAX_LENGTH\n",
    "        )\n",
    "        \n",
    "        input_ids = inputs['input_ids'].squeeze(0)\n",
    "        attention_mask = inputs['attention_mask'].squeeze(0)\n",
    "        \n",
    "        # Find the specific token position\n",
    "        target_position = find_assistant_position(\n",
    "            input_ids, attention_mask, ASSISTANT_HEADER, token_offset, tokenizer, device\n",
    "        )\n",
    "        \n",
    "        # Check if this prompt has token data at the target position\n",
    "        has_activation_at_position = False\n",
    "        max_activation_at_position = 0.0\n",
    "        \n",
    "        # Check if this was an active prompt with token details\n",
    "        if 'tokens' in prompt_data:\n",
    "            for token_data in prompt_data['tokens']:\n",
    "                if token_data['position'] == target_position:\n",
    "                    activation = token_data['feature_activation']\n",
    "                    if activation > activation_threshold:\n",
    "                        has_activation_at_position = True\n",
    "                        max_activation_at_position = max(max_activation_at_position, activation)\n",
    "        \n",
    "        # Create token-specific record\n",
    "        token_record = {\n",
    "            'prompt_id': prompt_data['prompt_id'],\n",
    "            'prompt_text': prompt_data['prompt_text'],\n",
    "            'tokenized_prompt': tokenized_prompt,\n",
    "            'token_type': token_type,\n",
    "            'token_position': target_position,\n",
    "            'max_feature_activation': prompt_data['max_feature_activation']\n",
    "        }\n",
    "        \n",
    "        if has_activation_at_position:\n",
    "            token_record['max_activation_at_position'] = max_activation_at_position\n",
    "            # Include token details for this specific position\n",
    "            position_tokens = []\n",
    "            if 'tokens' in prompt_data:\n",
    "                for token_data in prompt_data['tokens']:\n",
    "                    if token_data['position'] == target_position:\n",
    "                        position_tokens.append(token_data)\n",
    "            token_record['position_tokens'] = position_tokens\n",
    "            token_active.append(token_record)\n",
    "        else:\n",
    "            token_record['max_activation_at_position'] = max_activation_at_position\n",
    "            token_inactive.append(token_record)\n",
    "    \n",
    "    return token_active, token_inactive\n",
    "\n",
    "print(\"Updated token-specific filtering functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add New Features \n",
    "\n",
    "Convert the processed features to optimized format and merge them into the existing consolidated bundle for the web viewer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Process token-specific analysis for each feature and token type\n",
    "# print(f\"Processing token-specific analysis for {len(TARGET_FEATURES)} features and {len(TOKEN_OFFSETS)} token types...\")\n",
    "\n",
    "# for feature_id in TARGET_FEATURES:\n",
    "#     active_prompts, inactive_prompts = feature_results[feature_id]\n",
    "#     feature_dir = f\"{BASE_OUTPUT_DIR}/{feature_id}\"\n",
    "    \n",
    "#     print(f\"\\nProcessing feature {feature_id}:\")\n",
    "    \n",
    "#     for token_type, token_offset in TOKEN_OFFSETS.items():\n",
    "#         print(f\"  Processing token type: {token_type} (offset: {token_offset})\")\n",
    "        \n",
    "#         # Filter prompts based on activation at this specific token position\n",
    "#         token_active, token_inactive = filter_by_token_position(\n",
    "#             active_prompts, inactive_prompts, \n",
    "#             token_type, token_offset, feature_id, ACTIVATION_THRESHOLD\n",
    "#         )\n",
    "        \n",
    "#         # Save results for this token type\n",
    "#         active_file = f\"{feature_dir}/active_{token_type}.jsonl\"\n",
    "#         inactive_file = f\"{feature_dir}/inactive_{token_type}.jsonl\"\n",
    "        \n",
    "#         print(f\"    Active prompts at {token_type} position: {len(token_active)}\")\n",
    "#         print(f\"    Inactive prompts at {token_type} position: {len(token_inactive)}\")\n",
    "        \n",
    "#         # Save active prompts for this token type\n",
    "#         with open(active_file, 'w') as f:\n",
    "#             for prompt in token_active:\n",
    "#                 f.write(json.dumps(prompt) + '\\n')\n",
    "        \n",
    "#         # Save inactive prompts for this token type\n",
    "#         with open(inactive_file, 'w') as f:\n",
    "#             for prompt in token_inactive:\n",
    "#                 f.write(json.dumps(prompt) + '\\n')\n",
    "        \n",
    "#         print(f\"    Saved: {active_file}\")\n",
    "#         print(f\"    Saved: {inactive_file}\")\n",
    "\n",
    "\n",
    "# print(f\"\\nâœ“ Token-specific analysis complete for all features!\")\n",
    "# print(f\"Results saved in feature-specific directories under: {BASE_OUTPUT_DIR}\")\n",
    "# print(f\"Each feature directory contains:\")\n",
    "# print(f\"  - active.jsonl / inactive.jsonl (general)\")\n",
    "# for token_type in TOKEN_OFFSETS.keys():\n",
    "#     print(f\"  - active_{token_type}.jsonl / inactive_{token_type}.jsonl (position-specific)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading existing features index from ./results/6_active_prompts/viewer/features_index.json\n",
      "Current features index contains 80 features\n",
      "No existing prompts bundle found, creating new one\n",
      "Will add/update features: [78349, 69711, 52115, 103126, 76952]\n",
      "Processing feature 78349\n",
      "  Saving feature data to ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/features/78349.json\n",
      "  Added 994 active, 6 inactive prompts\n",
      "  Feature file size: 3834.2 KB\n",
      "Processing feature 69711\n",
      "  Saving feature data to ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/features/69711.json\n",
      "  Added 512 active, 488 inactive prompts\n",
      "  Feature file size: 413.8 KB\n",
      "Processing feature 52115\n",
      "  Saving feature data to ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/features/52115.json\n",
      "  Added 430 active, 570 inactive prompts\n",
      "  Feature file size: 509.1 KB\n",
      "Processing feature 103126\n",
      "  Saving feature data to ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/features/103126.json\n",
      "  Added 936 active, 64 inactive prompts\n",
      "  Feature file size: 2023.5 KB\n",
      "Processing feature 76952\n",
      "  Saving feature data to ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/features/76952.json\n",
      "  Added 255 active, 745 inactive prompts\n",
      "  Feature file size: 70.6 KB\n",
      "\\nFeatures index now contains 85 features\n",
      "Prompts bundle now contains 1000 prompts\n",
      "Saving updated features index to ./results/6_active_prompts/viewer/features_index.json\n",
      "Saving updated prompts to ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/prompts.json\n",
      "\\nUpdate complete!\n",
      "Features index: 2.5 KB\n",
      "Prompts file: 0.6 MB\n",
      "Individual features: 85 files (125.3 MB total)\n",
      "Total size: 125.9 MB\n",
      "Total features: 85\n",
      "Total prompts: 1000\n",
      "\\nâœ“ New features are now available in the web viewer with per-feature lazy loading!\n",
      "âœ“ Initial page load: 2.5 KB (features index only)\n",
      "âœ“ Per-feature load: ~1509.2 KB average\n"
     ]
    }
   ],
   "source": [
    "# Load existing features index and prompts data\n",
    "features_index_path = \"./results/6_active_prompts/viewer/features_index.json\"\n",
    "prompts_path = f\"./results/6_active_prompts/{MODEL_TYPE}_trainer{SAE_TRAINER}_layer{SAE_LAYER}/1000_prompts/prompts.json\"\n",
    "features_dir = f\"./results/6_active_prompts/{MODEL_TYPE}_trainer{SAE_TRAINER}_layer{SAE_LAYER}/1000_prompts/features\"\n",
    "\n",
    "# Load existing data\n",
    "if os.path.exists(features_index_path):\n",
    "    print(f\"Loading existing features index from {features_index_path}\")\n",
    "    with open(features_index_path, 'r', encoding='utf-8') as f:\n",
    "        features_index = json.load(f)\n",
    "    existing_features = set(f['id'] for f in features_index['features'])\n",
    "    print(f\"Current features index contains {features_index['metadata']['total_features']} features\")\n",
    "else:\n",
    "    print(\"No existing features index found, creating new one\")\n",
    "    features_index = {\n",
    "        'features': [],\n",
    "        'metadata': {\n",
    "            'total_prompts': 1000,\n",
    "            'total_features': 0,\n",
    "            'generated_by': 'notebook_integration',\n",
    "            'structure_version': '2.0'\n",
    "        }\n",
    "    }\n",
    "    existing_features = set()\n",
    "\n",
    "if os.path.exists(prompts_path):\n",
    "    print(f\"Loading existing prompts from {prompts_path}\")\n",
    "    with open(prompts_path, 'r', encoding='utf-8') as f:\n",
    "        prompts_bundle = json.load(f)\n",
    "    print(f\"Current prompts bundle contains {len(prompts_bundle)} prompts\")\n",
    "else:\n",
    "    print(\"No existing prompts bundle found, creating new one\")\n",
    "    prompts_bundle = {}\n",
    "\n",
    "# Create features directory if it doesn't exist\n",
    "os.makedirs(features_dir, exist_ok=True)\n",
    "\n",
    "print(f\"Will add/update features: {TARGET_FEATURES}\")\n",
    "\n",
    "# Convert feature_results directly to optimized format and save individual files\n",
    "for feature_id in TARGET_FEATURES:\n",
    "    print(f\"Processing feature {feature_id}\")\n",
    "    \n",
    "    active_prompts, inactive_prompts = feature_results[feature_id]\n",
    "    \n",
    "    # Convert to optimized format - same as consolidate_data.py but without text/act fields\n",
    "    feature_data = {\n",
    "        'active': {'all': [], 'model': [], 'newline': []},\n",
    "        'inactive': {'all': [], 'model': [], 'newline': []}\n",
    "    }\n",
    "    \n",
    "    # Process active prompts (general - all tokens)\n",
    "    for prompt in active_prompts:\n",
    "        # Apply same filtering: only prompts with significant activation > 0.1\n",
    "        max_activation = prompt['max_feature_activation']\n",
    "        if max_activation <= 0.1:\n",
    "            continue\n",
    "            \n",
    "        # Convert to optimized format - no 'act' field\n",
    "        activation_entry = {\n",
    "            'id': prompt['prompt_id']\n",
    "        }\n",
    "        \n",
    "        # Add optimized token data - keep all active tokens, not just top 5\n",
    "        if 'tokens' in prompt and prompt['tokens']:\n",
    "            token_activations = []\n",
    "            for token in prompt['tokens']:\n",
    "                token_activation = token['feature_activation']\n",
    "                if token_activation > 0:  # Save all active tokens\n",
    "                    token_activations.append({\n",
    "                        'pos': token['position'],\n",
    "                        'act': round(token_activation, 3)\n",
    "                    })\n",
    "            \n",
    "            # Sort by activation \n",
    "            if token_activations:\n",
    "                token_activations.sort(key=lambda x: x['act'], reverse=True)\n",
    "                activation_entry['tokens'] = token_activations\n",
    "        \n",
    "        feature_data['active']['all'].append(activation_entry)\n",
    "        \n",
    "        # Store prompt data (deduplicated) - no 'text' field\n",
    "        prompt_id = prompt['prompt_id']\n",
    "        if str(prompt_id) not in prompts_bundle:\n",
    "            tokenized = prompt.get('tokenized_prompt', [])\n",
    "            prompts_bundle[str(prompt_id)] = {\n",
    "                'tokens': tokenized,\n",
    "                'len': len(tokenized)\n",
    "            }\n",
    "    \n",
    "    # Process inactive prompts (simplified - no token details needed)\n",
    "    for prompt in inactive_prompts:\n",
    "        activation_entry = {\n",
    "            'id': prompt['prompt_id']\n",
    "        }\n",
    "        feature_data['inactive']['all'].append(activation_entry)\n",
    "        \n",
    "        # Store prompt data for inactive prompts too\n",
    "        prompt_id = prompt['prompt_id']\n",
    "        if str(prompt_id) not in prompts_bundle:\n",
    "            tokenized = prompt.get('tokenized_prompt', [])\n",
    "            prompts_bundle[str(prompt_id)] = {\n",
    "                'tokens': tokenized,\n",
    "                'len': len(tokenized)\n",
    "            }\n",
    "    \n",
    "    # Save individual feature file\n",
    "    feature_file_path = f\"{features_dir}/{feature_id}.json\"\n",
    "    print(f\"  Saving feature data to {feature_file_path}\")\n",
    "    with open(feature_file_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(feature_data, f, separators=(',', ':'))  # Compact format\n",
    "    \n",
    "    # Get file size for index\n",
    "    feature_size = os.path.getsize(feature_file_path)\n",
    "    \n",
    "    # Update features index\n",
    "    feature_entry = {'id': str(feature_id), 'size': feature_size}\n",
    "    \n",
    "    # Remove existing entry if it exists, then add new one\n",
    "    features_index['features'] = [f for f in features_index['features'] if f['id'] != str(feature_id)]\n",
    "    features_index['features'].append(feature_entry)\n",
    "    \n",
    "    print(f\"  Added {len(feature_data['active']['all'])} active, {len(feature_data['inactive']['all'])} inactive prompts\")\n",
    "    print(f\"  Feature file size: {feature_size / 1024:.1f} KB\")\n",
    "\n",
    "# Sort features index by ID\n",
    "features_index['features'].sort(key=lambda x: int(x['id']))\n",
    "\n",
    "# Update metadata\n",
    "features_index['metadata']['total_features'] = len(features_index['features'])\n",
    "features_index['metadata']['total_prompts'] = len(prompts_bundle)\n",
    "\n",
    "print(f\"\\\\nFeatures index now contains {features_index['metadata']['total_features']} features\")\n",
    "print(f\"Prompts bundle now contains {len(prompts_bundle)} prompts\")\n",
    "\n",
    "# Save updated files\n",
    "print(f\"Saving updated features index to {features_index_path}\")\n",
    "with open(features_index_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(features_index, f, separators=(',', ':'))  # Compact format\n",
    "\n",
    "print(f\"Saving updated prompts to {prompts_path}\")\n",
    "with open(prompts_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(prompts_bundle, f, separators=(',', ':'))  # Compact format\n",
    "\n",
    "# Check file sizes\n",
    "features_index_size = os.path.getsize(features_index_path)\n",
    "prompts_size = os.path.getsize(prompts_path)\n",
    "total_features_size = sum(f['size'] for f in features_index['features'])\n",
    "total_size = features_index_size + prompts_size + total_features_size\n",
    "\n",
    "print(f\"\\\\nUpdate complete!\")\n",
    "print(f\"Features index: {features_index_size / 1024:.1f} KB\")\n",
    "print(f\"Prompts file: {prompts_size / 1024 / 1024:.1f} MB\")\n",
    "print(f\"Individual features: {len(features_index['features'])} files ({total_features_size / 1024 / 1024:.1f} MB total)\")\n",
    "print(f\"Total size: {total_size / 1024 / 1024:.1f} MB\")\n",
    "print(f\"Total features: {features_index['metadata']['total_features']}\")\n",
    "print(f\"Total prompts: {len(prompts_bundle)}\")\n",
    "\n",
    "print(\"\\\\nâœ“ New features are now available in the web viewer with per-feature lazy loading!\")\n",
    "print(f\"âœ“ Initial page load: {features_index_size / 1024:.1f} KB (features index only)\")\n",
    "print(f\"âœ“ Per-feature load: ~{total_features_size / len(features_index['features']) / 1024:.1f} KB average\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
