{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Analysis: Token-level activations for target features\n",
    "\n",
    "This notebook analyzes token-level activations for specific SAE features on prompts, using a two-stage approach:\n",
    "1. Screen all prompts for target feature activation\n",
    "2. Extract detailed token activations only for prompts where target features fire\n",
    "\n",
    "Saves results to `active.jsonl` and `inactive.jsonl`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import json\n",
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple, Optional, Set\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from huggingface_hub import hf_hub_download\n",
    "from dictionary_learning.utils import load_dictionary\n",
    "from tqdm.auto import tqdm\n",
    "from sae_lens import SAE\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Dict, Tuple, Optional\n",
    "\n",
    "@dataclass\n",
    "class ModelConfig:\n",
    "    \"\"\"Configuration for model-specific settings\"\"\"\n",
    "    base_model_name: str\n",
    "    chat_model_name: str\n",
    "    hf_release: str  # Reference only - actual loading uses saelens_release/sae_id\n",
    "    assistant_header: str\n",
    "    token_offsets: Dict[str, int]\n",
    "    sae_base_path: str\n",
    "    saelens_release: str  # Template for sae_lens release parameter\n",
    "    sae_id_template: str  # Template for sae_lens sae_id parameter\n",
    "    base_url: str  # Base URL for neuronpedia\n",
    "    \n",
    "    def get_sae_params(self, sae_layer: int, sae_trainer: str) -> Tuple[str, str]:\n",
    "        \"\"\"\n",
    "        Generate SAE lens release and sae_id parameters.\n",
    "        \n",
    "        Args:\n",
    "            sae_layer: Layer number for the SAE\n",
    "            sae_trainer: Trainer identifier for the SAE\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (release, sae_id) for sae_lens.SAE.from_pretrained()\n",
    "        \"\"\"\n",
    "        if self.saelens_release == \"llama_scope_lxr_{trainer}\":\n",
    "            release = self.saelens_release.format(trainer=sae_trainer)\n",
    "            sae_id = self.sae_id_template.format(layer=sae_layer, trainer=sae_trainer)\n",
    "        elif self.saelens_release == \"gemma-scope-9b-pt-res\":\n",
    "            # Parse SAE_TRAINER \"131k-l0-34\" into components for Gemma\n",
    "            parts = sae_trainer.split(\"-\")\n",
    "            width = parts[0]  # \"131k\"\n",
    "            l0_value = parts[2]  # \"34\"\n",
    "            \n",
    "            release = self.saelens_release\n",
    "            sae_id = self.sae_id_template.format(layer=sae_layer, width=width, l0=l0_value)\n",
    "        elif self.saelens_release == \"gemma-scope-9b-pt-res-canonical\":\n",
    "            # Parse SAE_TRAINER \"131k-l0-34\" into components for Gemma\n",
    "            parts = sae_trainer.split(\"-\")\n",
    "            width = parts[0]  # \"131k\"\n",
    "\n",
    "            release = self.saelens_release\n",
    "            sae_id = self.sae_id_template.format(layer=sae_layer, width=width)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown SAE lens release template: {self.saelens_release}\")\n",
    "        \n",
    "        return release, sae_id\n",
    "\n",
    "# Model configurations\n",
    "MODEL_CONFIGS = {\n",
    "    \"llama\": ModelConfig(\n",
    "        base_model_name=\"meta-llama/Llama-3.1-8B\",\n",
    "        chat_model_name=\"meta-llama/Llama-3.1-8B-Instruct\",\n",
    "        hf_release=\"fnlp/Llama3_1-8B-Base-LXR-32x\",\n",
    "        assistant_header=\"<|start_header_id|>assistant<|end_header_id|>\",\n",
    "        token_offsets={\"asst\": -2, \"endheader\": -1, \"newline\": 0},\n",
    "        sae_base_path=\"/workspace/sae/llama-3.1-8b/saes\",\n",
    "        saelens_release=\"llama_scope_lxr_{trainer}\",\n",
    "        sae_id_template=\"l{layer}r_{trainer}\",\n",
    "        base_url=\"https://www.neuronpedia.org/llama-3.1-8b/{layer}-llamascope-res-131k\"\n",
    "    ),\n",
    "    \"gemma\": ModelConfig(\n",
    "        base_model_name=\"google/gemma-2-9b\",\n",
    "        chat_model_name=\"google/gemma-2-9b-it\",\n",
    "        hf_release=\"google/gemma-scope-9b-pt-res/layer_{layer}/width_{width}/average_l0_{l0}\",\n",
    "        assistant_header=\"<start_of_turn>model\",\n",
    "        token_offsets={\"model\": -1, \"newline\": 0},\n",
    "        sae_base_path=\"/workspace/sae/gemma-2-9b/saes\",\n",
    "        saelens_release=\"gemma-scope-9b-pt-res-canonical\",\n",
    "        sae_id_template=\"layer_{layer}/width_{width}/canonical\",\n",
    "        base_url=\"https://www.neuronpedia.org/gemma-2-9b/{layer}-gemmascope-res-131k\"\n",
    "    )\n",
    "}\n",
    "\n",
    "# =============================================================================\n",
    "# MODEL SELECTION - Change this to switch between models\n",
    "# =============================================================================\n",
    "MODEL_TYPE = \"gemma\"  # Options: \"gemma\" or \"llama\"\n",
    "MODEL_VER = \"chat\"\n",
    "SAE_LAYER = 20\n",
    "SAE_TRAINER = \"131k-l0-114\"\n",
    "N_PROMPTS = 1000\n",
    "\n",
    "# =============================================================================\n",
    "# TARGET FEATURES - Specify which features to analyze\n",
    "# =============================================================================\n",
    "ACTIVATION_THRESHOLD = 0.0  # Minimum activation to consider \"active\"\n",
    "\n",
    "# =============================================================================\n",
    "# DEDUPLICATION AND CONFIGURATION SETUP\n",
    "# =============================================================================\n",
    "if MODEL_TYPE not in MODEL_CONFIGS:\n",
    "    raise ValueError(f\"Unknown MODEL_TYPE: {MODEL_TYPE}. Available: {list(MODEL_CONFIGS.keys())}\")\n",
    "\n",
    "config = MODEL_CONFIGS[MODEL_TYPE]\n",
    "# Set model name based on version\n",
    "if MODEL_VER == \"chat\":\n",
    "    MODEL_NAME = config.chat_model_name\n",
    "elif MODEL_VER == \"base\":\n",
    "    MODEL_NAME = config.base_model_name\n",
    "else:\n",
    "    raise ValueError(f\"Unknown MODEL_VER: {MODEL_VER}. Use 'chat' or 'base'\")\n",
    "\n",
    "# Always use chat model for tokenizer (has chat template)\n",
    "CHAT_MODEL_NAME = config.chat_model_name\n",
    "\n",
    "# Set up derived configurations\n",
    "ASSISTANT_HEADER = config.assistant_header\n",
    "TOKEN_OFFSETS = config.token_offsets\n",
    "SAE_BASE_PATH = config.sae_base_path\n",
    "\n",
    "# =============================================================================\n",
    "# OUTPUT FILE CONFIGURATION\n",
    "# =============================================================================\n",
    "# Base output directory - individual feature directories will be created under this\n",
    "BASE_OUTPUT_DIR = f\"./results/6_active_prompts/{MODEL_TYPE}_trainer{SAE_TRAINER}_layer{SAE_LAYER}/{N_PROMPTS}_prompts\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Will process 10 new features: [109889, 83102, 60824, 98462, 118890, 117729, 40273, 58632, 76329, 85940]\n"
     ]
    }
   ],
   "source": [
    "# LOAD FEATURES FROM FILE\n",
    "# FEATURE_FILE = \"./results/4_diffing/gemma_trainer131k-l0-114_layer20/1000_prompts/top_all_mean.csv\"\n",
    "# df = pd.read_csv(FEATURE_FILE)\n",
    "# TARGET_FEATURES = df['feature_id'].tolist()\n",
    "TARGET_FEATURES = [75618,]\n",
    "\n",
    "# =============================================================================\n",
    "# CHECK FOR EXISTING DIRECTORIES AND FILTER TARGET FEATURES\n",
    "# =============================================================================\n",
    "original_target_features = TARGET_FEATURES.copy()\n",
    "filtered_target_features = []\n",
    "existing_features = []\n",
    "\n",
    "for feature_id in TARGET_FEATURES:\n",
    "    feature_dir = f\"{BASE_OUTPUT_DIR}/{feature_id}\"\n",
    "    if os.path.exists(feature_dir):\n",
    "        existing_features.append(feature_id)\n",
    "        print(f\"âš ï¸  WARNING: Directory already exists for feature {feature_id}, skipping: {feature_dir}\")\n",
    "    else:\n",
    "        filtered_target_features.append(feature_id)\n",
    "\n",
    "# Update TARGET_FEATURES to only include features that don't have existing directories\n",
    "TARGET_FEATURES = filtered_target_features\n",
    "\n",
    "if existing_features:\n",
    "    print(f\"\\nðŸ”„ Skipped {len(existing_features)} existing features: {existing_features}\")\n",
    "    \n",
    "if not TARGET_FEATURES:\n",
    "    print(f\"\\nâŒ No new features to process - all {len(original_target_features)} features already have existing directories!\")\n",
    "    print(\"To reprocess existing features, delete their directories first.\")\n",
    "else:\n",
    "    print(f\"\\nâœ… Will process {len(TARGET_FEATURES)} new features: {TARGET_FEATURES}\")\n",
    "\n",
    "\n",
    "# Deduplicate TARGET_FEATURES\n",
    "original_count = len(TARGET_FEATURES)\n",
    "TARGET_FEATURES = list(set(TARGET_FEATURES))\n",
    "duplicates_removed = original_count - len(TARGET_FEATURES)\n",
    "\n",
    "if duplicates_removed > 0:\n",
    "    print(f\"ðŸ”„ Removed {duplicates_removed} duplicate feature(s) from TARGET_FEATURES\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Configuration Summary:\n",
      "  Model Type: gemma\n",
      "  Model to load: google/gemma-2-9b-it\n",
      "  SAE Layer: 20, Trainer: 131k-l0-114\n",
      "  Target Features: [109889, 117729, 58632, 76329, 118890, 40273, 85940, 60824, 83102, 98462]\n",
      "  Activation Threshold: 0.0\n",
      "  Base Output Directory: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# DERIVED CONFIGURATIONS\n",
    "# =============================================================================\n",
    "SAE_PATH = f\"{SAE_BASE_PATH}/resid_post_layer_{SAE_LAYER}/trainer_{SAE_TRAINER}\"\n",
    "LAYER_INDEX = SAE_LAYER\n",
    "\n",
    "# Data paths\n",
    "PROMPTS_HF = \"lmsys/lmsys-chat-1m\"\n",
    "SEED = 42\n",
    "PROMPTS_PATH = f\"/workspace/data/{PROMPTS_HF.split('/')[-1]}/chat_{N_PROMPTS}.jsonl\"\n",
    "os.makedirs(os.path.dirname(PROMPTS_PATH), exist_ok=True)\n",
    "\n",
    "# Processing parameters\n",
    "BATCH_SIZE = 32\n",
    "MAX_LENGTH = 512\n",
    "\n",
    "# =============================================================================\n",
    "# SUMMARY\n",
    "# =============================================================================\n",
    "print(f\"\\nConfiguration Summary:\")\n",
    "print(f\"  Model Type: {MODEL_TYPE}\")\n",
    "print(f\"  Model to load: {MODEL_NAME}\")\n",
    "print(f\"  SAE Layer: {SAE_LAYER}, Trainer: {SAE_TRAINER}\")\n",
    "print(f\"  Target Features: {TARGET_FEATURES}\")\n",
    "print(f\"  Activation Threshold: {ACTIVATION_THRESHOLD}\")\n",
    "print(f\"  Base Output Directory: {BASE_OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompts already exist at /workspace/data/lmsys-chat-1m/chat_1000.jsonl\n",
      "Loaded 1000 prompts\n"
     ]
    }
   ],
   "source": [
    "def load_lmsys_prompts(prompts_path: str, prompts_hf: str, n_prompts: int, seed: int) -> pd.DataFrame:\n",
    "    # Check if prompts_path exists\n",
    "    if os.path.exists(prompts_path):\n",
    "        print(f\"Prompts already exist at {prompts_path}\")\n",
    "        return pd.read_json(prompts_path, lines=True)\n",
    "    else:\n",
    "        print(f\"Prompts do not exist at {prompts_path}. Loading from {prompts_hf}...\")\n",
    "        dataset = load_dataset(prompts_hf)\n",
    "        dataset = dataset['train'].shuffle(seed=seed).select(range(n_prompts))\n",
    "        df = dataset.to_pandas()\n",
    "\n",
    "        # Extract the prompt from the first conversation item\n",
    "        df['prompt'] = df['conversation'].apply(lambda x: x[0]['content'])\n",
    "\n",
    "        # Only keep some columns\n",
    "        df = df[['conversation_id', 'prompt', 'redacted', 'language']]\n",
    "\n",
    "        # Save to .jsonl file\n",
    "        df.to_json(prompts_path, orient='records', lines=True)\n",
    "        return df\n",
    "\n",
    "prompts_df = load_lmsys_prompts(PROMPTS_PATH, PROMPTS_HF, N_PROMPTS, SEED)\n",
    "print(f\"Loaded {prompts_df.shape[0]} prompts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model and SAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer loaded: GemmaTokenizerFast\n"
     ]
    }
   ],
   "source": [
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "print(f\"Tokenizer loaded: {tokenizer.__class__.__name__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b184fb466fa4403da43ab791dd4eac9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded: Gemma2ForCausalLM\n",
      "Model device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Load model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map={\"\":0}\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "print(f\"Model loaded: {model.__class__.__name__}\")\n",
    "print(f\"Model device: {next(model.parameters()).device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading SAE from /workspace/sae/gemma-2-9b/saes/resid_post_layer_20/trainer_131k-l0-114\n",
      "âœ“ Found SAE files at: /workspace/sae/gemma-2-9b/saes/resid_post_layer_20/trainer_131k-l0-114\n",
      "SAE loaded with 131072 features\n",
      "SAE device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "def load_sae(config: ModelConfig, sae_path: str, sae_layer: int, sae_trainer: str) -> SAE:\n",
    "    \"\"\"\n",
    "    Unified SAE loading function that handles both Llama and Gemma models.\n",
    "    \n",
    "    Args:\n",
    "        config: ModelConfig object containing model-specific settings\n",
    "        sae_path: Local path to store/load SAE files\n",
    "        sae_layer: Layer number for the SAE\n",
    "        sae_trainer: Trainer identifier for the SAE\n",
    "    \n",
    "    Returns:\n",
    "        SAE: Loaded SAE model\n",
    "    \"\"\"\n",
    "    # Check if SAE file exists locally\n",
    "    print(f\"Loading SAE from {sae_path}\")\n",
    "    ae_file_path = os.path.join(sae_path, \"sae_weights.safetensors\")\n",
    "    \n",
    "    if os.path.exists(ae_file_path):\n",
    "        print(f\"âœ“ Found SAE files at: {os.path.dirname(ae_file_path)}\")\n",
    "        sae = SAE.load_from_disk(sae_path)\n",
    "        return sae\n",
    "    \n",
    "    print(f\"SAE not found locally, downloading from HF via sae_lens...\")\n",
    "    os.makedirs(os.path.dirname(sae_path), exist_ok=True)\n",
    "    \n",
    "    # Get SAE parameters from config\n",
    "    release, sae_id = config.get_sae_params(sae_layer, sae_trainer)\n",
    "    print(f\"Loading SAE with release='{release}', sae_id='{sae_id}'\")\n",
    "    \n",
    "    # Load the SAE using sae_lens\n",
    "    sae, _, _ = SAE.from_pretrained(\n",
    "        release=release,\n",
    "        sae_id=sae_id,\n",
    "        device=\"cuda\" # Hardcoded because it wants a string\n",
    "    )\n",
    "    \n",
    "    # Save the SAE locally for future use\n",
    "    sae.save_model(sae_path)\n",
    "    return sae\n",
    "\n",
    "# Load SAE using the unified function\n",
    "sae = load_sae(config, SAE_PATH, SAE_LAYER, SAE_TRAINER)\n",
    "sae = sae.to(device)  # Move SAE to GPU\n",
    "\n",
    "print(f\"SAE loaded with {sae.cfg.d_sae} features\")\n",
    "print(f\"SAE device: {next(sae.parameters()).device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two-Stage Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated two-stage processing function defined\n"
     ]
    }
   ],
   "source": [
    "class StopForward(Exception):\n",
    "    \"\"\"Exception to stop forward pass after target layer.\"\"\"\n",
    "    pass\n",
    "\n",
    "@torch.no_grad()\n",
    "def process_prompts_for_features(prompts: List[str], target_features: List[int], \n",
    "                                layer_idx: int, activation_threshold: float = 0.0) -> Dict[int, Tuple[List[Dict], List[Dict]]]:\n",
    "    \"\"\"Two-stage processing: screen for target features, then get detailed tokens for active prompts.\n",
    "    \n",
    "    Returns:\n",
    "        Dict mapping feature_id to (active_prompts, inactive_prompts) for that feature\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize results dictionary for each feature\n",
    "    results = {}\n",
    "    for feature_id in target_features:\n",
    "        results[feature_id] = ([], [])  # (active_prompts, inactive_prompts)\n",
    "    \n",
    "    # Get target layer\n",
    "    target_layer = model.model.layers[layer_idx]\n",
    "    \n",
    "    # Process in batches\n",
    "    for i in tqdm(range(0, len(prompts), BATCH_SIZE), desc=\"Processing prompts\"):\n",
    "        batch_prompts = prompts[i:i+BATCH_SIZE]\n",
    "        \n",
    "        # Format prompts as chat messages\n",
    "        formatted_prompts = []\n",
    "        for prompt in batch_prompts:\n",
    "            messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "            formatted_prompt = tokenizer.apply_chat_template(\n",
    "                messages, tokenize=False, add_generation_prompt=True\n",
    "            )\n",
    "            formatted_prompts.append(formatted_prompt)\n",
    "        \n",
    "        # Tokenize batch\n",
    "        batch_inputs = tokenizer(\n",
    "            formatted_prompts,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=MAX_LENGTH\n",
    "        )\n",
    "        \n",
    "        # Move to device\n",
    "        batch_inputs = {k: v.to(device) for k, v in batch_inputs.items()}\n",
    "        \n",
    "        # Stage 1: Get activations for screening\n",
    "        activations = None\n",
    "        \n",
    "        def hook_fn(module, input, output):\n",
    "            nonlocal activations\n",
    "            activations = output[0] if isinstance(output, tuple) else output\n",
    "            raise StopForward()\n",
    "        \n",
    "        # Register hook\n",
    "        handle = target_layer.register_forward_hook(hook_fn)\n",
    "        \n",
    "        try:\n",
    "            _ = model(**batch_inputs)\n",
    "        except StopForward:\n",
    "            pass\n",
    "        finally:\n",
    "            handle.remove()\n",
    "        \n",
    "        # Apply SAE to get feature activations\n",
    "        batch_size, seq_len, hidden_dim = activations.shape\n",
    "        flat_activations = activations.view(-1, hidden_dim)\n",
    "        \n",
    "        # Process SAE in chunks to avoid memory issues\n",
    "        sae_features = []\n",
    "        for chunk_start in range(0, flat_activations.shape[0], BATCH_SIZE * 8):\n",
    "            chunk_end = min(chunk_start + BATCH_SIZE * 8, flat_activations.shape[0])\n",
    "            chunk_activations = flat_activations[chunk_start:chunk_end]\n",
    "            chunk_features = sae.encode(chunk_activations)\n",
    "            sae_features.append(chunk_features.cpu())\n",
    "        \n",
    "        sae_features = torch.cat(sae_features, dim=0)\n",
    "        sae_features = sae_features.view(batch_size, seq_len, -1)\n",
    "        \n",
    "        # Stage 2: Process each prompt for each target feature separately\n",
    "        for batch_idx, (prompt, formatted_prompt) in enumerate(zip(batch_prompts, formatted_prompts)):\n",
    "            prompt_idx = i + batch_idx\n",
    "            prompt_features = sae_features[batch_idx]  # [seq_len, num_features]\n",
    "            input_ids = batch_inputs['input_ids'][batch_idx].cpu().numpy()\n",
    "            \n",
    "            # Create tokenized prompt (convert input_ids to token strings)\n",
    "            tokenized_prompt = []\n",
    "            for token_id in input_ids:\n",
    "                if token_id != tokenizer.pad_token_id:  # Skip padding tokens\n",
    "                    token_text = tokenizer.decode([int(token_id)])\n",
    "                    tokenized_prompt.append(token_text)\n",
    "            \n",
    "            # Process each target feature separately\n",
    "            for feature_id in target_features:\n",
    "                # Get activations for this specific feature\n",
    "                feature_activations = prompt_features[:, feature_id]  # [seq_len]\n",
    "                max_activation = float(feature_activations.max())\n",
    "                \n",
    "                # Check if this feature is active for this prompt\n",
    "                is_active = max_activation > activation_threshold\n",
    "                \n",
    "                if is_active:\n",
    "                    # Stage 3: Get detailed token analysis for active prompts\n",
    "                    tokens = []\n",
    "                    for pos in range(len(input_ids)):\n",
    "                        if pos >= prompt_features.shape[0]:\n",
    "                            break\n",
    "                            \n",
    "                        token_id = int(input_ids[pos])\n",
    "                        token_text = tokenizer.decode([token_id])\n",
    "                        \n",
    "                        # Get activation for this specific feature at this position\n",
    "                        activation_val = float(prompt_features[pos, feature_id])\n",
    "                        \n",
    "                        if activation_val > 0:  # Only store non-zero activations\n",
    "                            tokens.append({\n",
    "                                'position': pos,\n",
    "                                'token_id': token_id,\n",
    "                                'text': token_text,\n",
    "                                'feature_activation': activation_val\n",
    "                            })\n",
    "                    \n",
    "                    results[feature_id][0].append({\n",
    "                        'prompt_id': prompt_idx,\n",
    "                        'prompt_text': prompt,\n",
    "                        'tokenized_prompt': tokenized_prompt,\n",
    "                        'max_feature_activation': max_activation,\n",
    "                        'tokens': tokens\n",
    "                    })\n",
    "                else:\n",
    "                    # Inactive prompt - just basic info\n",
    "                    results[feature_id][1].append({\n",
    "                        'prompt_id': prompt_idx,\n",
    "                        'prompt_text': prompt,\n",
    "                        'tokenized_prompt': tokenized_prompt,\n",
    "                        'max_feature_activation': max_activation\n",
    "                    })\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"Updated two-stage processing function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process All Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 1000 prompts for 10 target features...\n",
      "Target features: [109889, 117729, 58632, 76329, 118890, 40273, 85940, 60824, 83102, 98462]\n",
      "Activation threshold: 0.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab480d841fbb4d258d17e1f46a8659d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing prompts:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(f\"Processing {len(prompts_df)} prompts for {len(TARGET_FEATURES)} target features...\")\n",
    "print(f\"Target features: {TARGET_FEATURES}\")\n",
    "print(f\"Activation threshold: {ACTIVATION_THRESHOLD}\")\n",
    "\n",
    "# Process all prompts for all features\n",
    "feature_results = process_prompts_for_features(\n",
    "    prompts_df['prompt'].tolist(), \n",
    "    TARGET_FEATURES, \n",
    "    LAYER_INDEX, \n",
    "    ACTIVATION_THRESHOLD\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results by feature:\n",
      "  Feature 109889: 462 active, 538 inactive, 10453 active tokens\n",
      "  Feature 117729: 520 active, 480 inactive, 3277 active tokens\n",
      "  Feature 58632: 280 active, 720 inactive, 1964 active tokens\n",
      "  Feature 76329: 372 active, 628 inactive, 2599 active tokens\n",
      "  Feature 118890: 88 active, 912 inactive, 248 active tokens\n",
      "  Feature 40273: 199 active, 801 inactive, 18629 active tokens\n",
      "  Feature 85940: 222 active, 778 inactive, 1114 active tokens\n",
      "  Feature 60824: 172 active, 828 inactive, 218 active tokens\n",
      "  Feature 83102: 142 active, 858 inactive, 498 active tokens\n",
      "  Feature 98462: 225 active, 775 inactive, 969 active tokens\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(f\"\\nResults by feature:\")\n",
    "for feature_id in TARGET_FEATURES:\n",
    "    active_prompts, inactive_prompts = feature_results[feature_id]\n",
    "    total_tokens = sum(len(p['tokens']) for p in active_prompts)\n",
    "    print(f\"  Feature {feature_id}: {len(active_prompts)} active, {len(inactive_prompts)} inactive, {total_tokens} active tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving 462 active prompts for feature 109889 to ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/109889/active.jsonl\n",
      "Saving 538 inactive prompts for feature 109889 to ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/109889/inactive.jsonl\n",
      "  Sample active prompt for feature 109889:\n",
      "    Prompt: \"How can we improve the effectiveness of our marketing campaigns using AI and machine learning?\" Is ...\n",
      "    Max activation: 4.561880111694336\n",
      "    Active tokens: 1\n",
      "    Top token: ' chatbot' (position 35)\n",
      "    Token activation: 4.561880111694336\n",
      "\n",
      "Saving 520 active prompts for feature 117729 to ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/117729/active.jsonl\n",
      "Saving 480 inactive prompts for feature 117729 to ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/117729/inactive.jsonl\n",
      "  Sample active prompt for feature 117729:\n",
      "    Prompt: Scrivi un testo, su questo tema: Nel mondo digitale in cui viviamo, spesso tendiamo a silenziare chi...\n",
      "    Max activation: 7.68842887878418\n",
      "    Active tokens: 4\n",
      "    Top token: ',' (position 9)\n",
      "    Token activation: 7.68842887878418\n",
      "\n",
      "Saving 280 active prompts for feature 58632 to ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/58632/active.jsonl\n",
      "Saving 720 inactive prompts for feature 58632 to ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/58632/inactive.jsonl\n",
      "  Sample active prompt for feature 58632:\n",
      "    Prompt: Scrivi un testo, su questo tema: Nel mondo digitale in cui viviamo, spesso tendiamo a silenziare chi...\n",
      "    Max activation: 3.6486613750457764\n",
      "    Active tokens: 1\n",
      "    Top token: ' figli' (position 155)\n",
      "    Token activation: 3.6486613750457764\n",
      "\n",
      "Saving 372 active prompts for feature 76329 to ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/76329/active.jsonl\n",
      "Saving 628 inactive prompts for feature 76329 to ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/76329/inactive.jsonl\n",
      "  Sample active prompt for feature 76329:\n",
      "    Prompt: Scrivi un testo, su questo tema: Nel mondo digitale in cui viviamo, spesso tendiamo a silenziare chi...\n",
      "    Max activation: 3.5733494758605957\n",
      "    Active tokens: 1\n",
      "    Top token: 'vi' (position 6)\n",
      "    Token activation: 3.5733494758605957\n",
      "\n",
      "Saving 88 active prompts for feature 118890 to ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/118890/active.jsonl\n",
      "Saving 912 inactive prompts for feature 118890 to ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/118890/inactive.jsonl\n",
      "  Sample active prompt for feature 118890:\n",
      "    Prompt: the table structure is: create table classroom\n",
      " (building varchar(15) comment 'æ¥¼å®‡',\n",
      " room_number var...\n",
      "    Max activation: 5.299915313720703\n",
      "    Active tokens: 2\n",
      "    Top token: ' is' (position 72)\n",
      "    Token activation: 5.299915313720703\n",
      "\n",
      "Saving 199 active prompts for feature 40273 to ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/40273/active.jsonl\n",
      "Saving 801 inactive prompts for feature 40273 to ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/40273/inactive.jsonl\n",
      "  Sample active prompt for feature 40273:\n",
      "    Prompt: You are the text completion model and you must complete the assistant answer below, only send the co...\n",
      "    Max activation: 5.3706159591674805\n",
      "    Active tokens: 1\n",
      "    Top token: 'criptive' (position 64)\n",
      "    Token activation: 5.3706159591674805\n",
      "\n",
      "Saving 222 active prompts for feature 85940 to ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/85940/active.jsonl\n",
      "Saving 778 inactive prompts for feature 85940 to ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/85940/inactive.jsonl\n",
      "  Sample active prompt for feature 85940:\n",
      "    Prompt: Scrivi un testo, su questo tema: Nel mondo digitale in cui viviamo, spesso tendiamo a silenziare chi...\n",
      "    Max activation: 4.013556957244873\n",
      "    Active tokens: 2\n",
      "    Top token: ' figli' (position 155)\n",
      "    Token activation: 4.013556957244873\n",
      "\n",
      "Saving 172 active prompts for feature 60824 to ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/60824/active.jsonl\n",
      "Saving 828 inactive prompts for feature 60824 to ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/60824/inactive.jsonl\n",
      "  Sample active prompt for feature 60824:\n",
      "    Prompt: the table structure is: create table classroom\n",
      " (building varchar(15) comment 'æ¥¼å®‡',\n",
      " room_number var...\n",
      "    Max activation: 4.6209187507629395\n",
      "    Active tokens: 1\n",
      "    Top token: 'model' (position 102)\n",
      "    Token activation: 4.6209187507629395\n",
      "\n",
      "Saving 142 active prompts for feature 83102 to ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/83102/active.jsonl\n",
      "Saving 858 inactive prompts for feature 83102 to ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/83102/inactive.jsonl\n",
      "  Sample active prompt for feature 83102:\n",
      "    Prompt: \"How can we improve the effectiveness of our marketing campaigns using AI and machine learning?\" Is ...\n",
      "    Max activation: 5.986907958984375\n",
      "    Active tokens: 1\n",
      "    Top token: ' AI' (position 17)\n",
      "    Token activation: 5.986907958984375\n",
      "\n",
      "Saving 225 active prompts for feature 98462 to ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/98462/active.jsonl\n",
      "Saving 775 inactive prompts for feature 98462 to ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/98462/inactive.jsonl\n",
      "  Sample active prompt for feature 98462:\n",
      "    Prompt: Scrivi un testo, su questo tema: Nel mondo digitale in cui viviamo, spesso tendiamo a silenziare chi...\n",
      "    Max activation: 3.513031244277954\n",
      "    Active tokens: 1\n",
      "    Top token: ' video' (position 227)\n",
      "    Token activation: 3.513031244277954\n",
      "\n",
      "âœ“ Results saved successfully for all 10 features!\n"
     ]
    }
   ],
   "source": [
    "# Save results for each feature in separate directories\n",
    "for feature_id in TARGET_FEATURES:\n",
    "    active_prompts, inactive_prompts = feature_results[feature_id]\n",
    "    \n",
    "    # Create feature-specific directory\n",
    "    feature_dir = f\"{BASE_OUTPUT_DIR}/{feature_id}\"\n",
    "    os.makedirs(feature_dir, exist_ok=True)\n",
    "    \n",
    "    # Define file paths\n",
    "    active_file = f\"{feature_dir}/active.jsonl\"\n",
    "    inactive_file = f\"{feature_dir}/inactive.jsonl\"\n",
    "    \n",
    "    # Save active prompts\n",
    "    print(f\"Saving {len(active_prompts)} active prompts for feature {feature_id} to {active_file}\")\n",
    "    with open(active_file, 'w') as f:\n",
    "        for prompt in active_prompts:\n",
    "            f.write(json.dumps(prompt) + '\\n')\n",
    "    \n",
    "    # Save inactive prompts\n",
    "    print(f\"Saving {len(inactive_prompts)} inactive prompts for feature {feature_id} to {inactive_file}\")\n",
    "    with open(inactive_file, 'w') as f:\n",
    "        for prompt in inactive_prompts:\n",
    "            f.write(json.dumps(prompt) + '\\n')\n",
    "    \n",
    "    # Show sample results for this feature\n",
    "    if active_prompts:\n",
    "        print(f\"  Sample active prompt for feature {feature_id}:\")\n",
    "        sample = active_prompts[0]\n",
    "        print(f\"    Prompt: {sample['prompt_text'][:100]}...\")\n",
    "        print(f\"    Max activation: {sample['max_feature_activation']}\")\n",
    "        print(f\"    Active tokens: {len(sample['tokens'])}\")\n",
    "        if sample['tokens']:\n",
    "            top_token = max(sample['tokens'], key=lambda x: x['feature_activation'])\n",
    "            print(f\"    Top token: '{top_token['text']}' (position {top_token['position']})\")\n",
    "            print(f\"    Token activation: {top_token['feature_activation']}\")\n",
    "        print()\n",
    "\n",
    "print(f\"âœ“ Results saved successfully for all {len(TARGET_FEATURES)} features!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated token-specific filtering functions defined\n"
     ]
    }
   ],
   "source": [
    "def find_assistant_position(input_ids: torch.Tensor, attention_mask: torch.Tensor, \n",
    "                          assistant_header: str, token_offset: int, tokenizer, device) -> int:\n",
    "    \"\"\"Find the position of the assistant token based on the given offset.\"\"\"\n",
    "    # Find assistant header position\n",
    "    assistant_tokens = tokenizer.encode(assistant_header, add_special_tokens=False)\n",
    "    \n",
    "    # Find where assistant section starts\n",
    "    assistant_pos = None\n",
    "    for k in range(len(input_ids) - len(assistant_tokens) + 1):\n",
    "        if torch.equal(input_ids[k:k+len(assistant_tokens)], torch.tensor(assistant_tokens, device=input_ids.device)):\n",
    "            assistant_pos = k + len(assistant_tokens) + token_offset\n",
    "            break\n",
    "    \n",
    "    if assistant_pos is None:\n",
    "        # Fallback to last non-padding token\n",
    "        assistant_pos = attention_mask.sum().item() - 1\n",
    "    \n",
    "    # Ensure position is within bounds\n",
    "    max_pos = attention_mask.sum().item() - 1\n",
    "    assistant_pos = min(assistant_pos, max_pos)\n",
    "    assistant_pos = max(assistant_pos, 0)\n",
    "    \n",
    "    return int(assistant_pos)\n",
    "\n",
    "@torch.no_grad()\n",
    "def filter_by_token_position(active_prompts: List[Dict], inactive_prompts: List[Dict], \n",
    "                           token_type: str, token_offset: int, feature_id: int,\n",
    "                           activation_threshold: float = 0.0) -> Tuple[List[Dict], List[Dict]]:\n",
    "    \"\"\"Filter prompts based on whether target feature is active at specific token positions.\"\"\"\n",
    "    \n",
    "    token_active = []\n",
    "    token_inactive = []\n",
    "    \n",
    "    # Process all prompts (both active and inactive from main analysis)\n",
    "    all_prompts = active_prompts + inactive_prompts\n",
    "    \n",
    "    for prompt_data in all_prompts:\n",
    "        prompt_text = prompt_data['prompt_text']\n",
    "        \n",
    "        # Use existing tokenized_prompt if available, otherwise tokenize\n",
    "        if 'tokenized_prompt' in prompt_data:\n",
    "            tokenized_prompt = prompt_data['tokenized_prompt']\n",
    "        else:\n",
    "            # Format as chat message to match processing\n",
    "            messages = [{\"role\": \"user\", \"content\": prompt_text}]\n",
    "            formatted_prompt = tokenizer.apply_chat_template(\n",
    "                messages, tokenize=False, add_generation_prompt=True\n",
    "            )\n",
    "            \n",
    "            # Tokenize to get input_ids and attention_mask\n",
    "            inputs = tokenizer(\n",
    "                formatted_prompt,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=MAX_LENGTH\n",
    "            )\n",
    "            \n",
    "            input_ids = inputs['input_ids'].squeeze(0)\n",
    "            \n",
    "            # Create tokenized prompt\n",
    "            tokenized_prompt = []\n",
    "            for token_id in input_ids:\n",
    "                if token_id != tokenizer.pad_token_id:  # Skip padding tokens\n",
    "                    token_text = tokenizer.decode([int(token_id)])\n",
    "                    tokenized_prompt.append(token_text)\n",
    "        \n",
    "        # For finding token position, we still need input_ids and attention_mask\n",
    "        messages = [{\"role\": \"user\", \"content\": prompt_text}]\n",
    "        formatted_prompt = tokenizer.apply_chat_template(\n",
    "            messages, tokenize=False, add_generation_prompt=True\n",
    "        )\n",
    "        \n",
    "        inputs = tokenizer(\n",
    "            formatted_prompt,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=MAX_LENGTH\n",
    "        )\n",
    "        \n",
    "        input_ids = inputs['input_ids'].squeeze(0)\n",
    "        attention_mask = inputs['attention_mask'].squeeze(0)\n",
    "        \n",
    "        # Find the specific token position\n",
    "        target_position = find_assistant_position(\n",
    "            input_ids, attention_mask, ASSISTANT_HEADER, token_offset, tokenizer, device\n",
    "        )\n",
    "        \n",
    "        # Check if this prompt has token data at the target position\n",
    "        has_activation_at_position = False\n",
    "        max_activation_at_position = 0.0\n",
    "        \n",
    "        # Check if this was an active prompt with token details\n",
    "        if 'tokens' in prompt_data:\n",
    "            for token_data in prompt_data['tokens']:\n",
    "                if token_data['position'] == target_position:\n",
    "                    activation = token_data['feature_activation']\n",
    "                    if activation > activation_threshold:\n",
    "                        has_activation_at_position = True\n",
    "                        max_activation_at_position = max(max_activation_at_position, activation)\n",
    "        \n",
    "        # Create token-specific record\n",
    "        token_record = {\n",
    "            'prompt_id': prompt_data['prompt_id'],\n",
    "            'prompt_text': prompt_data['prompt_text'],\n",
    "            'tokenized_prompt': tokenized_prompt,\n",
    "            'token_type': token_type,\n",
    "            'token_position': target_position,\n",
    "            'max_feature_activation': prompt_data['max_feature_activation']\n",
    "        }\n",
    "        \n",
    "        if has_activation_at_position:\n",
    "            token_record['max_activation_at_position'] = max_activation_at_position\n",
    "            # Include token details for this specific position\n",
    "            position_tokens = []\n",
    "            if 'tokens' in prompt_data:\n",
    "                for token_data in prompt_data['tokens']:\n",
    "                    if token_data['position'] == target_position:\n",
    "                        position_tokens.append(token_data)\n",
    "            token_record['position_tokens'] = position_tokens\n",
    "            token_active.append(token_record)\n",
    "        else:\n",
    "            token_record['max_activation_at_position'] = max_activation_at_position\n",
    "            token_inactive.append(token_record)\n",
    "    \n",
    "    return token_active, token_inactive\n",
    "\n",
    "print(\"Updated token-specific filtering functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing token-specific analysis for 10 features and 2 token types...\n",
      "\n",
      "Processing feature 109889:\n",
      "  Processing token type: model (offset: -1)\n",
      "    Active prompts at model position: 19\n",
      "    Inactive prompts at model position: 981\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/109889/active_model.jsonl\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/109889/inactive_model.jsonl\n",
      "  Processing token type: newline (offset: 0)\n",
      "    Active prompts at newline position: 20\n",
      "    Inactive prompts at newline position: 980\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/109889/active_newline.jsonl\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/109889/inactive_newline.jsonl\n",
      "\n",
      "Processing feature 117729:\n",
      "  Processing token type: model (offset: -1)\n",
      "    Active prompts at model position: 0\n",
      "    Inactive prompts at model position: 1000\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/117729/active_model.jsonl\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/117729/inactive_model.jsonl\n",
      "  Processing token type: newline (offset: 0)\n",
      "    Active prompts at newline position: 1\n",
      "    Inactive prompts at newline position: 999\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/117729/active_newline.jsonl\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/117729/inactive_newline.jsonl\n",
      "\n",
      "Processing feature 58632:\n",
      "  Processing token type: model (offset: -1)\n",
      "    Active prompts at model position: 1\n",
      "    Inactive prompts at model position: 999\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/58632/active_model.jsonl\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/58632/inactive_model.jsonl\n",
      "  Processing token type: newline (offset: 0)\n",
      "    Active prompts at newline position: 86\n",
      "    Inactive prompts at newline position: 914\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/58632/active_newline.jsonl\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/58632/inactive_newline.jsonl\n",
      "\n",
      "Processing feature 76329:\n",
      "  Processing token type: model (offset: -1)\n",
      "    Active prompts at model position: 94\n",
      "    Inactive prompts at model position: 906\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/76329/active_model.jsonl\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/76329/inactive_model.jsonl\n",
      "  Processing token type: newline (offset: 0)\n",
      "    Active prompts at newline position: 10\n",
      "    Inactive prompts at newline position: 990\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/76329/active_newline.jsonl\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/76329/inactive_newline.jsonl\n",
      "\n",
      "Processing feature 118890:\n",
      "  Processing token type: model (offset: -1)\n",
      "    Active prompts at model position: 0\n",
      "    Inactive prompts at model position: 1000\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/118890/active_model.jsonl\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/118890/inactive_model.jsonl\n",
      "  Processing token type: newline (offset: 0)\n",
      "    Active prompts at newline position: 0\n",
      "    Inactive prompts at newline position: 1000\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/118890/active_newline.jsonl\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/118890/inactive_newline.jsonl\n",
      "\n",
      "Processing feature 40273:\n",
      "  Processing token type: model (offset: -1)\n",
      "    Active prompts at model position: 23\n",
      "    Inactive prompts at model position: 977\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/40273/active_model.jsonl\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/40273/inactive_model.jsonl\n",
      "  Processing token type: newline (offset: 0)\n",
      "    Active prompts at newline position: 31\n",
      "    Inactive prompts at newline position: 969\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/40273/active_newline.jsonl\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/40273/inactive_newline.jsonl\n",
      "\n",
      "Processing feature 85940:\n",
      "  Processing token type: model (offset: -1)\n",
      "    Active prompts at model position: 67\n",
      "    Inactive prompts at model position: 933\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/85940/active_model.jsonl\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/85940/inactive_model.jsonl\n",
      "  Processing token type: newline (offset: 0)\n",
      "    Active prompts at newline position: 11\n",
      "    Inactive prompts at newline position: 989\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/85940/active_newline.jsonl\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/85940/inactive_newline.jsonl\n",
      "\n",
      "Processing feature 60824:\n",
      "  Processing token type: model (offset: -1)\n",
      "    Active prompts at model position: 131\n",
      "    Inactive prompts at model position: 869\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/60824/active_model.jsonl\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/60824/inactive_model.jsonl\n",
      "  Processing token type: newline (offset: 0)\n",
      "    Active prompts at newline position: 33\n",
      "    Inactive prompts at newline position: 967\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/60824/active_newline.jsonl\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/60824/inactive_newline.jsonl\n",
      "\n",
      "Processing feature 83102:\n",
      "  Processing token type: model (offset: -1)\n",
      "    Active prompts at model position: 0\n",
      "    Inactive prompts at model position: 1000\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/83102/active_model.jsonl\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/83102/inactive_model.jsonl\n",
      "  Processing token type: newline (offset: 0)\n",
      "    Active prompts at newline position: 0\n",
      "    Inactive prompts at newline position: 1000\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/83102/active_newline.jsonl\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/83102/inactive_newline.jsonl\n",
      "\n",
      "Processing feature 98462:\n",
      "  Processing token type: model (offset: -1)\n",
      "    Active prompts at model position: 0\n",
      "    Inactive prompts at model position: 1000\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/98462/active_model.jsonl\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/98462/inactive_model.jsonl\n",
      "  Processing token type: newline (offset: 0)\n",
      "    Active prompts at newline position: 8\n",
      "    Inactive prompts at newline position: 992\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/98462/active_newline.jsonl\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/98462/inactive_newline.jsonl\n",
      "\n",
      "âœ“ Token-specific analysis complete for all features!\n",
      "Results saved in feature-specific directories under: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts\n",
      "Each feature directory contains:\n",
      "  - active.jsonl / inactive.jsonl (general)\n",
      "  - active_model.jsonl / inactive_model.jsonl (position-specific)\n",
      "  - active_newline.jsonl / inactive_newline.jsonl (position-specific)\n"
     ]
    }
   ],
   "source": [
    "# Process token-specific analysis for each feature and token type\n",
    "print(f\"Processing token-specific analysis for {len(TARGET_FEATURES)} features and {len(TOKEN_OFFSETS)} token types...\")\n",
    "\n",
    "for feature_id in TARGET_FEATURES:\n",
    "    active_prompts, inactive_prompts = feature_results[feature_id]\n",
    "    feature_dir = f\"{BASE_OUTPUT_DIR}/{feature_id}\"\n",
    "    \n",
    "    print(f\"\\nProcessing feature {feature_id}:\")\n",
    "    \n",
    "    for token_type, token_offset in TOKEN_OFFSETS.items():\n",
    "        print(f\"  Processing token type: {token_type} (offset: {token_offset})\")\n",
    "        \n",
    "        # Filter prompts based on activation at this specific token position\n",
    "        token_active, token_inactive = filter_by_token_position(\n",
    "            active_prompts, inactive_prompts, \n",
    "            token_type, token_offset, feature_id, ACTIVATION_THRESHOLD\n",
    "        )\n",
    "        \n",
    "        # Save results for this token type\n",
    "        active_file = f\"{feature_dir}/active_{token_type}.jsonl\"\n",
    "        inactive_file = f\"{feature_dir}/inactive_{token_type}.jsonl\"\n",
    "        \n",
    "        print(f\"    Active prompts at {token_type} position: {len(token_active)}\")\n",
    "        print(f\"    Inactive prompts at {token_type} position: {len(token_inactive)}\")\n",
    "        \n",
    "        # Save active prompts for this token type\n",
    "        with open(active_file, 'w') as f:\n",
    "            for prompt in token_active:\n",
    "                f.write(json.dumps(prompt) + '\\n')\n",
    "        \n",
    "        # Save inactive prompts for this token type\n",
    "        with open(inactive_file, 'w') as f:\n",
    "            for prompt in token_inactive:\n",
    "                f.write(json.dumps(prompt) + '\\n')\n",
    "        \n",
    "        print(f\"    Saved: {active_file}\")\n",
    "        print(f\"    Saved: {inactive_file}\")\n",
    "\n",
    "\n",
    "print(f\"\\nâœ“ Token-specific analysis complete for all features!\")\n",
    "print(f\"Results saved in feature-specific directories under: {BASE_OUTPUT_DIR}\")\n",
    "print(f\"Each feature directory contains:\")\n",
    "print(f\"  - active.jsonl / inactive.jsonl (general)\")\n",
    "for token_type in TOKEN_OFFSETS.keys():\n",
    "    print(f\"  - active_{token_type}.jsonl / inactive_{token_type}.jsonl (position-specific)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
