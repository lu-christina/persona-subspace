{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Analysis: Token-level activations for target features\n",
    "\n",
    "This notebook analyzes token-level activations for specific SAE features on prompts, using a two-stage approach:\n",
    "1. Screen all prompts for target feature activation\n",
    "2. Extract detailed token activations only for prompts where target features fire\n",
    "\n",
    "Saves results to `active.jsonl` and `inactive.jsonl`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import json\n",
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple, Optional, Set\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from huggingface_hub import hf_hub_download\n",
    "from dictionary_learning.utils import load_dictionary\n",
    "from tqdm.auto import tqdm\n",
    "from sae_lens import SAE\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Dict, Tuple, Optional\n",
    "\n",
    "@dataclass\n",
    "class ModelConfig:\n",
    "    \"\"\"Configuration for model-specific settings\"\"\"\n",
    "    base_model_name: str\n",
    "    chat_model_name: str\n",
    "    hf_release: str  # Reference only - actual loading uses saelens_release/sae_id\n",
    "    assistant_header: str\n",
    "    token_offsets: Dict[str, int]\n",
    "    sae_base_path: str\n",
    "    saelens_release: str  # Template for sae_lens release parameter\n",
    "    sae_id_template: str  # Template for sae_lens sae_id parameter\n",
    "    base_url: str  # Base URL for neuronpedia\n",
    "    \n",
    "    def get_sae_params(self, sae_layer: int, sae_trainer: str) -> Tuple[str, str]:\n",
    "        \"\"\"\n",
    "        Generate SAE lens release and sae_id parameters.\n",
    "        \n",
    "        Args:\n",
    "            sae_layer: Layer number for the SAE\n",
    "            sae_trainer: Trainer identifier for the SAE\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (release, sae_id) for sae_lens.SAE.from_pretrained()\n",
    "        \"\"\"\n",
    "        if self.saelens_release == \"llama_scope_lxr_{trainer}\":\n",
    "            release = self.saelens_release.format(trainer=sae_trainer)\n",
    "            sae_id = self.sae_id_template.format(layer=sae_layer, trainer=sae_trainer)\n",
    "        elif self.saelens_release == \"gemma-scope-9b-pt-res\":\n",
    "            # Parse SAE_TRAINER \"131k-l0-34\" into components for Gemma\n",
    "            parts = sae_trainer.split(\"-\")\n",
    "            width = parts[0]  # \"131k\"\n",
    "            l0_value = parts[2]  # \"34\"\n",
    "            \n",
    "            release = self.saelens_release\n",
    "            sae_id = self.sae_id_template.format(layer=sae_layer, width=width, l0=l0_value)\n",
    "        elif self.saelens_release == \"gemma-scope-9b-pt-res-canonical\":\n",
    "            # Parse SAE_TRAINER \"131k-l0-34\" into components for Gemma\n",
    "            parts = sae_trainer.split(\"-\")\n",
    "            width = parts[0]  # \"131k\"\n",
    "\n",
    "            release = self.saelens_release\n",
    "            sae_id = self.sae_id_template.format(layer=sae_layer, width=width)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown SAE lens release template: {self.saelens_release}\")\n",
    "        \n",
    "        return release, sae_id\n",
    "\n",
    "# Model configurations\n",
    "MODEL_CONFIGS = {\n",
    "    \"llama\": ModelConfig(\n",
    "        base_model_name=\"meta-llama/Llama-3.1-8B\",\n",
    "        chat_model_name=\"meta-llama/Llama-3.1-8B-Instruct\",\n",
    "        hf_release=\"fnlp/Llama3_1-8B-Base-LXR-32x\",\n",
    "        assistant_header=\"<|start_header_id|>assistant<|end_header_id|>\",\n",
    "        token_offsets={\"asst\": -2, \"endheader\": -1, \"newline\": 0},\n",
    "        sae_base_path=\"/workspace/sae/llama-3.1-8b/saes\",\n",
    "        saelens_release=\"llama_scope_lxr_{trainer}\",\n",
    "        sae_id_template=\"l{layer}r_{trainer}\",\n",
    "        base_url=\"https://www.neuronpedia.org/llama-3.1-8b/{layer}-llamascope-res-131k\"\n",
    "    ),\n",
    "    \"gemma\": ModelConfig(\n",
    "        base_model_name=\"google/gemma-2-9b\",\n",
    "        chat_model_name=\"google/gemma-2-9b-it\",\n",
    "        hf_release=\"google/gemma-scope-9b-pt-res/layer_{layer}/width_{width}/average_l0_{l0}\",\n",
    "        assistant_header=\"<start_of_turn>model\",\n",
    "        token_offsets={\"model\": -1, \"newline\": 0},\n",
    "        sae_base_path=\"/workspace/sae/gemma-2-9b/saes\",\n",
    "        saelens_release=\"gemma-scope-9b-pt-res-canonical\",\n",
    "        sae_id_template=\"layer_{layer}/width_{width}/canonical\",\n",
    "        base_url=\"https://www.neuronpedia.org/gemma-2-9b/{layer}-gemmascope-res-131k\"\n",
    "    )\n",
    "}\n",
    "\n",
    "# =============================================================================\n",
    "# MODEL SELECTION - Change this to switch between models\n",
    "# =============================================================================\n",
    "MODEL_TYPE = \"gemma\"  # Options: \"gemma\" or \"llama\"\n",
    "MODEL_VER = \"chat\"\n",
    "SAE_LAYER = 20\n",
    "SAE_TRAINER = \"131k-l0-114\"\n",
    "N_PROMPTS = 1000\n",
    "\n",
    "# =============================================================================\n",
    "# TARGET FEATURES - Specify which features to analyze\n",
    "# =============================================================================\n",
    "# TARGET_FEATURES = [91547, 65116, 85422, 80134, 74855, 71187, 102414, 10392, 128628, 8524, 57516, 21953, 26196, 90900, 11383, 111921, 74079]  # List of feature IDs to analyze\n",
    "ACTIVATION_THRESHOLD = 0.0  # Minimum activation to consider \"active\"\n",
    "\n",
    "# =============================================================================\n",
    "# DEDUPLICATION AND CONFIGURATION SETUP\n",
    "# =============================================================================\n",
    "if MODEL_TYPE not in MODEL_CONFIGS:\n",
    "    raise ValueError(f\"Unknown MODEL_TYPE: {MODEL_TYPE}. Available: {list(MODEL_CONFIGS.keys())}\")\n",
    "\n",
    "config = MODEL_CONFIGS[MODEL_TYPE]\n",
    "# Set model name based on version\n",
    "if MODEL_VER == \"chat\":\n",
    "    MODEL_NAME = config.chat_model_name\n",
    "elif MODEL_VER == \"base\":\n",
    "    MODEL_NAME = config.base_model_name\n",
    "else:\n",
    "    raise ValueError(f\"Unknown MODEL_VER: {MODEL_VER}. Use 'chat' or 'base'\")\n",
    "\n",
    "# Always use chat model for tokenizer (has chat template)\n",
    "CHAT_MODEL_NAME = config.chat_model_name\n",
    "\n",
    "# Set up derived configurations\n",
    "ASSISTANT_HEADER = config.assistant_header\n",
    "TOKEN_OFFSETS = config.token_offsets\n",
    "SAE_BASE_PATH = config.sae_base_path\n",
    "\n",
    "# =============================================================================\n",
    "# OUTPUT FILE CONFIGURATION\n",
    "# =============================================================================\n",
    "# Base output directory - individual feature directories will be created under this\n",
    "BASE_OUTPUT_DIR = f\"./results/6_active_prompts/{MODEL_TYPE}_trainer{SAE_TRAINER}_layer{SAE_LAYER}/{N_PROMPTS}_prompts\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸  WARNING: Directory already exists for feature 74079, skipping: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/74079\n",
      "âš ï¸  WARNING: Directory already exists for feature 74855, skipping: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/74855\n",
      "\n",
      "ðŸ”„ Skipped 2 existing features: [74079, 74855]\n",
      "\n",
      "âœ… Will process 48 new features: [116246, 116246, 66831, 55935, 49865, 20668, 106376, 55935, 25340, 39123, 61774, 49865, 87768, 66831, 123729, 110358, 70370, 128007, 49536, 88838, 83076, 68824, 89132, 38451, 45315, 1699, 89358, 4385, 41460, 63154, 81156, 70370, 29239, 43662, 58461, 7418, 118967, 40097, 1738, 94940, 74980, 11171, 53933, 18420, 130744, 49818, 59687, 51330]\n",
      "ðŸ”„ Removed 5 duplicate feature(s) from TARGET_FEATURES\n"
     ]
    }
   ],
   "source": [
    "# LOAD FEATURES FROM FILE\n",
    "FEATURE_FILE = \"./results/4_diffing/gemma_trainer131k-l0-114_layer20/1000_prompts/top_all_mean.csv\"\n",
    "df = pd.read_csv(FEATURE_FILE)\n",
    "TARGET_FEATURES = df['feature_id'].tolist()\n",
    "\n",
    "# =============================================================================\n",
    "# CHECK FOR EXISTING DIRECTORIES AND FILTER TARGET FEATURES\n",
    "# =============================================================================\n",
    "original_target_features = TARGET_FEATURES.copy()\n",
    "filtered_target_features = []\n",
    "existing_features = []\n",
    "\n",
    "for feature_id in TARGET_FEATURES:\n",
    "    feature_dir = f\"{BASE_OUTPUT_DIR}/{feature_id}\"\n",
    "    if os.path.exists(feature_dir):\n",
    "        existing_features.append(feature_id)\n",
    "        print(f\"âš ï¸  WARNING: Directory already exists for feature {feature_id}, skipping: {feature_dir}\")\n",
    "    else:\n",
    "        filtered_target_features.append(feature_id)\n",
    "\n",
    "# Update TARGET_FEATURES to only include features that don't have existing directories\n",
    "TARGET_FEATURES = filtered_target_features\n",
    "\n",
    "if existing_features:\n",
    "    print(f\"\\nðŸ”„ Skipped {len(existing_features)} existing features: {existing_features}\")\n",
    "    \n",
    "if not TARGET_FEATURES:\n",
    "    print(f\"\\nâŒ No new features to process - all {len(original_target_features)} features already have existing directories!\")\n",
    "    print(\"To reprocess existing features, delete their directories first.\")\n",
    "else:\n",
    "    print(f\"\\nâœ… Will process {len(TARGET_FEATURES)} new features: {TARGET_FEATURES}\")\n",
    "\n",
    "\n",
    "# Deduplicate TARGET_FEATURES\n",
    "original_count = len(TARGET_FEATURES)\n",
    "TARGET_FEATURES = list(set(TARGET_FEATURES))\n",
    "duplicates_removed = original_count - len(TARGET_FEATURES)\n",
    "\n",
    "if duplicates_removed > 0:\n",
    "    print(f\"ðŸ”„ Removed {duplicates_removed} duplicate feature(s) from TARGET_FEATURES\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Configuration Summary:\n",
      "  Model Type: gemma\n",
      "  Model to load: google/gemma-2-9b-it\n",
      "  SAE Layer: 20, Trainer: 131k-l0-114\n",
      "  Target Features: [49536, 51330, 45315, 83076, 81156, 88838, 128007, 106376, 89358, 66831, 43662, 116246, 110358, 49818, 4385, 40097, 1699, 11171, 59687, 89132, 53933, 63154, 38451, 29239, 118967, 130744, 20668, 49865, 1738, 61774, 123729, 39123, 87768, 68824, 94940, 58461, 70370, 74980, 41460, 18420, 7418, 25340, 55935]\n",
      "  Activation Threshold: 0.0\n",
      "  Base Output Directory: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# DERIVED CONFIGURATIONS\n",
    "# =============================================================================\n",
    "SAE_PATH = f\"{SAE_BASE_PATH}/resid_post_layer_{SAE_LAYER}/trainer_{SAE_TRAINER}\"\n",
    "LAYER_INDEX = SAE_LAYER\n",
    "\n",
    "# Data paths\n",
    "PROMPTS_HF = \"lmsys/lmsys-chat-1m\"\n",
    "SEED = 42\n",
    "PROMPTS_PATH = f\"/workspace/data/{PROMPTS_HF.split('/')[-1]}/chat_{N_PROMPTS}.jsonl\"\n",
    "os.makedirs(os.path.dirname(PROMPTS_PATH), exist_ok=True)\n",
    "\n",
    "# Processing parameters\n",
    "BATCH_SIZE = 32\n",
    "MAX_LENGTH = 512\n",
    "\n",
    "# =============================================================================\n",
    "# SUMMARY\n",
    "# =============================================================================\n",
    "print(f\"\\nConfiguration Summary:\")\n",
    "print(f\"  Model Type: {MODEL_TYPE}\")\n",
    "print(f\"  Model to load: {MODEL_NAME}\")\n",
    "print(f\"  SAE Layer: {SAE_LAYER}, Trainer: {SAE_TRAINER}\")\n",
    "print(f\"  Target Features: {TARGET_FEATURES}\")\n",
    "print(f\"  Activation Threshold: {ACTIVATION_THRESHOLD}\")\n",
    "print(f\"  Base Output Directory: {BASE_OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompts already exist at /workspace/data/lmsys-chat-1m/chat_1000.jsonl\n",
      "Loaded 1000 prompts\n"
     ]
    }
   ],
   "source": [
    "def load_lmsys_prompts(prompts_path: str, prompts_hf: str, n_prompts: int, seed: int) -> pd.DataFrame:\n",
    "    # Check if prompts_path exists\n",
    "    if os.path.exists(prompts_path):\n",
    "        print(f\"Prompts already exist at {prompts_path}\")\n",
    "        return pd.read_json(prompts_path, lines=True)\n",
    "    else:\n",
    "        print(f\"Prompts do not exist at {prompts_path}. Loading from {prompts_hf}...\")\n",
    "        dataset = load_dataset(prompts_hf)\n",
    "        dataset = dataset['train'].shuffle(seed=seed).select(range(n_prompts))\n",
    "        df = dataset.to_pandas()\n",
    "\n",
    "        # Extract the prompt from the first conversation item\n",
    "        df['prompt'] = df['conversation'].apply(lambda x: x[0]['content'])\n",
    "\n",
    "        # Only keep some columns\n",
    "        df = df[['conversation_id', 'prompt', 'redacted', 'language']]\n",
    "\n",
    "        # Save to .jsonl file\n",
    "        df.to_json(prompts_path, orient='records', lines=True)\n",
    "        return df\n",
    "\n",
    "prompts_df = load_lmsys_prompts(PROMPTS_PATH, PROMPTS_HF, N_PROMPTS, SEED)\n",
    "print(f\"Loaded {prompts_df.shape[0]} prompts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model and SAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer loaded: GemmaTokenizerFast\n"
     ]
    }
   ],
   "source": [
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "print(f\"Tokenizer loaded: {tokenizer.__class__.__name__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d29c8e6f4dbc4abeb945aa5cf2b628b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded: Gemma2ForCausalLM\n",
      "Model device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Load model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map={\"\":0}\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "print(f\"Model loaded: {model.__class__.__name__}\")\n",
    "print(f\"Model device: {next(model.parameters()).device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading SAE from /workspace/sae/gemma-2-9b/saes/resid_post_layer_20/trainer_131k-l0-114\n",
      "âœ“ Found SAE files at: /workspace/sae/gemma-2-9b/saes/resid_post_layer_20/trainer_131k-l0-114\n",
      "SAE loaded with 131072 features\n",
      "SAE device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "def load_sae(config: ModelConfig, sae_path: str, sae_layer: int, sae_trainer: str) -> SAE:\n",
    "    \"\"\"\n",
    "    Unified SAE loading function that handles both Llama and Gemma models.\n",
    "    \n",
    "    Args:\n",
    "        config: ModelConfig object containing model-specific settings\n",
    "        sae_path: Local path to store/load SAE files\n",
    "        sae_layer: Layer number for the SAE\n",
    "        sae_trainer: Trainer identifier for the SAE\n",
    "    \n",
    "    Returns:\n",
    "        SAE: Loaded SAE model\n",
    "    \"\"\"\n",
    "    # Check if SAE file exists locally\n",
    "    print(f\"Loading SAE from {sae_path}\")\n",
    "    ae_file_path = os.path.join(sae_path, \"sae_weights.safetensors\")\n",
    "    \n",
    "    if os.path.exists(ae_file_path):\n",
    "        print(f\"âœ“ Found SAE files at: {os.path.dirname(ae_file_path)}\")\n",
    "        sae = SAE.load_from_disk(sae_path)\n",
    "        return sae\n",
    "    \n",
    "    print(f\"SAE not found locally, downloading from HF via sae_lens...\")\n",
    "    os.makedirs(os.path.dirname(sae_path), exist_ok=True)\n",
    "    \n",
    "    # Get SAE parameters from config\n",
    "    release, sae_id = config.get_sae_params(sae_layer, sae_trainer)\n",
    "    print(f\"Loading SAE with release='{release}', sae_id='{sae_id}'\")\n",
    "    \n",
    "    # Load the SAE using sae_lens\n",
    "    sae, _, _ = SAE.from_pretrained(\n",
    "        release=release,\n",
    "        sae_id=sae_id,\n",
    "        device=\"cuda\" # Hardcoded because it wants a string\n",
    "    )\n",
    "    \n",
    "    # Save the SAE locally for future use\n",
    "    sae.save_model(sae_path)\n",
    "    return sae\n",
    "\n",
    "# Load SAE using the unified function\n",
    "sae = load_sae(config, SAE_PATH, SAE_LAYER, SAE_TRAINER)\n",
    "sae = sae.to(device)  # Move SAE to GPU\n",
    "\n",
    "print(f\"SAE loaded with {sae.cfg.d_sae} features\")\n",
    "print(f\"SAE device: {next(sae.parameters()).device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two-Stage Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated two-stage processing function defined\n"
     ]
    }
   ],
   "source": [
    "class StopForward(Exception):\n",
    "    \"\"\"Exception to stop forward pass after target layer.\"\"\"\n",
    "    pass\n",
    "\n",
    "@torch.no_grad()\n",
    "def process_prompts_for_features(prompts: List[str], target_features: List[int], \n",
    "                                layer_idx: int, activation_threshold: float = 0.0) -> Dict[int, Tuple[List[Dict], List[Dict]]]:\n",
    "    \"\"\"Two-stage processing: screen for target features, then get detailed tokens for active prompts.\n",
    "    \n",
    "    Returns:\n",
    "        Dict mapping feature_id to (active_prompts, inactive_prompts) for that feature\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize results dictionary for each feature\n",
    "    results = {}\n",
    "    for feature_id in target_features:\n",
    "        results[feature_id] = ([], [])  # (active_prompts, inactive_prompts)\n",
    "    \n",
    "    # Get target layer\n",
    "    target_layer = model.model.layers[layer_idx]\n",
    "    \n",
    "    # Process in batches\n",
    "    for i in tqdm(range(0, len(prompts), BATCH_SIZE), desc=\"Processing prompts\"):\n",
    "        batch_prompts = prompts[i:i+BATCH_SIZE]\n",
    "        \n",
    "        # Format prompts as chat messages\n",
    "        formatted_prompts = []\n",
    "        for prompt in batch_prompts:\n",
    "            messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "            formatted_prompt = tokenizer.apply_chat_template(\n",
    "                messages, tokenize=False, add_generation_prompt=True\n",
    "            )\n",
    "            formatted_prompts.append(formatted_prompt)\n",
    "        \n",
    "        # Tokenize batch\n",
    "        batch_inputs = tokenizer(\n",
    "            formatted_prompts,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=MAX_LENGTH\n",
    "        )\n",
    "        \n",
    "        # Move to device\n",
    "        batch_inputs = {k: v.to(device) for k, v in batch_inputs.items()}\n",
    "        \n",
    "        # Stage 1: Get activations for screening\n",
    "        activations = None\n",
    "        \n",
    "        def hook_fn(module, input, output):\n",
    "            nonlocal activations\n",
    "            activations = output[0] if isinstance(output, tuple) else output\n",
    "            raise StopForward()\n",
    "        \n",
    "        # Register hook\n",
    "        handle = target_layer.register_forward_hook(hook_fn)\n",
    "        \n",
    "        try:\n",
    "            _ = model(**batch_inputs)\n",
    "        except StopForward:\n",
    "            pass\n",
    "        finally:\n",
    "            handle.remove()\n",
    "        \n",
    "        # Apply SAE to get feature activations\n",
    "        batch_size, seq_len, hidden_dim = activations.shape\n",
    "        flat_activations = activations.view(-1, hidden_dim)\n",
    "        \n",
    "        # Process SAE in chunks to avoid memory issues\n",
    "        sae_features = []\n",
    "        for chunk_start in range(0, flat_activations.shape[0], BATCH_SIZE * 8):\n",
    "            chunk_end = min(chunk_start + BATCH_SIZE * 8, flat_activations.shape[0])\n",
    "            chunk_activations = flat_activations[chunk_start:chunk_end]\n",
    "            chunk_features = sae.encode(chunk_activations)\n",
    "            sae_features.append(chunk_features.cpu())\n",
    "        \n",
    "        sae_features = torch.cat(sae_features, dim=0)\n",
    "        sae_features = sae_features.view(batch_size, seq_len, -1)\n",
    "        \n",
    "        # Stage 2: Process each prompt for each target feature separately\n",
    "        for batch_idx, (prompt, formatted_prompt) in enumerate(zip(batch_prompts, formatted_prompts)):\n",
    "            prompt_idx = i + batch_idx\n",
    "            prompt_features = sae_features[batch_idx]  # [seq_len, num_features]\n",
    "            input_ids = batch_inputs['input_ids'][batch_idx].cpu().numpy()\n",
    "            \n",
    "            # Create tokenized prompt (convert input_ids to token strings)\n",
    "            tokenized_prompt = []\n",
    "            for token_id in input_ids:\n",
    "                if token_id != tokenizer.pad_token_id:  # Skip padding tokens\n",
    "                    token_text = tokenizer.decode([int(token_id)])\n",
    "                    tokenized_prompt.append(token_text)\n",
    "            \n",
    "            # Process each target feature separately\n",
    "            for feature_id in target_features:\n",
    "                # Get activations for this specific feature\n",
    "                feature_activations = prompt_features[:, feature_id]  # [seq_len]\n",
    "                max_activation = float(feature_activations.max())\n",
    "                \n",
    "                # Check if this feature is active for this prompt\n",
    "                is_active = max_activation > activation_threshold\n",
    "                \n",
    "                if is_active:\n",
    "                    # Stage 3: Get detailed token analysis for active prompts\n",
    "                    tokens = []\n",
    "                    for pos in range(len(input_ids)):\n",
    "                        if pos >= prompt_features.shape[0]:\n",
    "                            break\n",
    "                            \n",
    "                        token_id = int(input_ids[pos])\n",
    "                        token_text = tokenizer.decode([token_id])\n",
    "                        \n",
    "                        # Get activation for this specific feature at this position\n",
    "                        activation_val = float(prompt_features[pos, feature_id])\n",
    "                        \n",
    "                        if activation_val > 0:  # Only store non-zero activations\n",
    "                            tokens.append({\n",
    "                                'position': pos,\n",
    "                                'token_id': token_id,\n",
    "                                'text': token_text,\n",
    "                                'feature_activation': activation_val\n",
    "                            })\n",
    "                    \n",
    "                    results[feature_id][0].append({\n",
    "                        'prompt_id': prompt_idx,\n",
    "                        'prompt_text': prompt,\n",
    "                        'tokenized_prompt': tokenized_prompt,\n",
    "                        'max_feature_activation': max_activation,\n",
    "                        'tokens': tokens\n",
    "                    })\n",
    "                else:\n",
    "                    # Inactive prompt - just basic info\n",
    "                    results[feature_id][1].append({\n",
    "                        'prompt_id': prompt_idx,\n",
    "                        'prompt_text': prompt,\n",
    "                        'tokenized_prompt': tokenized_prompt,\n",
    "                        'max_feature_activation': max_activation\n",
    "                    })\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"Updated two-stage processing function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process All Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 1000 prompts for 43 target features...\n",
      "Target features: [49536, 51330, 45315, 83076, 81156, 88838, 128007, 106376, 89358, 66831, 43662, 116246, 110358, 49818, 4385, 40097, 1699, 11171, 59687, 89132, 53933, 63154, 38451, 29239, 118967, 130744, 20668, 49865, 1738, 61774, 123729, 39123, 87768, 68824, 94940, 58461, 70370, 74980, 41460, 18420, 7418, 25340, 55935]\n",
      "Activation threshold: 0.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f287d58fe48a40249e074e591b5aeb45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing prompts:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results by feature:\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'lesn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m feature_id \u001b[38;5;129;01min\u001b[39;00m TARGET_FEATURES:\n\u001b[32m     15\u001b[39m     active_prompts, inactive_prompts = feature_results[feature_id]\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m     total_tokens = \u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mlesn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtokens\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mactive_prompts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  Feature \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfeature_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(active_prompts)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m active, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(inactive_prompts)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m inactive, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_tokens\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m active tokens\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 16\u001b[39m, in \u001b[36m<genexpr>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m feature_id \u001b[38;5;129;01min\u001b[39;00m TARGET_FEATURES:\n\u001b[32m     15\u001b[39m     active_prompts, inactive_prompts = feature_results[feature_id]\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m     total_tokens = \u001b[38;5;28msum\u001b[39m(\u001b[43mlesn\u001b[49m(p[\u001b[33m'\u001b[39m\u001b[33mtokens\u001b[39m\u001b[33m'\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m active_prompts)\n\u001b[32m     17\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  Feature \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfeature_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(active_prompts)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m active, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(inactive_prompts)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m inactive, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_tokens\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m active tokens\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'lesn' is not defined"
     ]
    }
   ],
   "source": [
    "print(f\"Processing {len(prompts_df)} prompts for {len(TARGET_FEATURES)} target features...\")\n",
    "print(f\"Target features: {TARGET_FEATURES}\")\n",
    "print(f\"Activation threshold: {ACTIVATION_THRESHOLD}\")\n",
    "\n",
    "# Process all prompts for all features\n",
    "feature_results = process_prompts_for_features(\n",
    "    prompts_df['prompt'].tolist(), \n",
    "    TARGET_FEATURES, \n",
    "    LAYER_INDEX, \n",
    "    ACTIVATION_THRESHOLD\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results by feature:\n",
      "  Feature 49536: 1000 active, 0 inactive, 3185 active tokens\n",
      "  Feature 51330: 1000 active, 0 inactive, 84073 active tokens\n",
      "  Feature 45315: 835 active, 165 inactive, 18855 active tokens\n",
      "  Feature 83076: 1000 active, 0 inactive, 2841 active tokens\n",
      "  Feature 81156: 1000 active, 0 inactive, 376700 active tokens\n",
      "  Feature 88838: 1000 active, 0 inactive, 328344 active tokens\n",
      "  Feature 128007: 1000 active, 0 inactive, 272198 active tokens\n",
      "  Feature 106376: 1000 active, 0 inactive, 459168 active tokens\n",
      "  Feature 89358: 673 active, 327 inactive, 731 active tokens\n",
      "  Feature 66831: 1000 active, 0 inactive, 255846 active tokens\n",
      "  Feature 43662: 479 active, 521 inactive, 483 active tokens\n",
      "  Feature 116246: 1000 active, 0 inactive, 446514 active tokens\n",
      "  Feature 110358: 666 active, 334 inactive, 4486 active tokens\n",
      "  Feature 49818: 187 active, 813 inactive, 547 active tokens\n",
      "  Feature 4385: 1000 active, 0 inactive, 392084 active tokens\n",
      "  Feature 40097: 1000 active, 0 inactive, 8096 active tokens\n",
      "  Feature 1699: 511 active, 489 inactive, 10743 active tokens\n",
      "  Feature 11171: 1000 active, 0 inactive, 7049 active tokens\n",
      "  Feature 59687: 638 active, 362 inactive, 14195 active tokens\n",
      "  Feature 89132: 1000 active, 0 inactive, 234142 active tokens\n",
      "  Feature 53933: 1000 active, 0 inactive, 4541 active tokens\n",
      "  Feature 63154: 1000 active, 0 inactive, 195985 active tokens\n",
      "  Feature 38451: 1000 active, 0 inactive, 2546 active tokens\n",
      "  Feature 29239: 1000 active, 0 inactive, 2939 active tokens\n",
      "  Feature 118967: 420 active, 580 inactive, 828 active tokens\n",
      "  Feature 130744: 1000 active, 0 inactive, 2399 active tokens\n",
      "  Feature 20668: 1000 active, 0 inactive, 429587 active tokens\n",
      "  Feature 49865: 1000 active, 0 inactive, 289371 active tokens\n",
      "  Feature 1738: 404 active, 596 inactive, 404 active tokens\n",
      "  Feature 61774: 907 active, 93 inactive, 67907 active tokens\n",
      "  Feature 123729: 1000 active, 0 inactive, 3033 active tokens\n",
      "  Feature 39123: 775 active, 225 inactive, 915 active tokens\n",
      "  Feature 87768: 804 active, 196 inactive, 867 active tokens\n",
      "  Feature 68824: 1000 active, 0 inactive, 249399 active tokens\n",
      "  Feature 94940: 1000 active, 0 inactive, 2586 active tokens\n",
      "  Feature 58461: 1000 active, 0 inactive, 2611 active tokens\n",
      "  Feature 70370: 1000 active, 0 inactive, 105385 active tokens\n",
      "  Feature 74980: 1000 active, 0 inactive, 3172 active tokens\n",
      "  Feature 41460: 493 active, 507 inactive, 839 active tokens\n",
      "  Feature 18420: 423 active, 577 inactive, 504 active tokens\n",
      "  Feature 7418: 295 active, 705 inactive, 764 active tokens\n",
      "  Feature 25340: 890 active, 110 inactive, 2415 active tokens\n",
      "  Feature 55935: 1000 active, 0 inactive, 206899 active tokens\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(f\"\\nResults by feature:\")\n",
    "for feature_id in TARGET_FEATURES:\n",
    "    active_prompts, inactive_prompts = feature_results[feature_id]\n",
    "    total_tokens = sum(len(p['tokens']) for p in active_prompts)\n",
    "    print(f\"  Feature {feature_id}: {len(active_prompts)} active, {len(inactive_prompts)} inactive, {total_tokens} active tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving 1000 active prompts for feature 49536 to ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/49536/active.jsonl\n",
      "Saving 0 inactive prompts for feature 49536 to ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/49536/inactive.jsonl\n",
      "  Sample active prompt for feature 49536:\n",
      "    Prompt: the table structure is: create table classroom\n",
      " (building varchar(15) comment 'æ¥¼å®‡',\n",
      " room_number var...\n",
      "    Max activation: 36.336143493652344\n",
      "    Active tokens: 3\n",
      "    Top token: '<bos>' (position 0)\n",
      "    Token activation: 36.336143493652344\n",
      "\n",
      "Saving 1000 active prompts for feature 51330 to ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/51330/active.jsonl\n",
      "Saving 0 inactive prompts for feature 51330 to ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/51330/inactive.jsonl\n",
      "  Sample active prompt for feature 51330:\n",
      "    Prompt: the table structure is: create table classroom\n",
      " (building varchar(15) comment 'æ¥¼å®‡',\n",
      " room_number var...\n",
      "    Max activation: 36.86264419555664\n",
      "    Active tokens: 81\n",
      "    Top token: 'building' (position 55)\n",
      "    Token activation: 36.86264419555664\n",
      "\n",
      "Saving 835 active prompts for feature 45315 to ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/45315/active.jsonl\n",
      "Saving 165 inactive prompts for feature 45315 to ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/45315/inactive.jsonl\n",
      "  Sample active prompt for feature 45315:\n",
      "    Prompt: Scrivi un testo, su questo tema: Nel mondo digitale in cui viviamo, spesso tendiamo a silenziare chi...\n",
      "    Max activation: 5.726430892944336\n",
      "    Active tokens: 1\n",
      "    Top token: 'Scri' (position 5)\n",
      "    Token activation: 5.726430892944336\n",
      "\n",
      "Saving 1000 active prompts for feature 83076 to ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/83076/active.jsonl\n",
      "Saving 0 inactive prompts for feature 83076 to ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/83076/inactive.jsonl\n",
      "  Sample active prompt for feature 83076:\n",
      "    Prompt: the table structure is: create table classroom\n",
      " (building varchar(15) comment 'æ¥¼å®‡',\n",
      " room_number var...\n",
      "    Max activation: 10.878552436828613\n",
      "    Active tokens: 3\n",
      "    Top token: '<bos>' (position 0)\n",
      "    Token activation: 10.878552436828613\n",
      "\n",
      "Saving 1000 active prompts for feature 81156 to ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/81156/active.jsonl\n",
      "Saving 0 inactive prompts for feature 81156 to ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/81156/inactive.jsonl\n",
      "  Sample active prompt for feature 81156:\n",
      "    Prompt: the table structure is: create table classroom\n",
      " (building varchar(15) comment 'æ¥¼å®‡',\n",
      " room_number var...\n",
      "    Max activation: 51.930877685546875\n",
      "    Active tokens: 480\n",
      "    Top token: '<bos>' (position 0)\n",
      "    Token activation: 51.930877685546875\n",
      "\n",
      "Saving 1000 active prompts for feature 88838 to ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/88838/active.jsonl\n",
      "Saving 0 inactive prompts for feature 88838 to ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/88838/inactive.jsonl\n",
      "  Sample active prompt for feature 88838:\n",
      "    Prompt: the table structure is: create table classroom\n",
      " (building varchar(15) comment 'æ¥¼å®‡',\n",
      " room_number var...\n",
      "    Max activation: 105.54953002929688\n",
      "    Active tokens: 240\n",
      "    Top token: '<bos>' (position 0)\n",
      "    Token activation: 105.54953002929688\n",
      "\n",
      "Saving 1000 active prompts for feature 128007 to ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/128007/active.jsonl\n",
      "Saving 0 inactive prompts for feature 128007 to ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/128007/inactive.jsonl\n",
      "  Sample active prompt for feature 128007:\n",
      "    Prompt: the table structure is: create table classroom\n",
      " (building varchar(15) comment 'æ¥¼å®‡',\n",
      " room_number var...\n",
      "    Max activation: 19.49155616760254\n",
      "    Active tokens: 414\n",
      "    Top token: '<pad>' (position 127)\n",
      "    Token activation: 19.49155616760254\n",
      "\n",
      "Saving 1000 active prompts for feature 106376 to ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/106376/active.jsonl\n",
      "Saving 0 inactive prompts for feature 106376 to ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/106376/inactive.jsonl\n",
      "  Sample active prompt for feature 106376:\n",
      "    Prompt: the table structure is: create table classroom\n",
      " (building varchar(15) comment 'æ¥¼å®‡',\n",
      " room_number var...\n",
      "    Max activation: 78.19650268554688\n",
      "    Active tokens: 512\n",
      "    Top token: 'user' (position 3)\n",
      "    Token activation: 78.19650268554688\n",
      "\n",
      "Saving 673 active prompts for feature 89358 to ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/89358/active.jsonl\n",
      "Saving 327 inactive prompts for feature 89358 to ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/89358/inactive.jsonl\n",
      "  Sample active prompt for feature 89358:\n",
      "    Prompt: how do I keep the \\n in a string when using console.log in javascript...\n",
      "    Max activation: 5.278935432434082\n",
      "    Active tokens: 1\n",
      "    Top token: '\n",
      "' (position 26)\n",
      "    Token activation: 5.278935432434082\n",
      "\n",
      "Saving 1000 active prompts for feature 66831 to ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/66831/active.jsonl\n",
      "Saving 0 inactive prompts for feature 66831 to ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/66831/inactive.jsonl\n",
      "  Sample active prompt for feature 66831:\n",
      "    Prompt: the table structure is: create table classroom\n",
      " (building varchar(15) comment 'æ¥¼å®‡',\n",
      " room_number var...\n",
      "    Max activation: 72.6351089477539\n",
      "    Active tokens: 440\n",
      "    Top token: 'user' (position 3)\n",
      "    Token activation: 72.6351089477539\n",
      "\n",
      "Saving 479 active prompts for feature 43662 to ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/43662/active.jsonl\n",
      "Saving 521 inactive prompts for feature 43662 to ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/43662/inactive.jsonl\n",
      "  Sample active prompt for feature 43662:\n",
      "    Prompt: the table structure is: create table classroom\n",
      " (building varchar(15) comment 'æ¥¼å®‡',\n",
      " room_number var...\n",
      "    Max activation: 9.73930549621582\n",
      "    Active tokens: 1\n",
      "    Top token: '\n",
      "' (position 103)\n",
      "    Token activation: 9.73930549621582\n",
      "\n",
      "Saving 1000 active prompts for feature 116246 to ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/116246/active.jsonl\n",
      "Saving 0 inactive prompts for feature 116246 to ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/116246/inactive.jsonl\n",
      "  Sample active prompt for feature 116246:\n",
      "    Prompt: the table structure is: create table classroom\n",
      " (building varchar(15) comment 'æ¥¼å®‡',\n",
      " room_number var...\n",
      "    Max activation: 3108.865234375\n",
      "    Active tokens: 508\n",
      "    Top token: '<bos>' (position 0)\n",
      "    Token activation: 3108.865234375\n",
      "\n",
      "Saving 666 active prompts for feature 110358 to ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/110358/active.jsonl\n",
      "Saving 334 inactive prompts for feature 110358 to ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/110358/inactive.jsonl\n",
      "  Sample active prompt for feature 110358:\n",
      "    Prompt: the table structure is: create table classroom\n",
      " (building varchar(15) comment 'æ¥¼å®‡',\n",
      " room_number var...\n",
      "    Max activation: 11.321044921875\n",
      "    Active tokens: 5\n",
      "    Top token: '\n",
      "' (position 103)\n",
      "    Token activation: 11.321044921875\n",
      "\n",
      "Saving 187 active prompts for feature 49818 to ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/49818/active.jsonl\n",
      "Saving 813 inactive prompts for feature 49818 to ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/49818/inactive.jsonl\n",
      "  Sample active prompt for feature 49818:\n",
      "    Prompt: \"How can we improve the effectiveness of our marketing campaigns using AI and machine learning?\" Is ...\n",
      "    Max activation: 4.110776424407959\n",
      "    Active tokens: 1\n",
      "    Top token: '\n",
      "' (position 41)\n",
      "    Token activation: 4.110776424407959\n",
      "\n",
      "Saving 1000 active prompts for feature 4385 to ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/4385/active.jsonl\n",
      "Saving 0 inactive prompts for feature 4385 to ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/4385/inactive.jsonl\n",
      "  Sample active prompt for feature 4385:\n",
      "    Prompt: the table structure is: create table classroom\n",
      " (building varchar(15) comment 'æ¥¼å®‡',\n",
      " room_number var...\n",
      "    Max activation: 106.36541748046875\n",
      "    Active tokens: 352\n",
      "    Top token: '<bos>' (position 0)\n",
      "    Token activation: 106.36541748046875\n",
      "\n",
      "Saving 1000 active prompts for feature 40097 to ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/40097/active.jsonl\n",
      "Saving 0 inactive prompts for feature 40097 to ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/40097/inactive.jsonl\n",
      "  Sample active prompt for feature 40097:\n",
      "    Prompt: the table structure is: create table classroom\n",
      " (building varchar(15) comment 'æ¥¼å®‡',\n",
      " room_number var...\n",
      "    Max activation: 24.40106773376465\n",
      "    Active tokens: 3\n",
      "    Top token: '<bos>' (position 0)\n",
      "    Token activation: 24.40106773376465\n",
      "\n",
      "Saving 511 active prompts for feature 1699 to ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/1699/active.jsonl\n",
      "Saving 489 inactive prompts for feature 1699 to ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/1699/inactive.jsonl\n",
      "  Sample active prompt for feature 1699:\n",
      "    Prompt: Scrivi un testo, su questo tema: Nel mondo digitale in cui viviamo, spesso tendiamo a silenziare chi...\n",
      "    Max activation: 7.892247676849365\n",
      "    Active tokens: 5\n",
      "    Top token: 'model' (position 296)\n",
      "    Token activation: 7.892247676849365\n",
      "\n",
      "Saving 1000 active prompts for feature 11171 to ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/11171/active.jsonl\n",
      "Saving 0 inactive prompts for feature 11171 to ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/11171/inactive.jsonl\n",
      "  Sample active prompt for feature 11171:\n",
      "    Prompt: the table structure is: create table classroom\n",
      " (building varchar(15) comment 'æ¥¼å®‡',\n",
      " room_number var...\n",
      "    Max activation: 44.708465576171875\n",
      "    Active tokens: 8\n",
      "    Top token: '<bos>' (position 0)\n",
      "    Token activation: 44.708465576171875\n",
      "\n",
      "Saving 638 active prompts for feature 59687 to ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/59687/active.jsonl\n",
      "Saving 362 inactive prompts for feature 59687 to ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/59687/inactive.jsonl\n",
      "  Sample active prompt for feature 59687:\n",
      "    Prompt: the table structure is: create table classroom\n",
      " (building varchar(15) comment 'æ¥¼å®‡',\n",
      " room_number var...\n",
      "    Max activation: 8.200186729431152\n",
      "    Active tokens: 13\n",
      "    Top token: 'convert' (position 88)\n",
      "    Token activation: 8.200186729431152\n",
      "\n",
      "Saving 1000 active prompts for feature 89132 to ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/89132/active.jsonl\n",
      "Saving 0 inactive prompts for feature 89132 to ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/89132/inactive.jsonl\n",
      "  Sample active prompt for feature 89132:\n",
      "    Prompt: the table structure is: create table classroom\n",
      " (building varchar(15) comment 'æ¥¼å®‡',\n",
      " room_number var...\n",
      "    Max activation: 21.87302017211914\n",
      "    Active tokens: 439\n",
      "    Top token: '',' (position 50)\n",
      "    Token activation: 21.87302017211914\n",
      "\n",
      "Saving 1000 active prompts for feature 53933 to ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/53933/active.jsonl\n",
      "Saving 0 inactive prompts for feature 53933 to ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/53933/inactive.jsonl\n",
      "  Sample active prompt for feature 53933:\n",
      "    Prompt: the table structure is: create table classroom\n",
      " (building varchar(15) comment 'æ¥¼å®‡',\n",
      " room_number var...\n",
      "    Max activation: 58.89487075805664\n",
      "    Active tokens: 3\n",
      "    Top token: '<bos>' (position 0)\n",
      "    Token activation: 58.89487075805664\n",
      "\n",
      "Saving 1000 active prompts for feature 63154 to ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/63154/active.jsonl\n",
      "Saving 0 inactive prompts for feature 63154 to ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/63154/inactive.jsonl\n",
      "  Sample active prompt for feature 63154:\n",
      "    Prompt: the table structure is: create table classroom\n",
      " (building varchar(15) comment 'æ¥¼å®‡',\n",
      " room_number var...\n",
      "    Max activation: 23.21176528930664\n",
      "    Active tokens: 10\n",
      "    Top token: 'ç¼–å·' (position 37)\n",
      "    Token activation: 23.21176528930664\n",
      "\n",
      "Saving 1000 active prompts for feature 38451 to ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/38451/active.jsonl\n",
      "Saving 0 inactive prompts for feature 38451 to ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/38451/inactive.jsonl\n",
      "  Sample active prompt for feature 38451:\n",
      "    Prompt: the table structure is: create table classroom\n",
      " (building varchar(15) comment 'æ¥¼å®‡',\n",
      " room_number var...\n",
      "    Max activation: 6.839779853820801\n",
      "    Active tokens: 2\n",
      "    Top token: '<bos>' (position 0)\n",
      "    Token activation: 6.839779853820801\n",
      "\n",
      "Saving 1000 active prompts for feature 29239 to ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/29239/active.jsonl\n",
      "Saving 0 inactive prompts for feature 29239 to ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/29239/inactive.jsonl\n",
      "  Sample active prompt for feature 29239:\n",
      "    Prompt: the table structure is: create table classroom\n",
      " (building varchar(15) comment 'æ¥¼å®‡',\n",
      " room_number var...\n",
      "    Max activation: 13.584754943847656\n",
      "    Active tokens: 2\n",
      "    Top token: '<bos>' (position 0)\n",
      "    Token activation: 13.584754943847656\n",
      "\n",
      "Saving 420 active prompts for feature 118967 to ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/118967/active.jsonl\n",
      "Saving 580 inactive prompts for feature 118967 to ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/118967/inactive.jsonl\n",
      "  Sample active prompt for feature 118967:\n",
      "    Prompt: ÐŸÑ€Ð¸Ð²ÐµÑ‚, ÐºÐ°ÐºÐ¾Ð¹ Ñ†Ð²ÐµÑ‚ ÑÐ°Ð¼Ñ‹Ð¹ Ð¿Ð¾Ð¿ÑƒÐ»ÑÑ€Ð½Ñ‹Ð¹ Ð² Ð¾Ð´ÐµÐ¶Ð´Ðµ?...\n",
      "    Max activation: 5.509716987609863\n",
      "    Active tokens: 2\n",
      "    Top token: 'model' (position 19)\n",
      "    Token activation: 5.509716987609863\n",
      "\n",
      "Saving 1000 active prompts for feature 130744 to ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/130744/active.jsonl\n",
      "Saving 0 inactive prompts for feature 130744 to ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/130744/inactive.jsonl\n",
      "  Sample active prompt for feature 130744:\n",
      "    Prompt: the table structure is: create table classroom\n",
      " (building varchar(15) comment 'æ¥¼å®‡',\n",
      " room_number var...\n",
      "    Max activation: 9.75382137298584\n",
      "    Active tokens: 3\n",
      "    Top token: '<bos>' (position 0)\n",
      "    Token activation: 9.75382137298584\n",
      "\n",
      "Saving 1000 active prompts for feature 20668 to ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/20668/active.jsonl\n",
      "Saving 0 inactive prompts for feature 20668 to ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/20668/inactive.jsonl\n",
      "  Sample active prompt for feature 20668:\n",
      "    Prompt: the table structure is: create table classroom\n",
      " (building varchar(15) comment 'æ¥¼å®‡',\n",
      " room_number var...\n",
      "    Max activation: 135.9832763671875\n",
      "    Active tokens: 476\n",
      "    Top token: '<bos>' (position 0)\n",
      "    Token activation: 135.9832763671875\n",
      "\n",
      "Saving 1000 active prompts for feature 49865 to ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/49865/active.jsonl\n",
      "Saving 0 inactive prompts for feature 49865 to ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/49865/inactive.jsonl\n",
      "  Sample active prompt for feature 49865:\n",
      "    Prompt: the table structure is: create table classroom\n",
      " (building varchar(15) comment 'æ¥¼å®‡',\n",
      " room_number var...\n",
      "    Max activation: 55.35134506225586\n",
      "    Active tokens: 349\n",
      "    Top token: ' according' (position 91)\n",
      "    Token activation: 55.35134506225586\n",
      "\n",
      "Saving 404 active prompts for feature 1738 to ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/1738/active.jsonl\n",
      "Saving 596 inactive prompts for feature 1738 to ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/1738/inactive.jsonl\n",
      "  Sample active prompt for feature 1738:\n",
      "    Prompt: the table structure is: create table classroom\n",
      " (building varchar(15) comment 'æ¥¼å®‡',\n",
      " room_number var...\n",
      "    Max activation: 5.141754150390625\n",
      "    Active tokens: 1\n",
      "    Top token: '\n",
      "' (position 103)\n",
      "    Token activation: 5.141754150390625\n",
      "\n",
      "Saving 907 active prompts for feature 61774 to ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/61774/active.jsonl\n",
      "Saving 93 inactive prompts for feature 61774 to ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/61774/inactive.jsonl\n",
      "  Sample active prompt for feature 61774:\n",
      "    Prompt: the table structure is: create table classroom\n",
      " (building varchar(15) comment 'æ¥¼å®‡',\n",
      " room_number var...\n",
      "    Max activation: 19.721851348876953\n",
      "    Active tokens: 20\n",
      "    Top token: 'model' (position 102)\n",
      "    Token activation: 19.721851348876953\n",
      "\n",
      "Saving 1000 active prompts for feature 123729 to ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/123729/active.jsonl\n",
      "Saving 0 inactive prompts for feature 123729 to ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/123729/inactive.jsonl\n",
      "  Sample active prompt for feature 123729:\n",
      "    Prompt: the table structure is: create table classroom\n",
      " (building varchar(15) comment 'æ¥¼å®‡',\n",
      " room_number var...\n",
      "    Max activation: 85.05247497558594\n",
      "    Active tokens: 3\n",
      "    Top token: '<bos>' (position 0)\n",
      "    Token activation: 85.05247497558594\n",
      "\n",
      "Saving 775 active prompts for feature 39123 to ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/39123/active.jsonl\n",
      "Saving 225 inactive prompts for feature 39123 to ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/39123/inactive.jsonl\n",
      "  Sample active prompt for feature 39123:\n",
      "    Prompt: the table structure is: create table classroom\n",
      " (building varchar(15) comment 'æ¥¼å®‡',\n",
      " room_number var...\n",
      "    Max activation: 7.372138500213623\n",
      "    Active tokens: 2\n",
      "    Top token: 'model' (position 102)\n",
      "    Token activation: 7.372138500213623\n",
      "\n",
      "Saving 804 active prompts for feature 87768 to ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/87768/active.jsonl\n",
      "Saving 196 inactive prompts for feature 87768 to ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/87768/inactive.jsonl\n",
      "  Sample active prompt for feature 87768:\n",
      "    Prompt: the table structure is: create table classroom\n",
      " (building varchar(15) comment 'æ¥¼å®‡',\n",
      " room_number var...\n",
      "    Max activation: 3.9434289932250977\n",
      "    Active tokens: 1\n",
      "    Top token: '\n",
      "' (position 103)\n",
      "    Token activation: 3.9434289932250977\n",
      "\n",
      "Saving 1000 active prompts for feature 68824 to ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/68824/active.jsonl\n",
      "Saving 0 inactive prompts for feature 68824 to ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/68824/inactive.jsonl\n",
      "  Sample active prompt for feature 68824:\n",
      "    Prompt: the table structure is: create table classroom\n",
      " (building varchar(15) comment 'æ¥¼å®‡',\n",
      " room_number var...\n",
      "    Max activation: 5.933142185211182\n",
      "    Active tokens: 6\n",
      "    Top token: '\n",
      "' (position 68)\n",
      "    Token activation: 5.933142185211182\n",
      "\n",
      "Saving 1000 active prompts for feature 94940 to ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/94940/active.jsonl\n",
      "Saving 0 inactive prompts for feature 94940 to ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/94940/inactive.jsonl\n",
      "  Sample active prompt for feature 94940:\n",
      "    Prompt: the table structure is: create table classroom\n",
      " (building varchar(15) comment 'æ¥¼å®‡',\n",
      " room_number var...\n",
      "    Max activation: 37.76776123046875\n",
      "    Active tokens: 2\n",
      "    Top token: '<bos>' (position 0)\n",
      "    Token activation: 37.76776123046875\n",
      "\n",
      "Saving 1000 active prompts for feature 58461 to ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/58461/active.jsonl\n",
      "Saving 0 inactive prompts for feature 58461 to ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/58461/inactive.jsonl\n",
      "  Sample active prompt for feature 58461:\n",
      "    Prompt: the table structure is: create table classroom\n",
      " (building varchar(15) comment 'æ¥¼å®‡',\n",
      " room_number var...\n",
      "    Max activation: 8.151444435119629\n",
      "    Active tokens: 2\n",
      "    Top token: '<bos>' (position 0)\n",
      "    Token activation: 8.151444435119629\n",
      "\n",
      "Saving 1000 active prompts for feature 70370 to ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/70370/active.jsonl\n",
      "Saving 0 inactive prompts for feature 70370 to ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/70370/inactive.jsonl\n",
      "  Sample active prompt for feature 70370:\n",
      "    Prompt: the table structure is: create table classroom\n",
      " (building varchar(15) comment 'æ¥¼å®‡',\n",
      " room_number var...\n",
      "    Max activation: 23.193159103393555\n",
      "    Active tokens: 218\n",
      "    Top token: ' question' (position 71)\n",
      "    Token activation: 23.193159103393555\n",
      "\n",
      "Saving 1000 active prompts for feature 74980 to ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/74980/active.jsonl\n",
      "Saving 0 inactive prompts for feature 74980 to ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/74980/inactive.jsonl\n",
      "  Sample active prompt for feature 74980:\n",
      "    Prompt: the table structure is: create table classroom\n",
      " (building varchar(15) comment 'æ¥¼å®‡',\n",
      " room_number var...\n",
      "    Max activation: 10.714823722839355\n",
      "    Active tokens: 4\n",
      "    Top token: '\n",
      "' (position 103)\n",
      "    Token activation: 10.714823722839355\n",
      "\n",
      "Saving 493 active prompts for feature 41460 to ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/41460/active.jsonl\n",
      "Saving 507 inactive prompts for feature 41460 to ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/41460/inactive.jsonl\n",
      "  Sample active prompt for feature 41460:\n",
      "    Prompt: the table structure is: create table classroom\n",
      " (building varchar(15) comment 'æ¥¼å®‡',\n",
      " room_number var...\n",
      "    Max activation: 17.91721534729004\n",
      "    Active tokens: 1\n",
      "    Top token: 'model' (position 102)\n",
      "    Token activation: 17.91721534729004\n",
      "\n",
      "Saving 423 active prompts for feature 18420 to ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/18420/active.jsonl\n",
      "Saving 577 inactive prompts for feature 18420 to ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/18420/inactive.jsonl\n",
      "  Sample active prompt for feature 18420:\n",
      "    Prompt: Scrivi un testo, su questo tema: Nel mondo digitale in cui viviamo, spesso tendiamo a silenziare chi...\n",
      "    Max activation: 5.557797431945801\n",
      "    Active tokens: 2\n",
      "    Top token: '\n",
      "' (position 297)\n",
      "    Token activation: 5.557797431945801\n",
      "\n",
      "Saving 295 active prompts for feature 7418 to ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/7418/active.jsonl\n",
      "Saving 705 inactive prompts for feature 7418 to ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/7418/inactive.jsonl\n",
      "  Sample active prompt for feature 7418:\n",
      "    Prompt: ÐŸÑ€Ð¸Ð²ÐµÑ‚, ÐºÐ°ÐºÐ¾Ð¹ Ñ†Ð²ÐµÑ‚ ÑÐ°Ð¼Ñ‹Ð¹ Ð¿Ð¾Ð¿ÑƒÐ»ÑÑ€Ð½Ñ‹Ð¹ Ð² Ð¾Ð´ÐµÐ¶Ð´Ðµ?...\n",
      "    Max activation: 9.24249267578125\n",
      "    Active tokens: 2\n",
      "    Top token: '\n",
      "' (position 20)\n",
      "    Token activation: 9.24249267578125\n",
      "\n",
      "Saving 890 active prompts for feature 25340 to ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/25340/active.jsonl\n",
      "Saving 110 inactive prompts for feature 25340 to ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/25340/inactive.jsonl\n",
      "  Sample active prompt for feature 25340:\n",
      "    Prompt: the table structure is: create table classroom\n",
      " (building varchar(15) comment 'æ¥¼å®‡',\n",
      " room_number var...\n",
      "    Max activation: 12.889314651489258\n",
      "    Active tokens: 1\n",
      "    Top token: 'model' (position 102)\n",
      "    Token activation: 12.889314651489258\n",
      "\n",
      "Saving 1000 active prompts for feature 55935 to ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/55935/active.jsonl\n",
      "Saving 0 inactive prompts for feature 55935 to ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/55935/inactive.jsonl\n",
      "  Sample active prompt for feature 55935:\n",
      "    Prompt: the table structure is: create table classroom\n",
      " (building varchar(15) comment 'æ¥¼å®‡',\n",
      " room_number var...\n",
      "    Max activation: 23.879119873046875\n",
      "    Active tokens: 9\n",
      "    Top token: 'model' (position 102)\n",
      "    Token activation: 23.879119873046875\n",
      "\n",
      "âœ“ Results saved successfully for all 43 features!\n"
     ]
    }
   ],
   "source": [
    "# Save results for each feature in separate directories\n",
    "for feature_id in TARGET_FEATURES:\n",
    "    active_prompts, inactive_prompts = feature_results[feature_id]\n",
    "    \n",
    "    # Create feature-specific directory\n",
    "    feature_dir = f\"{BASE_OUTPUT_DIR}/{feature_id}\"\n",
    "    os.makedirs(feature_dir, exist_ok=True)\n",
    "    \n",
    "    # Define file paths\n",
    "    active_file = f\"{feature_dir}/active.jsonl\"\n",
    "    inactive_file = f\"{feature_dir}/inactive.jsonl\"\n",
    "    \n",
    "    # Save active prompts\n",
    "    print(f\"Saving {len(active_prompts)} active prompts for feature {feature_id} to {active_file}\")\n",
    "    with open(active_file, 'w') as f:\n",
    "        for prompt in active_prompts:\n",
    "            f.write(json.dumps(prompt) + '\\n')\n",
    "    \n",
    "    # Save inactive prompts\n",
    "    print(f\"Saving {len(inactive_prompts)} inactive prompts for feature {feature_id} to {inactive_file}\")\n",
    "    with open(inactive_file, 'w') as f:\n",
    "        for prompt in inactive_prompts:\n",
    "            f.write(json.dumps(prompt) + '\\n')\n",
    "    \n",
    "    # Show sample results for this feature\n",
    "    if active_prompts:\n",
    "        print(f\"  Sample active prompt for feature {feature_id}:\")\n",
    "        sample = active_prompts[0]\n",
    "        print(f\"    Prompt: {sample['prompt_text'][:100]}...\")\n",
    "        print(f\"    Max activation: {sample['max_feature_activation']}\")\n",
    "        print(f\"    Active tokens: {len(sample['tokens'])}\")\n",
    "        if sample['tokens']:\n",
    "            top_token = max(sample['tokens'], key=lambda x: x['feature_activation'])\n",
    "            print(f\"    Top token: '{top_token['text']}' (position {top_token['position']})\")\n",
    "            print(f\"    Token activation: {top_token['feature_activation']}\")\n",
    "        print()\n",
    "\n",
    "print(f\"âœ“ Results saved successfully for all {len(TARGET_FEATURES)} features!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated token-specific filtering functions defined\n"
     ]
    }
   ],
   "source": [
    "def find_assistant_position(input_ids: torch.Tensor, attention_mask: torch.Tensor, \n",
    "                          assistant_header: str, token_offset: int, tokenizer, device) -> int:\n",
    "    \"\"\"Find the position of the assistant token based on the given offset.\"\"\"\n",
    "    # Find assistant header position\n",
    "    assistant_tokens = tokenizer.encode(assistant_header, add_special_tokens=False)\n",
    "    \n",
    "    # Find where assistant section starts\n",
    "    assistant_pos = None\n",
    "    for k in range(len(input_ids) - len(assistant_tokens) + 1):\n",
    "        if torch.equal(input_ids[k:k+len(assistant_tokens)], torch.tensor(assistant_tokens, device=input_ids.device)):\n",
    "            assistant_pos = k + len(assistant_tokens) + token_offset\n",
    "            break\n",
    "    \n",
    "    if assistant_pos is None:\n",
    "        # Fallback to last non-padding token\n",
    "        assistant_pos = attention_mask.sum().item() - 1\n",
    "    \n",
    "    # Ensure position is within bounds\n",
    "    max_pos = attention_mask.sum().item() - 1\n",
    "    assistant_pos = min(assistant_pos, max_pos)\n",
    "    assistant_pos = max(assistant_pos, 0)\n",
    "    \n",
    "    return int(assistant_pos)\n",
    "\n",
    "@torch.no_grad()\n",
    "def filter_by_token_position(active_prompts: List[Dict], inactive_prompts: List[Dict], \n",
    "                           token_type: str, token_offset: int, feature_id: int,\n",
    "                           activation_threshold: float = 0.0) -> Tuple[List[Dict], List[Dict]]:\n",
    "    \"\"\"Filter prompts based on whether target feature is active at specific token positions.\"\"\"\n",
    "    \n",
    "    token_active = []\n",
    "    token_inactive = []\n",
    "    \n",
    "    # Process all prompts (both active and inactive from main analysis)\n",
    "    all_prompts = active_prompts + inactive_prompts\n",
    "    \n",
    "    for prompt_data in all_prompts:\n",
    "        prompt_text = prompt_data['prompt_text']\n",
    "        \n",
    "        # Use existing tokenized_prompt if available, otherwise tokenize\n",
    "        if 'tokenized_prompt' in prompt_data:\n",
    "            tokenized_prompt = prompt_data['tokenized_prompt']\n",
    "        else:\n",
    "            # Format as chat message to match processing\n",
    "            messages = [{\"role\": \"user\", \"content\": prompt_text}]\n",
    "            formatted_prompt = tokenizer.apply_chat_template(\n",
    "                messages, tokenize=False, add_generation_prompt=True\n",
    "            )\n",
    "            \n",
    "            # Tokenize to get input_ids and attention_mask\n",
    "            inputs = tokenizer(\n",
    "                formatted_prompt,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=MAX_LENGTH\n",
    "            )\n",
    "            \n",
    "            input_ids = inputs['input_ids'].squeeze(0)\n",
    "            \n",
    "            # Create tokenized prompt\n",
    "            tokenized_prompt = []\n",
    "            for token_id in input_ids:\n",
    "                if token_id != tokenizer.pad_token_id:  # Skip padding tokens\n",
    "                    token_text = tokenizer.decode([int(token_id)])\n",
    "                    tokenized_prompt.append(token_text)\n",
    "        \n",
    "        # For finding token position, we still need input_ids and attention_mask\n",
    "        messages = [{\"role\": \"user\", \"content\": prompt_text}]\n",
    "        formatted_prompt = tokenizer.apply_chat_template(\n",
    "            messages, tokenize=False, add_generation_prompt=True\n",
    "        )\n",
    "        \n",
    "        inputs = tokenizer(\n",
    "            formatted_prompt,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=MAX_LENGTH\n",
    "        )\n",
    "        \n",
    "        input_ids = inputs['input_ids'].squeeze(0)\n",
    "        attention_mask = inputs['attention_mask'].squeeze(0)\n",
    "        \n",
    "        # Find the specific token position\n",
    "        target_position = find_assistant_position(\n",
    "            input_ids, attention_mask, ASSISTANT_HEADER, token_offset, tokenizer, device\n",
    "        )\n",
    "        \n",
    "        # Check if this prompt has token data at the target position\n",
    "        has_activation_at_position = False\n",
    "        max_activation_at_position = 0.0\n",
    "        \n",
    "        # Check if this was an active prompt with token details\n",
    "        if 'tokens' in prompt_data:\n",
    "            for token_data in prompt_data['tokens']:\n",
    "                if token_data['position'] == target_position:\n",
    "                    activation = token_data['feature_activation']\n",
    "                    if activation > activation_threshold:\n",
    "                        has_activation_at_position = True\n",
    "                        max_activation_at_position = max(max_activation_at_position, activation)\n",
    "        \n",
    "        # Create token-specific record\n",
    "        token_record = {\n",
    "            'prompt_id': prompt_data['prompt_id'],\n",
    "            'prompt_text': prompt_data['prompt_text'],\n",
    "            'tokenized_prompt': tokenized_prompt,\n",
    "            'token_type': token_type,\n",
    "            'token_position': target_position,\n",
    "            'max_feature_activation': prompt_data['max_feature_activation']\n",
    "        }\n",
    "        \n",
    "        if has_activation_at_position:\n",
    "            token_record['max_activation_at_position'] = max_activation_at_position\n",
    "            # Include token details for this specific position\n",
    "            position_tokens = []\n",
    "            if 'tokens' in prompt_data:\n",
    "                for token_data in prompt_data['tokens']:\n",
    "                    if token_data['position'] == target_position:\n",
    "                        position_tokens.append(token_data)\n",
    "            token_record['position_tokens'] = position_tokens\n",
    "            token_active.append(token_record)\n",
    "        else:\n",
    "            token_record['max_activation_at_position'] = max_activation_at_position\n",
    "            token_inactive.append(token_record)\n",
    "    \n",
    "    return token_active, token_inactive\n",
    "\n",
    "print(\"Updated token-specific filtering functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing token-specific analysis for 43 features and 2 token types...\n",
      "\n",
      "Processing feature 49536:\n",
      "  Processing token type: model (offset: -1)\n",
      "    Active prompts at model position: 696\n",
      "    Inactive prompts at model position: 304\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/49536/active_model.jsonl\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/49536/inactive_model.jsonl\n",
      "  Processing token type: newline (offset: 0)\n",
      "    Active prompts at newline position: 255\n",
      "    Inactive prompts at newline position: 745\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/49536/active_newline.jsonl\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/49536/inactive_newline.jsonl\n",
      "\n",
      "Processing feature 51330:\n",
      "  Processing token type: model (offset: -1)\n",
      "    Active prompts at model position: 92\n",
      "    Inactive prompts at model position: 908\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/51330/active_model.jsonl\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/51330/inactive_model.jsonl\n",
      "  Processing token type: newline (offset: 0)\n",
      "    Active prompts at newline position: 346\n",
      "    Inactive prompts at newline position: 654\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/51330/active_newline.jsonl\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/51330/inactive_newline.jsonl\n",
      "\n",
      "Processing feature 45315:\n",
      "  Processing token type: model (offset: -1)\n",
      "    Active prompts at model position: 491\n",
      "    Inactive prompts at model position: 509\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/45315/active_model.jsonl\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/45315/inactive_model.jsonl\n",
      "  Processing token type: newline (offset: 0)\n",
      "    Active prompts at newline position: 220\n",
      "    Inactive prompts at newline position: 780\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/45315/active_newline.jsonl\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/45315/inactive_newline.jsonl\n",
      "\n",
      "Processing feature 83076:\n",
      "  Processing token type: model (offset: -1)\n",
      "    Active prompts at model position: 513\n",
      "    Inactive prompts at model position: 487\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/83076/active_model.jsonl\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/83076/inactive_model.jsonl\n",
      "  Processing token type: newline (offset: 0)\n",
      "    Active prompts at newline position: 100\n",
      "    Inactive prompts at newline position: 900\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/83076/active_newline.jsonl\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/83076/inactive_newline.jsonl\n",
      "\n",
      "Processing feature 81156:\n",
      "  Processing token type: model (offset: -1)\n",
      "    Active prompts at model position: 554\n",
      "    Inactive prompts at model position: 446\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/81156/active_model.jsonl\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/81156/inactive_model.jsonl\n",
      "  Processing token type: newline (offset: 0)\n",
      "    Active prompts at newline position: 216\n",
      "    Inactive prompts at newline position: 784\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/81156/active_newline.jsonl\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/81156/inactive_newline.jsonl\n",
      "\n",
      "Processing feature 88838:\n",
      "  Processing token type: model (offset: -1)\n",
      "    Active prompts at model position: 643\n",
      "    Inactive prompts at model position: 357\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/88838/active_model.jsonl\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/88838/inactive_model.jsonl\n",
      "  Processing token type: newline (offset: 0)\n",
      "    Active prompts at newline position: 210\n",
      "    Inactive prompts at newline position: 790\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/88838/active_newline.jsonl\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/88838/inactive_newline.jsonl\n",
      "\n",
      "Processing feature 128007:\n",
      "  Processing token type: model (offset: -1)\n",
      "    Active prompts at model position: 662\n",
      "    Inactive prompts at model position: 338\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/128007/active_model.jsonl\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/128007/inactive_model.jsonl\n",
      "  Processing token type: newline (offset: 0)\n",
      "    Active prompts at newline position: 266\n",
      "    Inactive prompts at newline position: 734\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/128007/active_newline.jsonl\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/128007/inactive_newline.jsonl\n",
      "\n",
      "Processing feature 106376:\n",
      "  Processing token type: model (offset: -1)\n",
      "    Active prompts at model position: 990\n",
      "    Inactive prompts at model position: 10\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/106376/active_model.jsonl\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/106376/inactive_model.jsonl\n",
      "  Processing token type: newline (offset: 0)\n",
      "    Active prompts at newline position: 983\n",
      "    Inactive prompts at newline position: 17\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/106376/active_newline.jsonl\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/106376/inactive_newline.jsonl\n",
      "\n",
      "Processing feature 89358:\n",
      "  Processing token type: model (offset: -1)\n",
      "    Active prompts at model position: 0\n",
      "    Inactive prompts at model position: 1000\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/89358/active_model.jsonl\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/89358/inactive_model.jsonl\n",
      "  Processing token type: newline (offset: 0)\n",
      "    Active prompts at newline position: 653\n",
      "    Inactive prompts at newline position: 347\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/89358/active_newline.jsonl\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/89358/inactive_newline.jsonl\n",
      "\n",
      "Processing feature 66831:\n",
      "  Processing token type: model (offset: -1)\n",
      "    Active prompts at model position: 984\n",
      "    Inactive prompts at model position: 16\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/66831/active_model.jsonl\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/66831/inactive_model.jsonl\n",
      "  Processing token type: newline (offset: 0)\n",
      "    Active prompts at newline position: 751\n",
      "    Inactive prompts at newline position: 249\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/66831/active_newline.jsonl\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/66831/inactive_newline.jsonl\n",
      "\n",
      "Processing feature 43662:\n",
      "  Processing token type: model (offset: -1)\n",
      "    Active prompts at model position: 1\n",
      "    Inactive prompts at model position: 999\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/43662/active_model.jsonl\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/43662/inactive_model.jsonl\n",
      "  Processing token type: newline (offset: 0)\n",
      "    Active prompts at newline position: 479\n",
      "    Inactive prompts at newline position: 521\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/43662/active_newline.jsonl\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/43662/inactive_newline.jsonl\n",
      "\n",
      "Processing feature 116246:\n",
      "  Processing token type: model (offset: -1)\n",
      "    Active prompts at model position: 994\n",
      "    Inactive prompts at model position: 6\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/116246/active_model.jsonl\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/116246/inactive_model.jsonl\n",
      "  Processing token type: newline (offset: 0)\n",
      "    Active prompts at newline position: 994\n",
      "    Inactive prompts at newline position: 6\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/116246/active_newline.jsonl\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/116246/inactive_newline.jsonl\n",
      "\n",
      "Processing feature 110358:\n",
      "  Processing token type: model (offset: -1)\n",
      "    Active prompts at model position: 491\n",
      "    Inactive prompts at model position: 509\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/110358/active_model.jsonl\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/110358/inactive_model.jsonl\n",
      "  Processing token type: newline (offset: 0)\n",
      "    Active prompts at newline position: 406\n",
      "    Inactive prompts at newline position: 594\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/110358/active_newline.jsonl\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/110358/inactive_newline.jsonl\n",
      "\n",
      "Processing feature 49818:\n",
      "  Processing token type: model (offset: -1)\n",
      "    Active prompts at model position: 132\n",
      "    Inactive prompts at model position: 868\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/49818/active_model.jsonl\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/49818/inactive_model.jsonl\n",
      "  Processing token type: newline (offset: 0)\n",
      "    Active prompts at newline position: 126\n",
      "    Inactive prompts at newline position: 874\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/49818/active_newline.jsonl\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/49818/inactive_newline.jsonl\n",
      "\n",
      "Processing feature 4385:\n",
      "  Processing token type: model (offset: -1)\n",
      "    Active prompts at model position: 657\n",
      "    Inactive prompts at model position: 343\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/4385/active_model.jsonl\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/4385/inactive_model.jsonl\n",
      "  Processing token type: newline (offset: 0)\n",
      "    Active prompts at newline position: 329\n",
      "    Inactive prompts at newline position: 671\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/4385/active_newline.jsonl\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/4385/inactive_newline.jsonl\n",
      "\n",
      "Processing feature 40097:\n",
      "  Processing token type: model (offset: -1)\n",
      "    Active prompts at model position: 154\n",
      "    Inactive prompts at model position: 846\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/40097/active_model.jsonl\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/40097/inactive_model.jsonl\n",
      "  Processing token type: newline (offset: 0)\n",
      "    Active prompts at newline position: 298\n",
      "    Inactive prompts at newline position: 702\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/40097/active_newline.jsonl\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/40097/inactive_newline.jsonl\n",
      "\n",
      "Processing feature 1699:\n",
      "  Processing token type: model (offset: -1)\n",
      "    Active prompts at model position: 481\n",
      "    Inactive prompts at model position: 519\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/1699/active_model.jsonl\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/1699/inactive_model.jsonl\n",
      "  Processing token type: newline (offset: 0)\n",
      "    Active prompts at newline position: 36\n",
      "    Inactive prompts at newline position: 964\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/1699/active_newline.jsonl\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/1699/inactive_newline.jsonl\n",
      "\n",
      "Processing feature 11171:\n",
      "  Processing token type: model (offset: -1)\n",
      "    Active prompts at model position: 197\n",
      "    Inactive prompts at model position: 803\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/11171/active_model.jsonl\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/11171/inactive_model.jsonl\n",
      "  Processing token type: newline (offset: 0)\n",
      "    Active prompts at newline position: 378\n",
      "    Inactive prompts at newline position: 622\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/11171/active_newline.jsonl\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/11171/inactive_newline.jsonl\n",
      "\n",
      "Processing feature 59687:\n",
      "  Processing token type: model (offset: -1)\n",
      "    Active prompts at model position: 395\n",
      "    Inactive prompts at model position: 605\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/59687/active_model.jsonl\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/59687/inactive_model.jsonl\n",
      "  Processing token type: newline (offset: 0)\n",
      "    Active prompts at newline position: 248\n",
      "    Inactive prompts at newline position: 752\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/59687/active_newline.jsonl\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/59687/inactive_newline.jsonl\n",
      "\n",
      "Processing feature 89132:\n",
      "  Processing token type: model (offset: -1)\n",
      "    Active prompts at model position: 328\n",
      "    Inactive prompts at model position: 672\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/89132/active_model.jsonl\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/89132/inactive_model.jsonl\n",
      "  Processing token type: newline (offset: 0)\n",
      "    Active prompts at newline position: 621\n",
      "    Inactive prompts at newline position: 379\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/89132/active_newline.jsonl\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/89132/inactive_newline.jsonl\n",
      "\n",
      "Processing feature 53933:\n",
      "  Processing token type: model (offset: -1)\n",
      "    Active prompts at model position: 128\n",
      "    Inactive prompts at model position: 872\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/53933/active_model.jsonl\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/53933/inactive_model.jsonl\n",
      "  Processing token type: newline (offset: 0)\n",
      "    Active prompts at newline position: 282\n",
      "    Inactive prompts at newline position: 718\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/53933/active_newline.jsonl\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/53933/inactive_newline.jsonl\n",
      "\n",
      "Processing feature 63154:\n",
      "  Processing token type: model (offset: -1)\n",
      "    Active prompts at model position: 530\n",
      "    Inactive prompts at model position: 470\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/63154/active_model.jsonl\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/63154/inactive_model.jsonl\n",
      "  Processing token type: newline (offset: 0)\n",
      "    Active prompts at newline position: 378\n",
      "    Inactive prompts at newline position: 622\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/63154/active_newline.jsonl\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/63154/inactive_newline.jsonl\n",
      "\n",
      "Processing feature 38451:\n",
      "  Processing token type: model (offset: -1)\n",
      "    Active prompts at model position: 12\n",
      "    Inactive prompts at model position: 988\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/38451/active_model.jsonl\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/38451/inactive_model.jsonl\n",
      "  Processing token type: newline (offset: 0)\n",
      "    Active prompts at newline position: 501\n",
      "    Inactive prompts at newline position: 499\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/38451/active_newline.jsonl\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/38451/inactive_newline.jsonl\n",
      "\n",
      "Processing feature 29239:\n",
      "  Processing token type: model (offset: -1)\n",
      "    Active prompts at model position: 272\n",
      "    Inactive prompts at model position: 728\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/29239/active_model.jsonl\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/29239/inactive_model.jsonl\n",
      "  Processing token type: newline (offset: 0)\n",
      "    Active prompts at newline position: 365\n",
      "    Inactive prompts at newline position: 635\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/29239/active_newline.jsonl\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/29239/inactive_newline.jsonl\n",
      "\n",
      "Processing feature 118967:\n",
      "  Processing token type: model (offset: -1)\n",
      "    Active prompts at model position: 234\n",
      "    Inactive prompts at model position: 766\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/118967/active_model.jsonl\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/118967/inactive_model.jsonl\n",
      "  Processing token type: newline (offset: 0)\n",
      "    Active prompts at newline position: 306\n",
      "    Inactive prompts at newline position: 694\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/118967/active_newline.jsonl\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/118967/inactive_newline.jsonl\n",
      "\n",
      "Processing feature 130744:\n",
      "  Processing token type: model (offset: -1)\n",
      "    Active prompts at model position: 0\n",
      "    Inactive prompts at model position: 1000\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/130744/active_model.jsonl\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/130744/inactive_model.jsonl\n",
      "  Processing token type: newline (offset: 0)\n",
      "    Active prompts at newline position: 398\n",
      "    Inactive prompts at newline position: 602\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/130744/active_newline.jsonl\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/130744/inactive_newline.jsonl\n",
      "\n",
      "Processing feature 20668:\n",
      "  Processing token type: model (offset: -1)\n",
      "    Active prompts at model position: 985\n",
      "    Inactive prompts at model position: 15\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/20668/active_model.jsonl\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/20668/inactive_model.jsonl\n",
      "  Processing token type: newline (offset: 0)\n",
      "    Active prompts at newline position: 985\n",
      "    Inactive prompts at newline position: 15\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/20668/active_newline.jsonl\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/20668/inactive_newline.jsonl\n",
      "\n",
      "Processing feature 49865:\n",
      "  Processing token type: model (offset: -1)\n",
      "    Active prompts at model position: 978\n",
      "    Inactive prompts at model position: 22\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/49865/active_model.jsonl\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/49865/inactive_model.jsonl\n",
      "  Processing token type: newline (offset: 0)\n",
      "    Active prompts at newline position: 932\n",
      "    Inactive prompts at newline position: 68\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/49865/active_newline.jsonl\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/49865/inactive_newline.jsonl\n",
      "\n",
      "Processing feature 1738:\n",
      "  Processing token type: model (offset: -1)\n",
      "    Active prompts at model position: 0\n",
      "    Inactive prompts at model position: 1000\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/1738/active_model.jsonl\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/1738/inactive_model.jsonl\n",
      "  Processing token type: newline (offset: 0)\n",
      "    Active prompts at newline position: 404\n",
      "    Inactive prompts at newline position: 596\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/1738/active_newline.jsonl\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/1738/inactive_newline.jsonl\n",
      "\n",
      "Processing feature 61774:\n",
      "  Processing token type: model (offset: -1)\n",
      "    Active prompts at model position: 821\n",
      "    Inactive prompts at model position: 179\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/61774/active_model.jsonl\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/61774/inactive_model.jsonl\n",
      "  Processing token type: newline (offset: 0)\n",
      "    Active prompts at newline position: 303\n",
      "    Inactive prompts at newline position: 697\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/61774/active_newline.jsonl\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/61774/inactive_newline.jsonl\n",
      "\n",
      "Processing feature 123729:\n",
      "  Processing token type: model (offset: -1)\n",
      "    Active prompts at model position: 857\n",
      "    Inactive prompts at model position: 143\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/123729/active_model.jsonl\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/123729/inactive_model.jsonl\n",
      "  Processing token type: newline (offset: 0)\n",
      "    Active prompts at newline position: 3\n",
      "    Inactive prompts at newline position: 997\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/123729/active_newline.jsonl\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/123729/inactive_newline.jsonl\n",
      "\n",
      "Processing feature 39123:\n",
      "  Processing token type: model (offset: -1)\n",
      "    Active prompts at model position: 772\n",
      "    Inactive prompts at model position: 228\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/39123/active_model.jsonl\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/39123/inactive_model.jsonl\n",
      "  Processing token type: newline (offset: 0)\n",
      "    Active prompts at newline position: 131\n",
      "    Inactive prompts at newline position: 869\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/39123/active_newline.jsonl\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/39123/inactive_newline.jsonl\n",
      "\n",
      "Processing feature 87768:\n",
      "  Processing token type: model (offset: -1)\n",
      "    Active prompts at model position: 62\n",
      "    Inactive prompts at model position: 938\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/87768/active_model.jsonl\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/87768/inactive_model.jsonl\n",
      "  Processing token type: newline (offset: 0)\n",
      "    Active prompts at newline position: 800\n",
      "    Inactive prompts at newline position: 200\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/87768/active_newline.jsonl\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/87768/inactive_newline.jsonl\n",
      "\n",
      "Processing feature 68824:\n",
      "  Processing token type: model (offset: -1)\n",
      "    Active prompts at model position: 652\n",
      "    Inactive prompts at model position: 348\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/68824/active_model.jsonl\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/68824/inactive_model.jsonl\n",
      "  Processing token type: newline (offset: 0)\n",
      "    Active prompts at newline position: 281\n",
      "    Inactive prompts at newline position: 719\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/68824/active_newline.jsonl\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/68824/inactive_newline.jsonl\n",
      "\n",
      "Processing feature 94940:\n",
      "  Processing token type: model (offset: -1)\n",
      "    Active prompts at model position: 198\n",
      "    Inactive prompts at model position: 802\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/94940/active_model.jsonl\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/94940/inactive_model.jsonl\n",
      "  Processing token type: newline (offset: 0)\n",
      "    Active prompts at newline position: 373\n",
      "    Inactive prompts at newline position: 627\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/94940/active_newline.jsonl\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/94940/inactive_newline.jsonl\n",
      "\n",
      "Processing feature 58461:\n",
      "  Processing token type: model (offset: -1)\n",
      "    Active prompts at model position: 72\n",
      "    Inactive prompts at model position: 928\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/58461/active_model.jsonl\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/58461/inactive_model.jsonl\n",
      "  Processing token type: newline (offset: 0)\n",
      "    Active prompts at newline position: 491\n",
      "    Inactive prompts at newline position: 509\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/58461/active_newline.jsonl\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/58461/inactive_newline.jsonl\n",
      "\n",
      "Processing feature 70370:\n",
      "  Processing token type: model (offset: -1)\n",
      "    Active prompts at model position: 669\n",
      "    Inactive prompts at model position: 331\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/70370/active_model.jsonl\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/70370/inactive_model.jsonl\n",
      "  Processing token type: newline (offset: 0)\n",
      "    Active prompts at newline position: 568\n",
      "    Inactive prompts at newline position: 432\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/70370/active_newline.jsonl\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/70370/inactive_newline.jsonl\n",
      "\n",
      "Processing feature 74980:\n",
      "  Processing token type: model (offset: -1)\n",
      "    Active prompts at model position: 322\n",
      "    Inactive prompts at model position: 678\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/74980/active_model.jsonl\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/74980/inactive_model.jsonl\n",
      "  Processing token type: newline (offset: 0)\n",
      "    Active prompts at newline position: 354\n",
      "    Inactive prompts at newline position: 646\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/74980/active_newline.jsonl\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/74980/inactive_newline.jsonl\n",
      "\n",
      "Processing feature 41460:\n",
      "  Processing token type: model (offset: -1)\n",
      "    Active prompts at model position: 478\n",
      "    Inactive prompts at model position: 522\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/41460/active_model.jsonl\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/41460/inactive_model.jsonl\n",
      "  Processing token type: newline (offset: 0)\n",
      "    Active prompts at newline position: 139\n",
      "    Inactive prompts at newline position: 861\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/41460/active_newline.jsonl\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/41460/inactive_newline.jsonl\n",
      "\n",
      "Processing feature 18420:\n",
      "  Processing token type: model (offset: -1)\n",
      "    Active prompts at model position: 120\n",
      "    Inactive prompts at model position: 880\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/18420/active_model.jsonl\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/18420/inactive_model.jsonl\n",
      "  Processing token type: newline (offset: 0)\n",
      "    Active prompts at newline position: 378\n",
      "    Inactive prompts at newline position: 622\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/18420/active_newline.jsonl\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/18420/inactive_newline.jsonl\n",
      "\n",
      "Processing feature 7418:\n",
      "  Processing token type: model (offset: -1)\n",
      "    Active prompts at model position: 94\n",
      "    Inactive prompts at model position: 906\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/7418/active_model.jsonl\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/7418/inactive_model.jsonl\n",
      "  Processing token type: newline (offset: 0)\n",
      "    Active prompts at newline position: 272\n",
      "    Inactive prompts at newline position: 728\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/7418/active_newline.jsonl\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/7418/inactive_newline.jsonl\n",
      "\n",
      "Processing feature 25340:\n",
      "  Processing token type: model (offset: -1)\n",
      "    Active prompts at model position: 872\n",
      "    Inactive prompts at model position: 128\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/25340/active_model.jsonl\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/25340/inactive_model.jsonl\n",
      "  Processing token type: newline (offset: 0)\n",
      "    Active prompts at newline position: 132\n",
      "    Inactive prompts at newline position: 868\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/25340/active_newline.jsonl\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/25340/inactive_newline.jsonl\n",
      "\n",
      "Processing feature 55935:\n",
      "  Processing token type: model (offset: -1)\n",
      "    Active prompts at model position: 962\n",
      "    Inactive prompts at model position: 38\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/55935/active_model.jsonl\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/55935/inactive_model.jsonl\n",
      "  Processing token type: newline (offset: 0)\n",
      "    Active prompts at newline position: 724\n",
      "    Inactive prompts at newline position: 276\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/55935/active_newline.jsonl\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/55935/inactive_newline.jsonl\n",
      "\n",
      "âœ“ Token-specific analysis complete for all features!\n",
      "Results saved in feature-specific directories under: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts\n",
      "Each feature directory contains:\n",
      "  - active.jsonl / inactive.jsonl (general)\n",
      "  - active_model.jsonl / inactive_model.jsonl (position-specific)\n",
      "  - active_newline.jsonl / inactive_newline.jsonl (position-specific)\n"
     ]
    }
   ],
   "source": [
    "# Process token-specific analysis for each feature and token type\n",
    "print(f\"Processing token-specific analysis for {len(TARGET_FEATURES)} features and {len(TOKEN_OFFSETS)} token types...\")\n",
    "\n",
    "for feature_id in TARGET_FEATURES:\n",
    "    active_prompts, inactive_prompts = feature_results[feature_id]\n",
    "    feature_dir = f\"{BASE_OUTPUT_DIR}/{feature_id}\"\n",
    "    \n",
    "    print(f\"\\nProcessing feature {feature_id}:\")\n",
    "    \n",
    "    for token_type, token_offset in TOKEN_OFFSETS.items():\n",
    "        print(f\"  Processing token type: {token_type} (offset: {token_offset})\")\n",
    "        \n",
    "        # Filter prompts based on activation at this specific token position\n",
    "        token_active, token_inactive = filter_by_token_position(\n",
    "            active_prompts, inactive_prompts, \n",
    "            token_type, token_offset, feature_id, ACTIVATION_THRESHOLD\n",
    "        )\n",
    "        \n",
    "        # Save results for this token type\n",
    "        active_file = f\"{feature_dir}/active_{token_type}.jsonl\"\n",
    "        inactive_file = f\"{feature_dir}/inactive_{token_type}.jsonl\"\n",
    "        \n",
    "        print(f\"    Active prompts at {token_type} position: {len(token_active)}\")\n",
    "        print(f\"    Inactive prompts at {token_type} position: {len(token_inactive)}\")\n",
    "        \n",
    "        # Save active prompts for this token type\n",
    "        with open(active_file, 'w') as f:\n",
    "            for prompt in token_active:\n",
    "                f.write(json.dumps(prompt) + '\\n')\n",
    "        \n",
    "        # Save inactive prompts for this token type\n",
    "        with open(inactive_file, 'w') as f:\n",
    "            for prompt in token_inactive:\n",
    "                f.write(json.dumps(prompt) + '\\n')\n",
    "        \n",
    "        print(f\"    Saved: {active_file}\")\n",
    "        print(f\"    Saved: {inactive_file}\")\n",
    "\n",
    "\n",
    "print(f\"\\nâœ“ Token-specific analysis complete for all features!\")\n",
    "print(f\"Results saved in feature-specific directories under: {BASE_OUTPUT_DIR}\")\n",
    "print(f\"Each feature directory contains:\")\n",
    "print(f\"  - active.jsonl / inactive.jsonl (general)\")\n",
    "for token_type in TOKEN_OFFSETS.keys():\n",
    "    print(f\"  - active_{token_type}.jsonl / inactive_{token_type}.jsonl (position-specific)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
