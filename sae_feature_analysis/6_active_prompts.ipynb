{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Analysis: Token-level activations for target features\n",
    "\n",
    "This notebook analyzes token-level activations for specific SAE features on prompts, using a two-stage approach:\n",
    "1. Screen all prompts for target feature activation\n",
    "2. Extract detailed token activations only for prompts where target features fire\n",
    "\n",
    "Saves results to `active.jsonl` and `inactive.jsonl`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import json\n",
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from huggingface_hub import hf_hub_download\n",
    "from dictionary_learning.utils import load_dictionary\n",
    "from tqdm.auto import tqdm\n",
    "from sae_lens import SAE\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from dataclasses import dataclass\nfrom typing import Dict, Tuple, Optional\n\n@dataclass\nclass ModelConfig:\n    \"\"\"Configuration for model-specific settings\"\"\"\n    base_model_name: str\n    chat_model_name: str\n    hf_release: str  # Reference only - actual loading uses saelens_release/sae_id\n    assistant_header: str\n    token_offsets: Dict[str, int]\n    sae_base_path: str\n    saelens_release: str  # Template for sae_lens release parameter\n    sae_id_template: str  # Template for sae_lens sae_id parameter\n    base_url: str  # Base URL for neuronpedia\n    \n    def get_sae_params(self, sae_layer: int, sae_trainer: str) -> Tuple[str, str]:\n        \"\"\"\n        Generate SAE lens release and sae_id parameters.\n        \n        Args:\n            sae_layer: Layer number for the SAE\n            sae_trainer: Trainer identifier for the SAE\n            \n        Returns:\n            Tuple of (release, sae_id) for sae_lens.SAE.from_pretrained()\n        \"\"\"\n        if self.saelens_release == \"llama_scope_lxr_{trainer}\":\n            release = self.saelens_release.format(trainer=sae_trainer)\n            sae_id = self.sae_id_template.format(layer=sae_layer, trainer=sae_trainer)\n        elif self.saelens_release == \"gemma-scope-9b-pt-res\":\n            # Parse SAE_TRAINER \"131k-l0-34\" into components for Gemma\n            parts = sae_trainer.split(\"-\")\n            width = parts[0]  # \"131k\"\n            l0_value = parts[2]  # \"34\"\n            \n            release = self.saelens_release\n            sae_id = self.sae_id_template.format(layer=sae_layer, width=width, l0=l0_value)\n        elif self.saelens_release == \"gemma-scope-9b-pt-res-canonical\":\n            # Parse SAE_TRAINER \"131k-l0-34\" into components for Gemma\n            parts = sae_trainer.split(\"-\")\n            width = parts[0]  # \"131k\"\n\n            release = self.saelens_release\n            sae_id = self.sae_id_template.format(layer=sae_layer, width=width)\n        else:\n            raise ValueError(f\"Unknown SAE lens release template: {self.saelens_release}\")\n        \n        return release, sae_id\n\n# Model configurations\nMODEL_CONFIGS = {\n    \"llama\": ModelConfig(\n        base_model_name=\"meta-llama/Llama-3.1-8B\",\n        chat_model_name=\"meta-llama/Llama-3.1-8B-Instruct\",\n        hf_release=\"fnlp/Llama3_1-8B-Base-LXR-32x\",\n        assistant_header=\"<|start_header_id|>assistant<|end_header_id|>\",\n        token_offsets={\"asst\": -2, \"endheader\": -1, \"newline\": 0},\n        sae_base_path=\"/workspace/sae/llama-3.1-8b/saes\",\n        saelens_release=\"llama_scope_lxr_{trainer}\",\n        sae_id_template=\"l{layer}r_{trainer}\",\n        base_url=\"https://www.neuronpedia.org/llama-3.1-8b/{layer}-llamascope-res-131k\"\n    ),\n    \"gemma\": ModelConfig(\n        base_model_name=\"google/gemma-2-9b\",\n        chat_model_name=\"google/gemma-2-9b-it\",\n        hf_release=\"google/gemma-scope-9b-pt-res/layer_{layer}/width_{width}/average_l0_{l0}\",\n        assistant_header=\"<start_of_turn>model\",\n        token_offsets={\"model\": -1, \"newline\": 0},\n        sae_base_path=\"/workspace/sae/gemma-2-9b/saes\",\n        saelens_release=\"gemma-scope-9b-pt-res-canonical\",\n        sae_id_template=\"layer_{layer}/width_{width}/canonical\",\n        base_url=\"https://www.neuronpedia.org/gemma-2-9b/{layer}-gemmascope-res-131k\"\n    )\n}\n\n# =============================================================================\n# MODEL SELECTION - Change this to switch between models\n# =============================================================================\nMODEL_TYPE = \"gemma\"  # Options: \"gemma\" or \"llama\"\nMODEL_VER = \"chat\"\nSAE_LAYER = 20\nSAE_TRAINER = \"131k-l0-114\"\nN_PROMPTS = 1000\n\n# =============================================================================\n# TARGET FEATURES - Specify which features to analyze\n# =============================================================================\nTARGET_FEATURES = [91547, 65116, 85422, 80134, 74855, 71187, 102414, 10392, 128628, 8524, 57516, 21953, 26196, 90900, 11383, 111921, 74079]  # List of feature IDs to analyze\nACTIVATION_THRESHOLD = 0.0  # Minimum activation to consider \"active\"\n\n# =============================================================================\n# DEDUPLICATION AND CONFIGURATION SETUP\n# =============================================================================\nif MODEL_TYPE not in MODEL_CONFIGS:\n    raise ValueError(f\"Unknown MODEL_TYPE: {MODEL_TYPE}. Available: {list(MODEL_CONFIGS.keys())}\")\n\nconfig = MODEL_CONFIGS[MODEL_TYPE]\n\n# Deduplicate TARGET_FEATURES while preserving order\noriginal_count = len(TARGET_FEATURES)\nTARGET_FEATURES = list(dict.fromkeys(TARGET_FEATURES))  # Preserves order, removes duplicates\nduplicates_removed = original_count - len(TARGET_FEATURES)\n\nif duplicates_removed > 0:\n    print(f\"ðŸ”„ Removed {duplicates_removed} duplicate feature(s) from TARGET_FEATURES\")\n\n# Set model name based on version\nif MODEL_VER == \"chat\":\n    MODEL_NAME = config.chat_model_name\nelif MODEL_VER == \"base\":\n    MODEL_NAME = config.base_model_name\nelse:\n    raise ValueError(f\"Unknown MODEL_VER: {MODEL_VER}. Use 'chat' or 'base'\")\n\n# Always use chat model for tokenizer (has chat template)\nCHAT_MODEL_NAME = config.chat_model_name\n\n# Set up derived configurations\nASSISTANT_HEADER = config.assistant_header\nTOKEN_OFFSETS = config.token_offsets\nSAE_BASE_PATH = config.sae_base_path\n\n# =============================================================================\n# OUTPUT FILE CONFIGURATION\n# =============================================================================\n# Base output directory - individual feature directories will be created under this\nBASE_OUTPUT_DIR = f\"./results/6_active_prompts/{MODEL_TYPE}_trainer{SAE_TRAINER}_layer{SAE_LAYER}/{N_PROMPTS}_prompts\"\n\n# =============================================================================\n# CHECK FOR EXISTING DIRECTORIES AND FILTER TARGET FEATURES\n# =============================================================================\noriginal_target_features = TARGET_FEATURES.copy()\nfiltered_target_features = []\nexisting_features = []\n\nfor feature_id in TARGET_FEATURES:\n    feature_dir = f\"{BASE_OUTPUT_DIR}/{feature_id}\"\n    if os.path.exists(feature_dir):\n        existing_features.append(feature_id)\n        print(f\"âš ï¸  WARNING: Directory already exists for feature {feature_id}, skipping: {feature_dir}\")\n    else:\n        filtered_target_features.append(feature_id)\n\n# Update TARGET_FEATURES to only include features that don't have existing directories\nTARGET_FEATURES = filtered_target_features\n\nif existing_features:\n    print(f\"\\nðŸ”„ Skipped {len(existing_features)} existing features: {existing_features}\")\n    \nif not TARGET_FEATURES:\n    print(f\"\\nâŒ No new features to process - all {len(original_target_features)} features already have existing directories!\")\n    print(\"To reprocess existing features, delete their directories first.\")\nelse:\n    print(f\"\\nâœ… Will process {len(TARGET_FEATURES)} new features: {TARGET_FEATURES}\")\n\n# =============================================================================\n# DERIVED CONFIGURATIONS\n# =============================================================================\nSAE_PATH = f\"{SAE_BASE_PATH}/resid_post_layer_{SAE_LAYER}/trainer_{SAE_TRAINER}\"\nLAYER_INDEX = SAE_LAYER\n\n# Data paths\nPROMPTS_HF = \"lmsys/lmsys-chat-1m\"\nSEED = 42\nPROMPTS_PATH = f\"/workspace/data/{PROMPTS_HF.split('/')[-1]}/chat_{N_PROMPTS}.jsonl\"\nos.makedirs(os.path.dirname(PROMPTS_PATH), exist_ok=True)\n\n# Processing parameters\nBATCH_SIZE = 32\nMAX_LENGTH = 512\n\n# =============================================================================\n# SUMMARY\n# =============================================================================\nprint(f\"\\nConfiguration Summary:\")\nprint(f\"  Model Type: {MODEL_TYPE}\")\nprint(f\"  Model to load: {MODEL_NAME}\")\nprint(f\"  SAE Layer: {SAE_LAYER}, Trainer: {SAE_TRAINER}\")\nprint(f\"  Target Features: {TARGET_FEATURES}\")\nprint(f\"  Activation Threshold: {ACTIVATION_THRESHOLD}\")\nprint(f\"  Base Output Directory: {BASE_OUTPUT_DIR}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompts already exist at /workspace/data/lmsys-chat-1m/chat_1000.jsonl\n",
      "Loaded 1000 prompts\n"
     ]
    }
   ],
   "source": [
    "def load_lmsys_prompts(prompts_path: str, prompts_hf: str, n_prompts: int, seed: int) -> pd.DataFrame:\n",
    "    # Check if prompts_path exists\n",
    "    if os.path.exists(prompts_path):\n",
    "        print(f\"Prompts already exist at {prompts_path}\")\n",
    "        return pd.read_json(prompts_path, lines=True)\n",
    "    else:\n",
    "        print(f\"Prompts do not exist at {prompts_path}. Loading from {prompts_hf}...\")\n",
    "        dataset = load_dataset(prompts_hf)\n",
    "        dataset = dataset['train'].shuffle(seed=seed).select(range(n_prompts))\n",
    "        df = dataset.to_pandas()\n",
    "\n",
    "        # Extract the prompt from the first conversation item\n",
    "        df['prompt'] = df['conversation'].apply(lambda x: x[0]['content'])\n",
    "\n",
    "        # Only keep some columns\n",
    "        df = df[['conversation_id', 'prompt', 'redacted', 'language']]\n",
    "\n",
    "        # Save to .jsonl file\n",
    "        df.to_json(prompts_path, orient='records', lines=True)\n",
    "        return df\n",
    "\n",
    "prompts_df = load_lmsys_prompts(PROMPTS_PATH, PROMPTS_HF, N_PROMPTS, SEED)\n",
    "print(f\"Loaded {prompts_df.shape[0]} prompts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model and SAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer loaded: GemmaTokenizerFast\n"
     ]
    }
   ],
   "source": [
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "print(f\"Tokenizer loaded: {tokenizer.__class__.__name__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "290b7f07ba9945ee8735b15f55e85260",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded: Gemma2ForCausalLM\n",
      "Model device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Load model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map={\"\":0}\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "print(f\"Model loaded: {model.__class__.__name__}\")\n",
    "print(f\"Model device: {next(model.parameters()).device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading SAE from /workspace/sae/gemma-2-9b/saes/resid_post_layer_20/trainer_131k-l0-114\n",
      "âœ“ Found SAE files at: /workspace/sae/gemma-2-9b/saes/resid_post_layer_20/trainer_131k-l0-114\n",
      "SAE loaded with 131072 features\n",
      "SAE device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "def load_sae(config: ModelConfig, sae_path: str, sae_layer: int, sae_trainer: str) -> SAE:\n",
    "    \"\"\"\n",
    "    Unified SAE loading function that handles both Llama and Gemma models.\n",
    "    \n",
    "    Args:\n",
    "        config: ModelConfig object containing model-specific settings\n",
    "        sae_path: Local path to store/load SAE files\n",
    "        sae_layer: Layer number for the SAE\n",
    "        sae_trainer: Trainer identifier for the SAE\n",
    "    \n",
    "    Returns:\n",
    "        SAE: Loaded SAE model\n",
    "    \"\"\"\n",
    "    # Check if SAE file exists locally\n",
    "    print(f\"Loading SAE from {sae_path}\")\n",
    "    ae_file_path = os.path.join(sae_path, \"sae_weights.safetensors\")\n",
    "    \n",
    "    if os.path.exists(ae_file_path):\n",
    "        print(f\"âœ“ Found SAE files at: {os.path.dirname(ae_file_path)}\")\n",
    "        sae = SAE.load_from_disk(sae_path)\n",
    "        return sae\n",
    "    \n",
    "    print(f\"SAE not found locally, downloading from HF via sae_lens...\")\n",
    "    os.makedirs(os.path.dirname(sae_path), exist_ok=True)\n",
    "    \n",
    "    # Get SAE parameters from config\n",
    "    release, sae_id = config.get_sae_params(sae_layer, sae_trainer)\n",
    "    print(f\"Loading SAE with release='{release}', sae_id='{sae_id}'\")\n",
    "    \n",
    "    # Load the SAE using sae_lens\n",
    "    sae, _, _ = SAE.from_pretrained(\n",
    "        release=release,\n",
    "        sae_id=sae_id,\n",
    "        device=\"cuda\" # Hardcoded because it wants a string\n",
    "    )\n",
    "    \n",
    "    # Save the SAE locally for future use\n",
    "    sae.save_model(sae_path)\n",
    "    return sae\n",
    "\n",
    "# Load SAE using the unified function\n",
    "sae = load_sae(config, SAE_PATH, SAE_LAYER, SAE_TRAINER)\n",
    "sae = sae.to(device)  # Move SAE to GPU\n",
    "\n",
    "print(f\"SAE loaded with {sae.cfg.d_sae} features\")\n",
    "print(f\"SAE device: {next(sae.parameters()).device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two-Stage Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "class StopForward(Exception):\n    \"\"\"Exception to stop forward pass after target layer.\"\"\"\n    pass\n\n@torch.no_grad()\ndef process_prompts_for_features(prompts: List[str], target_features: List[int], \n                                layer_idx: int, activation_threshold: float = 0.0) -> Dict[int, Tuple[List[Dict], List[Dict]]]:\n    \"\"\"Two-stage processing: screen for target features, then get detailed tokens for active prompts.\n    \n    Returns:\n        Dict mapping feature_id to (active_prompts, inactive_prompts) for that feature\n    \"\"\"\n    \n    # Initialize results dictionary for each feature\n    results = {}\n    for feature_id in target_features:\n        results[feature_id] = ([], [])  # (active_prompts, inactive_prompts)\n    \n    # Get target layer\n    target_layer = model.model.layers[layer_idx]\n    \n    # Process in batches\n    for i in tqdm(range(0, len(prompts), BATCH_SIZE), desc=\"Processing prompts\"):\n        batch_prompts = prompts[i:i+BATCH_SIZE]\n        \n        # Format prompts as chat messages\n        formatted_prompts = []\n        for prompt in batch_prompts:\n            messages = [{\"role\": \"user\", \"content\": prompt}]\n            formatted_prompt = tokenizer.apply_chat_template(\n                messages, tokenize=False, add_generation_prompt=True\n            )\n            formatted_prompts.append(formatted_prompt)\n        \n        # Tokenize batch\n        batch_inputs = tokenizer(\n            formatted_prompts,\n            return_tensors=\"pt\",\n            padding=True,\n            truncation=True,\n            max_length=MAX_LENGTH\n        )\n        \n        # Move to device\n        batch_inputs = {k: v.to(device) for k, v in batch_inputs.items()}\n        \n        # Stage 1: Get activations for screening\n        activations = None\n        \n        def hook_fn(module, input, output):\n            nonlocal activations\n            activations = output[0] if isinstance(output, tuple) else output\n            raise StopForward()\n        \n        # Register hook\n        handle = target_layer.register_forward_hook(hook_fn)\n        \n        try:\n            _ = model(**batch_inputs)\n        except StopForward:\n            pass\n        finally:\n            handle.remove()\n        \n        # Apply SAE to get feature activations\n        batch_size, seq_len, hidden_dim = activations.shape\n        flat_activations = activations.view(-1, hidden_dim)\n        \n        # Process SAE in chunks to avoid memory issues\n        sae_features = []\n        for chunk_start in range(0, flat_activations.shape[0], BATCH_SIZE * 8):\n            chunk_end = min(chunk_start + BATCH_SIZE * 8, flat_activations.shape[0])\n            chunk_activations = flat_activations[chunk_start:chunk_end]\n            chunk_features = sae.encode(chunk_activations)\n            sae_features.append(chunk_features.cpu())\n        \n        sae_features = torch.cat(sae_features, dim=0)\n        sae_features = sae_features.view(batch_size, seq_len, -1)\n        \n        # Stage 2: Process each prompt for each target feature separately\n        for batch_idx, (prompt, formatted_prompt) in enumerate(zip(batch_prompts, formatted_prompts)):\n            prompt_idx = i + batch_idx\n            prompt_features = sae_features[batch_idx]  # [seq_len, num_features]\n            input_ids = batch_inputs['input_ids'][batch_idx].cpu().numpy()\n            \n            # Create tokenized prompt (convert input_ids to token strings)\n            tokenized_prompt = []\n            for token_id in input_ids:\n                if token_id != tokenizer.pad_token_id:  # Skip padding tokens\n                    token_text = tokenizer.decode([int(token_id)])\n                    tokenized_prompt.append(token_text)\n            \n            # Process each target feature separately\n            for feature_id in target_features:\n                # Get activations for this specific feature\n                feature_activations = prompt_features[:, feature_id]  # [seq_len]\n                max_activation = float(feature_activations.max())\n                \n                # Check if this feature is active for this prompt\n                is_active = max_activation > activation_threshold\n                \n                if is_active:\n                    # Stage 3: Get detailed token analysis for active prompts\n                    tokens = []\n                    for pos in range(len(input_ids)):\n                        if pos >= prompt_features.shape[0]:\n                            break\n                            \n                        token_id = int(input_ids[pos])\n                        token_text = tokenizer.decode([token_id])\n                        \n                        # Get activation for this specific feature at this position\n                        activation_val = float(prompt_features[pos, feature_id])\n                        \n                        if activation_val > 0:  # Only store non-zero activations\n                            tokens.append({\n                                'position': pos,\n                                'token_id': token_id,\n                                'text': token_text,\n                                'feature_activation': activation_val\n                            })\n                    \n                    results[feature_id][0].append({\n                        'prompt_id': prompt_idx,\n                        'prompt_text': prompt,\n                        'tokenized_prompt': tokenized_prompt,\n                        'max_feature_activation': max_activation,\n                        'tokens': tokens\n                    })\n                else:\n                    # Inactive prompt - just basic info\n                    results[feature_id][1].append({\n                        'prompt_id': prompt_idx,\n                        'prompt_text': prompt,\n                        'tokenized_prompt': tokenized_prompt,\n                        'max_feature_activation': max_activation\n                    })\n    \n    return results\n\nprint(\"Updated two-stage processing function defined\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process All Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(f\"Processing {len(prompts_df)} prompts for {len(TARGET_FEATURES)} target features...\")\nprint(f\"Target features: {TARGET_FEATURES}\")\nprint(f\"Activation threshold: {ACTIVATION_THRESHOLD}\")\n\n# Process all prompts for all features\nfeature_results = process_prompts_for_features(\n    prompts_df['prompt'].tolist(), \n    TARGET_FEATURES, \n    LAYER_INDEX, \n    ACTIVATION_THRESHOLD\n)\n\nprint(f\"\\nResults by feature:\")\nfor feature_id in TARGET_FEATURES:\n    active_prompts, inactive_prompts = feature_results[feature_id]\n    total_tokens = sum(len(p['tokens']) for p in active_prompts)\n    print(f\"  Feature {feature_id}: {len(active_prompts)} active, {len(inactive_prompts)} inactive, {total_tokens} active tokens\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Save results for each feature in separate directories\nfor feature_id in TARGET_FEATURES:\n    active_prompts, inactive_prompts = feature_results[feature_id]\n    \n    # Create feature-specific directory\n    feature_dir = f\"{BASE_OUTPUT_DIR}/{feature_id}\"\n    os.makedirs(feature_dir, exist_ok=True)\n    \n    # Define file paths\n    active_file = f\"{feature_dir}/active.jsonl\"\n    inactive_file = f\"{feature_dir}/inactive.jsonl\"\n    \n    # Save active prompts\n    print(f\"Saving {len(active_prompts)} active prompts for feature {feature_id} to {active_file}\")\n    with open(active_file, 'w') as f:\n        for prompt in active_prompts:\n            f.write(json.dumps(prompt) + '\\n')\n    \n    # Save inactive prompts\n    print(f\"Saving {len(inactive_prompts)} inactive prompts for feature {feature_id} to {inactive_file}\")\n    with open(inactive_file, 'w') as f:\n        for prompt in inactive_prompts:\n            f.write(json.dumps(prompt) + '\\n')\n    \n    # Show sample results for this feature\n    if active_prompts:\n        print(f\"  Sample active prompt for feature {feature_id}:\")\n        sample = active_prompts[0]\n        print(f\"    Prompt: {sample['prompt_text'][:100]}...\")\n        print(f\"    Max activation: {sample['max_feature_activation']}\")\n        print(f\"    Active tokens: {len(sample['tokens'])}\")\n        if sample['tokens']:\n            top_token = max(sample['tokens'], key=lambda x: x['feature_activation'])\n            print(f\"    Top token: '{top_token['text']}' (position {top_token['position']})\")\n            print(f\"    Token activation: {top_token['feature_activation']}\")\n        print()\n\nprint(f\"âœ“ Results saved successfully for all {len(TARGET_FEATURES)} features!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def find_assistant_position(input_ids: torch.Tensor, attention_mask: torch.Tensor, \n                          assistant_header: str, token_offset: int, tokenizer, device) -> int:\n    \"\"\"Find the position of the assistant token based on the given offset.\"\"\"\n    # Find assistant header position\n    assistant_tokens = tokenizer.encode(assistant_header, add_special_tokens=False)\n    \n    # Find where assistant section starts\n    assistant_pos = None\n    for k in range(len(input_ids) - len(assistant_tokens) + 1):\n        if torch.equal(input_ids[k:k+len(assistant_tokens)], torch.tensor(assistant_tokens, device=input_ids.device)):\n            assistant_pos = k + len(assistant_tokens) + token_offset\n            break\n    \n    if assistant_pos is None:\n        # Fallback to last non-padding token\n        assistant_pos = attention_mask.sum().item() - 1\n    \n    # Ensure position is within bounds\n    max_pos = attention_mask.sum().item() - 1\n    assistant_pos = min(assistant_pos, max_pos)\n    assistant_pos = max(assistant_pos, 0)\n    \n    return int(assistant_pos)\n\n@torch.no_grad()\ndef filter_by_token_position(active_prompts: List[Dict], inactive_prompts: List[Dict], \n                           token_type: str, token_offset: int, feature_id: int,\n                           activation_threshold: float = 0.0) -> Tuple[List[Dict], List[Dict]]:\n    \"\"\"Filter prompts based on whether target feature is active at specific token positions.\"\"\"\n    \n    token_active = []\n    token_inactive = []\n    \n    # Process all prompts (both active and inactive from main analysis)\n    all_prompts = active_prompts + inactive_prompts\n    \n    for prompt_data in all_prompts:\n        prompt_text = prompt_data['prompt_text']\n        \n        # Use existing tokenized_prompt if available, otherwise tokenize\n        if 'tokenized_prompt' in prompt_data:\n            tokenized_prompt = prompt_data['tokenized_prompt']\n        else:\n            # Format as chat message to match processing\n            messages = [{\"role\": \"user\", \"content\": prompt_text}]\n            formatted_prompt = tokenizer.apply_chat_template(\n                messages, tokenize=False, add_generation_prompt=True\n            )\n            \n            # Tokenize to get input_ids and attention_mask\n            inputs = tokenizer(\n                formatted_prompt,\n                return_tensors=\"pt\",\n                padding=True,\n                truncation=True,\n                max_length=MAX_LENGTH\n            )\n            \n            input_ids = inputs['input_ids'].squeeze(0)\n            \n            # Create tokenized prompt\n            tokenized_prompt = []\n            for token_id in input_ids:\n                if token_id != tokenizer.pad_token_id:  # Skip padding tokens\n                    token_text = tokenizer.decode([int(token_id)])\n                    tokenized_prompt.append(token_text)\n        \n        # For finding token position, we still need input_ids and attention_mask\n        messages = [{\"role\": \"user\", \"content\": prompt_text}]\n        formatted_prompt = tokenizer.apply_chat_template(\n            messages, tokenize=False, add_generation_prompt=True\n        )\n        \n        inputs = tokenizer(\n            formatted_prompt,\n            return_tensors=\"pt\",\n            padding=True,\n            truncation=True,\n            max_length=MAX_LENGTH\n        )\n        \n        input_ids = inputs['input_ids'].squeeze(0)\n        attention_mask = inputs['attention_mask'].squeeze(0)\n        \n        # Find the specific token position\n        target_position = find_assistant_position(\n            input_ids, attention_mask, ASSISTANT_HEADER, token_offset, tokenizer, device\n        )\n        \n        # Check if this prompt has token data at the target position\n        has_activation_at_position = False\n        max_activation_at_position = 0.0\n        \n        # Check if this was an active prompt with token details\n        if 'tokens' in prompt_data:\n            for token_data in prompt_data['tokens']:\n                if token_data['position'] == target_position:\n                    activation = token_data['feature_activation']\n                    if activation > activation_threshold:\n                        has_activation_at_position = True\n                        max_activation_at_position = max(max_activation_at_position, activation)\n        \n        # Create token-specific record\n        token_record = {\n            'prompt_id': prompt_data['prompt_id'],\n            'prompt_text': prompt_data['prompt_text'],\n            'tokenized_prompt': tokenized_prompt,\n            'token_type': token_type,\n            'token_position': target_position,\n            'max_feature_activation': prompt_data['max_feature_activation']\n        }\n        \n        if has_activation_at_position:\n            token_record['max_activation_at_position'] = max_activation_at_position\n            # Include token details for this specific position\n            position_tokens = []\n            if 'tokens' in prompt_data:\n                for token_data in prompt_data['tokens']:\n                    if token_data['position'] == target_position:\n                        position_tokens.append(token_data)\n            token_record['position_tokens'] = position_tokens\n            token_active.append(token_record)\n        else:\n            token_record['max_activation_at_position'] = max_activation_at_position\n            token_inactive.append(token_record)\n    \n    return token_active, token_inactive\n\nprint(\"Updated token-specific filtering functions defined\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Process token-specific analysis for each feature and token type\nprint(f\"Processing token-specific analysis for {len(TARGET_FEATURES)} features and {len(TOKEN_OFFSETS)} token types...\")\n\nfor feature_id in TARGET_FEATURES:\n    active_prompts, inactive_prompts = feature_results[feature_id]\n    feature_dir = f\"{BASE_OUTPUT_DIR}/{feature_id}\"\n    \n    print(f\"\\nProcessing feature {feature_id}:\")\n    \n    for token_type, token_offset in TOKEN_OFFSETS.items():\n        print(f\"  Processing token type: {token_type} (offset: {token_offset})\")\n        \n        # Filter prompts based on activation at this specific token position\n        token_active, token_inactive = filter_by_token_position(\n            active_prompts, inactive_prompts, \n            token_type, token_offset, feature_id, ACTIVATION_THRESHOLD\n        )\n        \n        # Save results for this token type\n        active_file = f\"{feature_dir}/active_{token_type}.jsonl\"\n        inactive_file = f\"{feature_dir}/inactive_{token_type}.jsonl\"\n        \n        print(f\"    Active prompts at {token_type} position: {len(token_active)}\")\n        print(f\"    Inactive prompts at {token_type} position: {len(token_inactive)}\")\n        \n        # Save active prompts for this token type\n        with open(active_file, 'w') as f:\n            for prompt in token_active:\n                f.write(json.dumps(prompt) + '\\n')\n        \n        # Save inactive prompts for this token type\n        with open(inactive_file, 'w') as f:\n            for prompt in token_inactive:\n                f.write(json.dumps(prompt) + '\\n')\n        \n        print(f\"    Saved: {active_file}\")\n        print(f\"    Saved: {inactive_file}\")\n        \n        # Show sample if available\n        if token_active:\n            sample = token_active[0]\n            print(f\"    Sample active prompt at {token_type} position:\")\n            print(f\"      Position: {sample['token_position']}\")\n            print(f\"      Max activation: {sample['max_activation_at_position']}\")\n            if 'position_tokens' in sample and sample['position_tokens']:\n                token_data = sample['position_tokens'][0]\n                print(f\"      Token text: '{token_data['text']}'\")\n                print(f\"      Feature activation: {token_data['feature_activation']}\")\n\nprint(f\"\\nâœ“ Token-specific analysis complete for all features!\")\nprint(f\"Results saved in feature-specific directories under: {BASE_OUTPUT_DIR}\")\nprint(f\"Each feature directory contains:\")\nprint(f\"  - active.jsonl / inactive.jsonl (general)\")\nfor token_type in TOKEN_OFFSETS.keys():\n    print(f\"  - active_{token_type}.jsonl / inactive_{token_type}.jsonl (position-specific)\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}