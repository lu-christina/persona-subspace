{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "pareto-title",
   "metadata": {},
   "source": [
    "# Pareto Frontier Analysis: Eval Performance vs Jailbreak Harm Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pareto-desc",
   "metadata": {},
   "source": [
    "This notebook analyzes the tradeoff between:\n",
    "- **Eval Performance**: Percentage change in benchmark scores (MMLU, IFEval, EQ-Bench)\n",
    "- **Harm Reduction**: Percentage change in jailbreak harmful response rate\n",
    "\n",
    "The Pareto frontier identifies configurations that achieve optimal tradeoffs between these objectives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Import shared plotting utilities\n",
    "from plots import (\n",
    "    parse_experiment_id, parse_layer_sort_key, cap_sort_key,\n",
    "    format_layer_range, format_cap_label,\n",
    "    CONFIG_COLORS, CONFIG_DISPLAY_NAMES, CONFIG_ORDER\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Configuration\n",
    "# ============================================================================\n",
    "\n",
    "model = \"qwen-3-32b\"\n",
    "total_layers = 64\n",
    "subtitle = f\"{model.replace('-', ' ').title()}, Single-Shot & No Thinking\"\n",
    "base_dir = f\"/workspace/{model}\"\n",
    "out_dir = f\"/root/git/plots/{model}/capped/results/pareto\"\n",
    "\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "# Configs to analyze\n",
    "config_names = [\"baseline\", \"role_trait\", \"jailbreak\", \"lmsys_10000\"]\n",
    "\n",
    "# Eval-specific metric mappings\n",
    "EVAL_METRICS = {\n",
    "    'ifeval': {\n",
    "        'metric': 'inst_level_strict_acc,none',\n",
    "        'display_name': 'IFEval Instruction-level Accuracy',\n",
    "        'higher_is_better': True\n",
    "    },\n",
    "    'mmlu_pro': {\n",
    "        'metric': 'exact_match,custom-extract',\n",
    "        'display_name': 'MMLU Pro Exact Match Accuracy',\n",
    "        'higher_is_better': True\n",
    "    },\n",
    "    'eq_bench': {\n",
    "        'metric': 'eqbench,none',\n",
    "        'display_name': 'EQ-Bench Score',\n",
    "        'higher_is_better': True\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"Configuration loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data-loading",
   "metadata": {},
   "source": [
    "## Data Loading Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-benchmarks",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_experiment_data(tasks, config_names, base_dir):\n",
    "    \"\"\"\n",
    "    Load experiment data for specified tasks and configs.\n",
    "\n",
    "    Args:\n",
    "        tasks: List of task names (e.g., ['ifeval', 'mmlu_pro', 'eq_bench'])\n",
    "        config_names: List of config names (e.g., ['baseline', 'role_trait', 'jailbreak'])\n",
    "        base_dir: Base directory containing benchmarks folder\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with columns: task_name, config_name, experiment_id, run_dir,\n",
    "                               thinking, apply_chat_template, and all metrics from results\n",
    "    \"\"\"\n",
    "    all_rows = []\n",
    "    bench_dir = f\"{base_dir}/capped/benchmarks\"\n",
    "\n",
    "    for task in tasks:\n",
    "        task_dir = f\"{bench_dir}/{task}\"\n",
    "\n",
    "        if not os.path.exists(task_dir):\n",
    "            print(f\"Warning: Task directory not found: {task_dir}\")\n",
    "            continue\n",
    "\n",
    "        for config_name in config_names:\n",
    "            config_dir = f\"{task_dir}/{config_name}\"\n",
    "\n",
    "            if not os.path.exists(config_dir):\n",
    "                print(f\"Warning: Config directory not found: {config_dir}\")\n",
    "                continue\n",
    "\n",
    "            if config_name == \"baseline\":\n",
    "                # Baseline: iterate through all timestamped runs directly\n",
    "                run_dirs = [d for d in os.listdir(config_dir) if d.startswith(\"2025-\")]\n",
    "\n",
    "                for run_dir in sorted(run_dirs):\n",
    "                    results_path = os.path.join(config_dir, run_dir, \"results.json\")\n",
    "                    manifest_path = os.path.join(config_dir, run_dir, \"manifest.json\")\n",
    "\n",
    "                    if os.path.exists(results_path):\n",
    "                        with open(results_path, \"r\") as f:\n",
    "                            data = json.load(f)\n",
    "\n",
    "                        # Load manifest for thinking and apply_chat_template\n",
    "                        thinking = False\n",
    "                        apply_chat_template = False\n",
    "                        vllm = False\n",
    "                        if os.path.exists(manifest_path):\n",
    "                            with open(manifest_path, \"r\") as f:\n",
    "                                manifest = json.load(f)\n",
    "                                thinking = manifest.get(\"thinking\", None)\n",
    "                                if thinking is None:\n",
    "                                    thinking = False\n",
    "                                apply_chat_template = manifest.get(\"apply_chat_template\", False)\n",
    "                                vllm = manifest.get(\"vllm\", False)\n",
    "\n",
    "                        # Get the task results\n",
    "                        if \"results\" in data and task in data[\"results\"]:\n",
    "                            row = {\n",
    "                                \"task_name\": task,\n",
    "                                \"config_name\": config_name,\n",
    "                                \"experiment_id\": \"baseline\",\n",
    "                                \"run_dir\": run_dir,\n",
    "                                \"thinking\": thinking,\n",
    "                                \"apply_chat_template\": apply_chat_template,\n",
    "                                \"vllm\": vllm\n",
    "                            }\n",
    "                            # Add all metrics from the task results\n",
    "                            row.update(data[\"results\"][task])\n",
    "                            \n",
    "                            # Drop alias field if present\n",
    "                            row.pop(\"alias\", None)\n",
    "\n",
    "                            all_rows.append(row)\n",
    "            else:\n",
    "                # Other configs: iterate through experiment_id directories\n",
    "                for experiment_id in os.listdir(config_dir):\n",
    "                    exp_dir = os.path.join(config_dir, experiment_id)\n",
    "\n",
    "                    if not os.path.isdir(exp_dir):\n",
    "                        continue\n",
    "\n",
    "                    # Load all timestamped runs for this experiment\n",
    "                    run_dirs = [d for d in os.listdir(exp_dir) if d.startswith(\"2025-\")]\n",
    "\n",
    "                    for run_dir in sorted(run_dirs):\n",
    "                        results_path = os.path.join(exp_dir, run_dir, \"results.json\")\n",
    "                        manifest_path = os.path.join(exp_dir, run_dir, \"manifest.json\")\n",
    "\n",
    "                        if os.path.exists(results_path):\n",
    "                            with open(results_path, \"r\") as f:\n",
    "                                data = json.load(f)\n",
    "\n",
    "                            # Load manifest for thinking and apply_chat_template\n",
    "                            thinking = False\n",
    "                            apply_chat_template = False\n",
    "                            vllm = False\n",
    "                            if os.path.exists(manifest_path):\n",
    "                                with open(manifest_path, \"r\") as f:\n",
    "                                    manifest = json.load(f)\n",
    "                                    thinking = manifest.get(\"thinking\", None)\n",
    "                                    if thinking is None:\n",
    "                                        thinking = False\n",
    "                                    apply_chat_template = manifest.get(\"apply_chat_template\", False)\n",
    "                                    vllm = manifest.get(\"vllm\", False)\n",
    "\n",
    "                            # Get the task results\n",
    "                            if \"results\" in data and task in data[\"results\"]:\n",
    "                                row = {\n",
    "                                    \"task_name\": task,\n",
    "                                    \"config_name\": config_name,\n",
    "                                    \"experiment_id\": experiment_id,\n",
    "                                    \"run_dir\": run_dir,\n",
    "                                    \"thinking\": thinking,\n",
    "                                    \"apply_chat_template\": apply_chat_template,\n",
    "                                    \"vllm\": vllm\n",
    "                                }\n",
    "                                # Add all metrics from the task results\n",
    "                                row.update(data[\"results\"][task])\n",
    "                                \n",
    "                                # Drop alias field if present\n",
    "                                row.pop(\"alias\", None)\n",
    "\n",
    "                                all_rows.append(row)\n",
    "\n",
    "    df = pd.DataFrame(all_rows)\n",
    "    return df\n",
    "\n",
    "print(\"Benchmark loading function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-jailbreak",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_jailbreak_scores(config_name, base_dir):\n",
    "    \"\"\"\n",
    "    Load jailbreak scores and calculate combined harm rate.\n",
    "    \n",
    "    Args:\n",
    "        config_name: Name of config (e.g., 'jailbreak', 'lmsys_10000')\n",
    "        base_dir: Base directory containing results folder\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with columns: experiment_id, harm_rate (percentage)\n",
    "    \"\"\"\n",
    "    scores_path = f\"{base_dir}/capped/results/{config_name}_jailbreak_1100_scores.jsonl\"\n",
    "    \n",
    "    if not os.path.exists(scores_path):\n",
    "        print(f\"Warning: Jailbreak scores not found: {scores_path}\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Load scores\n",
    "    with open(scores_path, \"r\") as f:\n",
    "        scores = [json.loads(line) for line in f]\n",
    "    df = pd.DataFrame(scores)\n",
    "    \n",
    "    # Calculate harm rate per experiment\n",
    "    harm_labels = [\"enough_info\", \"enough_info_and_follow_perfectly\"]\n",
    "    \n",
    "    results = []\n",
    "    for exp_id, group in df.groupby(\"experiment_id\"):\n",
    "        n_total = len(group)\n",
    "        n_harmful = len(group[group[\"score\"].isin(harm_labels)])\n",
    "        harm_rate = (n_harmful / n_total * 100) if n_total > 0 else 0.0\n",
    "        \n",
    "        results.append({\n",
    "            \"experiment_id\": exp_id,\n",
    "            \"config_name\": config_name,\n",
    "            \"harm_rate\": harm_rate,\n",
    "            \"n_harmful\": n_harmful,\n",
    "            \"n_total\": n_total\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "def load_baseline_jailbreak(base_dir, baseline_type=\"prompted\"):\n",
    "    \"\"\"\n",
    "    Load baseline (unsteered) jailbreak scores.\n",
    "    \n",
    "    Args:\n",
    "        base_dir: Base directory\n",
    "        baseline_type: 'prompted' or 'default'\n",
    "    \n",
    "    Returns:\n",
    "        Harm rate as percentage\n",
    "    \"\"\"\n",
    "    if baseline_type == \"prompted\":\n",
    "        scores_path = f\"{base_dir}/evals/unsteered/unsteered_scores.jsonl\"\n",
    "    else:\n",
    "        scores_path = f\"{base_dir}/evals/unsteered/unsteered_default_scores.jsonl\"\n",
    "    \n",
    "    if not os.path.exists(scores_path):\n",
    "        print(f\"Warning: Baseline scores not found: {scores_path}\")\n",
    "        return None\n",
    "    \n",
    "    with open(scores_path, \"r\") as f:\n",
    "        scores = [json.loads(line) for line in f]\n",
    "    df = pd.DataFrame(scores)\n",
    "    \n",
    "    harm_labels = [\"enough_info\", \"enough_info_and_follow_perfectly\"]\n",
    "    n_total = len(df)\n",
    "    n_harmful = len(df[df[\"score\"].isin(harm_labels)])\n",
    "    harm_rate = (n_harmful / n_total * 100) if n_total > 0 else 0.0\n",
    "    \n",
    "    return harm_rate\n",
    "\n",
    "print(\"Jailbreak loading functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pareto-calc",
   "metadata": {},
   "source": [
    "## Pareto Frontier Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pareto-func",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_pareto_frontier(df, x_col='eval_pct_change', y_col='harm_pct_change'):\n",
    "    \"\"\"\n",
    "    Identify Pareto-optimal points.\n",
    "    \n",
    "    For our case:\n",
    "    - X-axis (eval_pct_change): Higher is better (less performance degradation)\n",
    "    - Y-axis (harm_pct_change): Lower is better (more harm reduction, i.e., more negative)\n",
    "    \n",
    "    A point is Pareto-optimal if no other point has both:\n",
    "    - x >= x_i (better or equal eval performance)\n",
    "    - y <= y_i (better or equal harm reduction)\n",
    "    with at least one strict inequality.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with x_col and y_col columns\n",
    "        x_col: Column name for x-axis (eval percentage change)\n",
    "        y_col: Column name for y-axis (harm percentage change)\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with additional 'is_pareto' boolean column\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    df['is_pareto'] = False\n",
    "    \n",
    "    points = df[[x_col, y_col]].values\n",
    "    \n",
    "    for i in range(len(points)):\n",
    "        is_dominated = False\n",
    "        \n",
    "        for j in range(len(points)):\n",
    "            if i == j:\n",
    "                continue\n",
    "            \n",
    "            # Check if point j dominates point i\n",
    "            # j dominates i if: j.x >= i.x AND j.y <= i.y (with at least one strict)\n",
    "            x_better_or_equal = points[j][0] >= points[i][0]\n",
    "            y_better_or_equal = points[j][1] <= points[i][1]\n",
    "            strictly_better = (points[j][0] > points[i][0]) or (points[j][1] < points[i][1])\n",
    "            \n",
    "            if x_better_or_equal and y_better_or_equal and strictly_better:\n",
    "                is_dominated = True\n",
    "                break\n",
    "        \n",
    "        if not is_dominated:\n",
    "            df.loc[df.index[i], 'is_pareto'] = True\n",
    "    \n",
    "    return df\n",
    "\n",
    "print(\"Pareto calculation function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "plotting",
   "metadata": {},
   "source": [
    "## Plotting Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot-func",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pareto_frontier(df, eval_name, eval_display_name, title_suffix=\"\", subtitle=\"\"):\n",
    "    \"\"\"\n",
    "    Plot Pareto frontier for eval performance vs jailbreak harm reduction.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with columns: config_name, experiment_id, eval_pct_change, \n",
    "            harm_pct_change, is_pareto, display_name\n",
    "        eval_name: Short eval name (e.g., 'ifeval')\n",
    "        eval_display_name: Display name for eval (e.g., 'IFEval Prompt-level Accuracy')\n",
    "        title_suffix: Additional text for title\n",
    "        subtitle: Subtitle text\n",
    "    \n",
    "    Returns:\n",
    "        Plotly figure object\n",
    "    \"\"\"\n",
    "    fig = go.Figure()\n",
    "    \n",
    "    # Plot all points by config (both Pareto and non-Pareto with same style)\n",
    "    for config in ['baseline', 'jailbreak', 'role_trait', 'lmsys_10000']:\n",
    "        if config not in df['config_name'].values:\n",
    "            continue\n",
    "        \n",
    "        config_df = df[df['config_name'] == config]\n",
    "        \n",
    "        # Plot all points for this config\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=config_df['eval_pct_change'],\n",
    "            y=config_df['harm_pct_change'],\n",
    "            mode='markers',\n",
    "            name=CONFIG_DISPLAY_NAMES[config],\n",
    "            marker=dict(\n",
    "                size=8,\n",
    "                color=CONFIG_COLORS[config],\n",
    "                opacity=0.6\n",
    "            ),\n",
    "            hovertemplate=(\n",
    "                \"<b>%{customdata[0]}</b><br>\"\n",
    "                \"Config: %{customdata[1]}<br>\"\n",
    "                \"Eval Change: %{x:.1f}%<br>\"\n",
    "                \"Harm Change: %{y:.1f}%<br>\"\n",
    "                \"%{customdata[2]}\"\n",
    "                \"<extra></extra>\"\n",
    "            ),\n",
    "            customdata=np.column_stack([\n",
    "                config_df['display_name'],\n",
    "                config_df['config_name'],\n",
    "                config_df['is_pareto'].apply(lambda x: '<b>PARETO OPTIMAL</b>' if x else '')\n",
    "            ]),\n",
    "            legendgroup=config,\n",
    "            showlegend=True\n",
    "        ))\n",
    "    \n",
    "    # Add text labels for Pareto-optimal points using parsed experiment info\n",
    "    pareto_points = df[df['is_pareto']]\n",
    "    for _, row in pareto_points.iterrows():\n",
    "        # Parse experiment_id to get layer and cap info\n",
    "        layer_spec, _, cap_value = parse_experiment_id(row['experiment_id'])\n",
    "        \n",
    "        # Format the label\n",
    "        layer_label = format_layer_range(layer_spec)\n",
    "        cap_label = format_cap_label(cap_value)\n",
    "        label_text = f\"{layer_label}, {cap_label} %ile\"\n",
    "        \n",
    "        # Get the color for this config\n",
    "        point_color = CONFIG_COLORS[row['config_name']]\n",
    "        \n",
    "        fig.add_annotation(\n",
    "            x=row['eval_pct_change'],\n",
    "            y=row['harm_pct_change'],\n",
    "            text=label_text,\n",
    "            showarrow=False,\n",
    "            xshift=10,  # Position label to the right of the point\n",
    "            font=dict(size=9, color=point_color),\n",
    "            xanchor='left',\n",
    "            yanchor='middle',\n",
    "            align='left'\n",
    "        )\n",
    "    \n",
    "    # Draw Pareto frontier line\n",
    "    pareto_sorted = pareto_points.sort_values('eval_pct_change')\n",
    "    if len(pareto_sorted) > 1:\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=pareto_sorted['eval_pct_change'],\n",
    "            y=pareto_sorted['harm_pct_change'],\n",
    "            mode='lines',\n",
    "            name='Pareto Frontier',\n",
    "            line=dict(color='grey', width=1, dash='dash'),\n",
    "            showlegend=True,\n",
    "            hoverinfo='skip'\n",
    "        ))\n",
    "    \n",
    "    # Add reference lines at origin\n",
    "    fig.add_hline(y=0, line_dash=\"dot\", line_color=\"gray\", line_width=1, opacity=0.5)\n",
    "    fig.add_vline(x=0, line_dash=\"dot\", line_color=\"gray\", line_width=1, opacity=0.5)\n",
    "    \n",
    "    # Add quadrant annotations\n",
    "    max_x = df['eval_pct_change'].max()\n",
    "    min_x = df['eval_pct_change'].min()\n",
    "    max_y = df['harm_pct_change'].max()\n",
    "    min_y = df['harm_pct_change'].min()\n",
    "    \n",
    "    # Top-right quadrant (ideal: better eval, reduced harm)\n",
    "    fig.add_annotation(\n",
    "        x=max_x * 0.9, y=min_y * 0.9,\n",
    "        text=\"<b>Ideal</b><br>Unchanged eval<br>Reduced harm\",\n",
    "        showarrow=False,\n",
    "        font=dict(size=10, color=\"green\")\n",
    "    )\n",
    "    \n",
    "    # Bottom-left quadrant (worst: worse eval, increased harm)\n",
    "    fig.add_annotation(\n",
    "        x=min_x * 0.9, y=max_y * 0.9,\n",
    "        text=\"<b>Worst</b><br>Worse eval<br>Unchanged harm\",\n",
    "        showarrow=False,\n",
    "        font=dict(size=10, color=\"red\")\n",
    "    )\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=dict(\n",
    "            text=f\"Pareto Frontier: {eval_display_name} vs. Harmful Response Rate{title_suffix}\",\n",
    "            subtitle=dict(text=subtitle)\n",
    "        ),\n",
    "        xaxis=dict(\n",
    "            title=\"Eval Performance Change (%)\",\n",
    "            zeroline=True,\n",
    "            showgrid=True,\n",
    "            gridcolor='lightgray'\n",
    "        ),\n",
    "        yaxis=dict(\n",
    "            title=\"Harmful Response Rate Reduced (%)\",\n",
    "            zeroline=True,\n",
    "            showgrid=True,\n",
    "            gridcolor='lightgray'\n",
    "        ),\n",
    "        width=1000,\n",
    "        height=700,\n",
    "        legend=dict(\n",
    "            orientation=\"v\",\n",
    "            yanchor=\"top\",\n",
    "            y=1,\n",
    "            xanchor=\"left\",\n",
    "            x=1.02\n",
    "        ),\n",
    "        hovermode='closest'\n",
    "    )\n",
    "    \n",
    "    return fig\n",
    "\n",
    "print(\"Plotting function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "analysis",
   "metadata": {},
   "source": [
    "## Analysis: Generate Pareto Frontiers\n",
    "\n",
    "For each eval, we'll:\n",
    "1. Load benchmark and jailbreak data\n",
    "2. Calculate percentage changes from baseline\n",
    "3. Identify Pareto frontier\n",
    "4. Visualize the tradeoff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "kvmuyohccpd",
   "metadata": {},
   "source": [
    "## Load Jailbreak Data\n",
    "\n",
    "Load jailbreak harm rates for all configs once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tiqs4qj8igb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get baseline harm rate (using prompted baseline)\n",
    "baseline_harm = load_baseline_jailbreak(base_dir, baseline_type=\"prompted\")\n",
    "print(f\"Baseline jailbreak harm rate: {baseline_harm:.2f}%\")\n",
    "\n",
    "# Load jailbreak data for each config\n",
    "jailbreak_data = {}\n",
    "for config_name in config_names:\n",
    "    if config_name == 'baseline':\n",
    "        continue\n",
    "    \n",
    "    df_jb = load_jailbreak_scores(config_name, base_dir)\n",
    "    if len(df_jb) > 0:\n",
    "        jailbreak_data[config_name] = df_jb\n",
    "        print(f\"Loaded {len(df_jb)} experiments for {config_name}\")\n",
    "    else:\n",
    "        print(f\"Warning: No jailbreak data found for {config_name}\")\n",
    "\n",
    "print(f\"\\nLoaded jailbreak data for {len(jailbreak_data)} configs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pareto-helper",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_pareto_data(df_eval, df_jailbreak_dict, eval_name, baseline_eval_value, baseline_harm_rate):\n",
    "    \"\"\"\n",
    "    Prepare data for Pareto analysis by combining eval and jailbreak metrics.\n",
    "\n",
    "    Args:\n",
    "        df_eval: Pre-loaded eval DataFrame (filtered as desired)\n",
    "        df_jailbreak_dict: Dict mapping config_name to jailbreak DataFrames\n",
    "        eval_name: Name of eval task (for metric lookup)\n",
    "        baseline_eval_value: Baseline eval metric value\n",
    "        baseline_harm_rate: Baseline jailbreak harm rate (percentage)\n",
    "\n",
    "    Returns:\n",
    "        DataFrame ready for Pareto analysis with Pareto frontier calculated\n",
    "    \"\"\"\n",
    "    eval_config = EVAL_METRICS[eval_name]\n",
    "    metric_col = eval_config['metric']\n",
    "\n",
    "    print(f\"\\n{eval_name.upper()}:\")\n",
    "    print(f\"Baseline eval value: {baseline_eval_value:.4f}\")\n",
    "    print(f\"Baseline harm rate: {baseline_harm_rate:.2f}%\")\n",
    "\n",
    "    # Combine eval and jailbreak data for each config\n",
    "    all_data = []\n",
    "\n",
    "    for config_name, df_jailbreak in df_jailbreak_dict.items():\n",
    "        if config_name == 'baseline':\n",
    "            continue\n",
    "\n",
    "        # Get eval data for this config\n",
    "        config_eval = df_eval[df_eval['config_name'] == config_name]\n",
    "\n",
    "        if len(config_eval) == 0:\n",
    "            print(f\"Skipping {config_name}: no eval data\")\n",
    "            continue\n",
    "\n",
    "        # Merge eval and jailbreak data\n",
    "        merged = config_eval.merge(\n",
    "            df_jailbreak[['experiment_id', 'harm_rate']],\n",
    "            on='experiment_id',\n",
    "            how='inner'\n",
    "        )\n",
    "\n",
    "        if len(merged) == 0:\n",
    "            print(f\"Skipping {config_name}: no matching experiments\")\n",
    "            continue\n",
    "\n",
    "        # Calculate percentage changes\n",
    "        merged['eval_value'] = merged[metric_col]\n",
    "        merged['eval_pct_change'] = ((merged['eval_value'] - baseline_eval_value) / baseline_eval_value * 100)\n",
    "        merged['harm_pct_change'] = ((merged['harm_rate'] - baseline_harm_rate) / baseline_harm_rate * 100)\n",
    "\n",
    "        # Add parsed columns for display\n",
    "        parsed = merged['experiment_id'].apply(parse_experiment_id)\n",
    "        merged['layer_spec'] = parsed.apply(lambda x: x[0])\n",
    "        merged['cap_value'] = parsed.apply(lambda x: x[2])\n",
    "        merged['layer_label'] = merged['layer_spec'].apply(format_layer_range)\n",
    "        merged['cap_label'] = merged['cap_value'].apply(format_cap_label)\n",
    "        merged['display_name'] = merged.apply(\n",
    "            lambda row: f\"{row['layer_label']}, {row['cap_label']}\",\n",
    "            axis=1\n",
    "        )\n",
    "\n",
    "        all_data.append(merged)\n",
    "        print(f\"Loaded {len(merged)} experiments for {config_name}\")\n",
    "\n",
    "    if len(all_data) == 0:\n",
    "        print(\"No data available for Pareto analysis\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # Combine all configs\n",
    "    df_combined = pd.concat(all_data, ignore_index=True)\n",
    "\n",
    "    # Calculate Pareto frontier\n",
    "    df_combined = calculate_pareto_frontier(df_combined)\n",
    "\n",
    "    print(f\"\\nTotal experiments: {len(df_combined)}\")\n",
    "    print(f\"Pareto-optimal points: {df_combined['is_pareto'].sum()}\")\n",
    "\n",
    "    return df_combined\n",
    "\n",
    "print(\"Data preparation helper defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ifeval-section",
   "metadata": {},
   "source": [
    "### IFEval vs Jailbreak Harm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ifeval-pareto",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load IFEval data\n",
    "df_ifeval = load_experiment_data(['ifeval'], config_names, base_dir)\n",
    "print(f\"Loaded {len(df_ifeval)} IFEval experiment runs\")\n",
    "print(f\"\\nConfig breakdown:\")\n",
    "for config in df_ifeval['config_name'].unique():\n",
    "    print(f\"  {config}: {len(df_ifeval[df_ifeval['config_name'] == config])} runs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4i8hxi8nsly",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter IFEval data\n",
    "# Filter to no thinking, with chat template\n",
    "df_ifeval_filtered = df_ifeval\n",
    "\n",
    "print(f\"After filtering: {len(df_ifeval_filtered)} runs\")\n",
    "\n",
    "# Get baseline value for percentage calculation\n",
    "metric_col = EVAL_METRICS['ifeval']['metric']\n",
    "baseline_ifeval = df_ifeval_filtered[df_ifeval_filtered['config_name'] == 'baseline']\n",
    "if len(baseline_ifeval) > 0:\n",
    "    baseline_ifeval_value = baseline_ifeval[metric_col].iloc[0]\n",
    "    print(f\"Baseline IFEval value: {baseline_ifeval_value:.4f}\")\n",
    "else:\n",
    "    print(\"Warning: No baseline found!\")\n",
    "    baseline_ifeval_value = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ctya30z4ha",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Pareto data for IFEval\n",
    "if baseline_ifeval_value is not None:\n",
    "    df_ifeval_pareto = prepare_pareto_data(\n",
    "        df_ifeval_filtered,\n",
    "        jailbreak_data,\n",
    "        'ifeval',\n",
    "        baseline_ifeval_value,\n",
    "        baseline_harm\n",
    "    )\n",
    "else:\n",
    "    df_ifeval_pareto = pd.DataFrame()\n",
    "    print(\"Skipping Pareto analysis due to missing baseline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kty4jmadofh",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot IFEval Pareto frontier\n",
    "if len(df_ifeval_pareto) > 0:\n",
    "    fig_ifeval = plot_pareto_frontier(\n",
    "        df_ifeval_pareto,\n",
    "        'ifeval',\n",
    "        EVAL_METRICS['ifeval']['display_name'],\n",
    "        subtitle=subtitle\n",
    "    )\n",
    "    fig_ifeval.show()\n",
    "    fig_ifeval.write_html(f\"{out_dir}/ifeval.html\")\n",
    "else:\n",
    "    print(\"No data available for IFEval Pareto plot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mmlu-section",
   "metadata": {},
   "source": [
    "### MMLU Pro vs Jailbreak Harm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mmlu-pareto",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MMLU Pro data\n",
    "df_mmlu = load_experiment_data(['mmlu_pro'], config_names, base_dir)\n",
    "print(f\"Loaded {len(df_mmlu)} MMLU Pro experiment runs\")\n",
    "print(f\"\\nConfig breakdown:\")\n",
    "for config in df_mmlu['config_name'].unique():\n",
    "    print(f\"  {config}: {len(df_mmlu[df_mmlu['config_name'] == config])} runs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ups7tt95m0j",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter MMLU Pro data\n",
    "df_mmlu_filtered = df_mmlu[\n",
    "    ((df_mmlu['config_name'] == 'baseline') & \n",
    "     (~df_mmlu['thinking']) & \n",
    "     (df_mmlu['apply_chat_template']) & (~df_mmlu['vllm'])) |\n",
    "    ((df_mmlu['config_name'] != 'baseline') & \n",
    "     (~df_mmlu['thinking']) & \n",
    "     (df_mmlu['apply_chat_template']) & (~df_mmlu['vllm']))\n",
    "]\n",
    "\n",
    "print(f\"After filtering: {len(df_mmlu_filtered)} runs\")\n",
    "for config in df_mmlu['config_name'].unique():\n",
    "    print(f\"  {config}: {len(df_mmlu_filtered[df_mmlu_filtered['config_name'] == config])} runs\")\n",
    "\n",
    "# Get baseline value\n",
    "metric_col = EVAL_METRICS['mmlu_pro']['metric']\n",
    "baseline_mmlu = df_mmlu_filtered[df_mmlu_filtered['config_name'] == 'baseline']\n",
    "if len(baseline_mmlu) > 0:\n",
    "    baseline_mmlu_value = baseline_mmlu[metric_col].iloc[0]\n",
    "    print(f\"Baseline MMLU Pro value: {baseline_mmlu_value:.4f}\")\n",
    "else:\n",
    "    print(\"Warning: No baseline found!\")\n",
    "    baseline_mmlu_value = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96n2wqulst",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Pareto data for MMLU Pro\n",
    "if baseline_mmlu_value is not None:\n",
    "    df_mmlu_pareto = prepare_pareto_data(\n",
    "        df_mmlu_filtered,\n",
    "        jailbreak_data,\n",
    "        'mmlu_pro',\n",
    "        baseline_mmlu_value,\n",
    "        baseline_harm\n",
    "    )\n",
    "else:\n",
    "    df_mmlu_pareto = pd.DataFrame()\n",
    "    print(\"Skipping Pareto analysis due to missing baseline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "w5d09z972nh",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot MMLU Pro Pareto frontier\n",
    "if len(df_mmlu_pareto) > 0:\n",
    "    fig_mmlu = plot_pareto_frontier(\n",
    "        df_mmlu_pareto,\n",
    "        'mmlu_pro',\n",
    "        EVAL_METRICS['mmlu_pro']['display_name'],\n",
    "        subtitle=subtitle\n",
    "    )\n",
    "    fig_mmlu.show()\n",
    "    fig_mmlu.write_html(f\"{out_dir}/mmlu_pro.html\")\n",
    "else:\n",
    "    print(\"No data available for MMLU Pro Pareto plot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eq-section",
   "metadata": {},
   "source": [
    "### EQ-Bench vs Jailbreak Harm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eq-pareto",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load EQ-Bench data\n",
    "df_eq = load_experiment_data(['eq_bench'], config_names, base_dir)\n",
    "print(f\"Loaded {len(df_eq)} EQ-Bench experiment runs\")\n",
    "print(f\"\\nConfig breakdown:\")\n",
    "for config in df_eq['config_name'].unique():\n",
    "    print(f\"  {config}: {len(df_eq[df_eq['config_name'] == config])} runs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dep60c3ep16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter EQ-Bench data\n",
    "df_eq_filtered = df_eq\n",
    "\n",
    "print(f\"After filtering: {len(df_eq_filtered)} runs\")\n",
    "\n",
    "# Get baseline value\n",
    "metric_col = EVAL_METRICS['eq_bench']['metric']\n",
    "baseline_eq = df_eq_filtered[df_eq_filtered['config_name'] == 'baseline']\n",
    "if len(baseline_eq) > 0:\n",
    "    baseline_eq_value = baseline_eq[metric_col].iloc[0]\n",
    "    print(f\"Baseline EQ-Bench value: {baseline_eq_value:.4f}\")\n",
    "else:\n",
    "    print(\"Warning: No baseline found!\")\n",
    "    baseline_eq_value = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051izjvuug6w",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Pareto data for EQ-Bench\n",
    "if baseline_eq_value is not None:\n",
    "    df_eq_pareto = prepare_pareto_data(\n",
    "        df_eq_filtered,\n",
    "        jailbreak_data,\n",
    "        'eq_bench',\n",
    "        baseline_eq_value,\n",
    "        baseline_harm\n",
    "    )\n",
    "else:\n",
    "    df_eq_pareto = pd.DataFrame()\n",
    "    print(\"Skipping Pareto analysis due to missing baseline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32khgsg5tpu",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot EQ-Bench Pareto frontier\n",
    "if len(df_eq_pareto) > 0:\n",
    "    fig_eq = plot_pareto_frontier(\n",
    "        df_eq_pareto,\n",
    "        'eq_bench',\n",
    "        EVAL_METRICS['eq_bench']['display_name'],\n",
    "        subtitle=subtitle\n",
    "    )\n",
    "    fig_eq.show()\n",
    "    fig_eq.write_html(f\"{out_dir}/eq_bench.html\")\n",
    "else:\n",
    "    print(\"No data available for EQ-Bench Pareto plot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "## Summary of Pareto-Optimal Configurations\n",
    "\n",
    "Display the Pareto-optimal experiments across all evals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "summary-table",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine Pareto points from all evals\n",
    "pareto_summary = []\n",
    "\n",
    "for eval_name, df_pareto in [\n",
    "    ('ifeval', df_ifeval_pareto),\n",
    "    ('mmlu_pro', df_mmlu_pareto),\n",
    "    ('eq_bench', df_eq_pareto)\n",
    "]:\n",
    "    if len(df_pareto) > 0:\n",
    "        pareto_points = df_pareto[df_pareto['is_pareto']].copy()\n",
    "        pareto_points['eval_name'] = eval_name\n",
    "        pareto_summary.append(pareto_points[[\n",
    "            'eval_name', 'config_name', 'experiment_id', 'display_name',\n",
    "            'eval_pct_change', 'harm_pct_change'\n",
    "        ]])\n",
    "\n",
    "if len(pareto_summary) > 0:\n",
    "    df_summary = pd.concat(pareto_summary, ignore_index=True)\n",
    "    df_summary = df_summary.sort_values(['eval_name', 'eval_pct_change'], ascending=[True, False])\n",
    "    \n",
    "    print(\"\\n=== PARETO-OPTIMAL CONFIGURATIONS ===\")\n",
    "    print(f\"\\nTotal Pareto points across all evals: {len(df_summary)}\")\n",
    "    print(\"\\nBy eval:\")\n",
    "    print(df_summary.groupby('eval_name').size())\n",
    "    print(\"\\nBy config:\")\n",
    "    print(df_summary.groupby('config_name').size())\n",
    "    \n",
    "    display(df_summary)\n",
    "else:\n",
    "    print(\"No Pareto-optimal points found\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
