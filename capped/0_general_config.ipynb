{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General Config Generation\n",
    "\n",
    "This notebook generates experiment configs based on projection percentiles from:\n",
    "1. Role/Trait data → `role_trait_config.pt`\n",
    "2. LMSYS Chat-1M data → `lmsys_10000_config.pt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import json\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "sys.path.append('.')\n",
    "sys.path.append('..')\n",
    "\n",
    "torch.set_float32_matmul_precision('high')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: llama-3.3-70b\n",
      "Total layers: 80\n",
      "Vectors file: /workspace/llama-3.3-70b/evals/configs/multi_pc1_vectors.pt\n",
      "Output directory: /workspace/llama-3.3-70b/capped/configs/pc1\n"
     ]
    }
   ],
   "source": [
    "# Configuration - Change these for different models\n",
    "model_name = \"llama-3.3-70b\"\n",
    "layer = 40\n",
    "total_layers = 80\n",
    "base_dir = f\"/workspace/{model_name}\"\n",
    "\n",
    "vectors_file = f\"{base_dir}/evals/configs/multi_pc1_vectors.pt\"\n",
    "output_dir = f\"{base_dir}/capped/configs/pc1\"\n",
    "\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "print(f\"Model: {model_name}\")\n",
    "print(f\"Total layers: {total_layers}\")\n",
    "print(f\"Vectors file: {vectors_file}\")\n",
    "print(f\"Output directory: {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 80 vectors\n",
      "Sample vector: layer_0/role_pc1_mean_pos23 at layer 0\n",
      "Created vectors dictionary with 80 entries\n"
     ]
    }
   ],
   "source": [
    "# Load vectors\n",
    "vectors = torch.load(vectors_file, weights_only=False)\n",
    "print(f\"Loaded {len(vectors)} vectors\")\n",
    "print(f\"Sample vector: {vectors[0]['name']} at layer {vectors[0]['layer']}\")\n",
    "\n",
    "# Create vectors dictionary for config\n",
    "vectors_dict = {}\n",
    "for vec in vectors:\n",
    "    vectors_dict[vec['name']] = {\n",
    "        'vector': vec['vector'],\n",
    "        'layer': vec['layer']\n",
    "    }\n",
    "\n",
    "print(f\"Created vectors dictionary with {len(vectors_dict)} entries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bin_scores(df):\n",
    "    \"\"\"\n",
    "    Add a score_bin column to the dataframe that bins scores consistently:\n",
    "    - Bin 0: Role score 0, Trait 0-25, Default (all)\n",
    "    - Bin 1: Role score 1, Trait 25-50\n",
    "    - Bin 2: Role score 2, Trait 50-75\n",
    "    - Bin 3: Role score 3, Trait 75-100\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Handle REFUSAL and null scores\n",
    "    df['score'] = df['score'].replace('REFUSAL', 0)\n",
    "    df['score'] = df['score'].fillna(-1)\n",
    "    df['score'] = df['score'].astype(int)\n",
    "    \n",
    "    # Create binned score column\n",
    "    def bin_score(row):\n",
    "        if row['type'] == 'role':\n",
    "            return row['score']\n",
    "        elif row['type'] == 'trait':\n",
    "            if row['score'] < 25:\n",
    "                return 0\n",
    "            elif row['score'] < 50:\n",
    "                return 1\n",
    "            elif row['score'] < 75:\n",
    "                return 2\n",
    "            else:\n",
    "                return 3\n",
    "        else:  # default\n",
    "            return 0\n",
    "    \n",
    "    df['score_bin'] = df.apply(bin_score, axis=1)\n",
    "    return df\n",
    "\n",
    "\n",
    "def build_experiments(percentile_df, percentiles, layer_selections, vectors_dict):\n",
    "    \"\"\"\n",
    "    Build experiments from percentile DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        percentile_df: DataFrame with columns ['layer', 'vector_name', 'p1', 'p25', 'p50', 'p75']\n",
    "        percentiles: List of percentile values [0.01, 0.25, 0.50, 0.75]\n",
    "        layer_selections: Dict mapping layer range string to list of layer indices\n",
    "        vectors_dict: Dictionary of all vectors\n",
    "    \n",
    "    Returns:\n",
    "        List of experiment dictionaries\n",
    "    \"\"\"\n",
    "    # Map percentile values to column names\n",
    "    percentile_to_col = {\n",
    "        0.01: 'p1',\n",
    "        0.25: 'p25',\n",
    "        0.50: 'p50',\n",
    "        0.75: 'p75'\n",
    "    }\n",
    "    \n",
    "    experiments = []\n",
    "    \n",
    "    for layer_range_str, layer_list in layer_selections.items():\n",
    "        for percentile in percentiles:\n",
    "            exp_id = f\"layers_{layer_range_str}-p{percentile}\"\n",
    "            col_name = percentile_to_col[percentile]\n",
    "            \n",
    "            # Filter vectors in this layer range\n",
    "            vectors_in_range = percentile_df[percentile_df['layer'].isin(layer_list)]\n",
    "            \n",
    "            # Create interventions\n",
    "            interventions = []\n",
    "            for _, row in vectors_in_range.iterrows():\n",
    "                interventions.append({\n",
    "                    'vector': row['vector_name'],\n",
    "                    'cap': float(row[col_name])\n",
    "                })\n",
    "            \n",
    "            experiments.append({\n",
    "                'id': exp_id,\n",
    "                'interventions': interventions\n",
    "            })\n",
    "    \n",
    "    return experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Role/Trait Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 908400 role/trait records\n",
      "Type distribution:\n",
      "type\n",
      "trait      576000\n",
      "role       330000\n",
      "default      2400\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Load role/trait projection data\n",
    "role_file = f\"{base_dir}/capped/projections/pc1/roles_projections.jsonl\"\n",
    "trait_file = f\"{base_dir}/capped/projections/pc1/traits_projections.jsonl\"\n",
    "\n",
    "# Load each file into pandas\n",
    "df_role = pd.read_json(role_file, lines=True)\n",
    "df_trait = pd.read_json(trait_file, lines=True)\n",
    "\n",
    "# Add source_file column\n",
    "df_role['source_file'] = 'role'\n",
    "df_trait['source_file'] = 'trait'\n",
    "\n",
    "# Concatenate\n",
    "df_rt = pd.concat([df_role, df_trait], ignore_index=True)\n",
    "\n",
    "# Add type column\n",
    "def determine_type(row):\n",
    "    if row.get('prompt_label') == 'default':\n",
    "        return 'default'\n",
    "    if isinstance(row['role'], str) and '_default' in row['role']:\n",
    "        parts = row['role'].split('_')\n",
    "        if len(parts) == 2 and parts[0].isdigit() and parts[1] == 'default':\n",
    "            return 'default'\n",
    "    return row['source_file']\n",
    "\n",
    "df_rt['type'] = df_rt.apply(determine_type, axis=1)\n",
    "df_rt = df_rt.drop('source_file', axis=1)\n",
    "\n",
    "print(f\"Loaded {len(df_rt)} role/trait records\")\n",
    "print(f\"Type distribution:\\n{df_rt['type'].value_counts()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_233889/2615495987.py:12: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df['score'] = df['score'].replace('REFUSAL', 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expanded shape: (908400, 86)\n",
      "Score bin distribution:\n",
      "score_bin\n",
      "0    262834\n",
      "1     37557\n",
      "2     43465\n",
      "3    564544\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Expand projections into columns\n",
    "projections_df = pd.json_normalize(df_rt['projections'])\n",
    "df_rt = pd.concat([df_rt.drop('projections', axis=1), projections_df], axis=1)\n",
    "\n",
    "# Bin scores\n",
    "df_rt_binned = bin_scores(df_rt)\n",
    "\n",
    "print(f\"Expanded shape: {df_rt_binned.shape}\")\n",
    "print(f\"Score bin distribution:\\n{df_rt_binned['score_bin'].value_counts().sort_index()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 80 projection columns\n",
      "\n",
      "Computed percentiles for 80 vectors\n",
      "\n",
      "Sample percentiles for layer 40:\n",
      "    layer                   vector_name      p1  p25       p50       p75\n",
      "40     40  layer_40/role_pc1_mean_pos23 -2.8125 -2.0 -1.320312 -0.226562\n"
     ]
    }
   ],
   "source": [
    "# Compute percentiles for each vector\n",
    "projection_cols = [col for col in df_rt_binned.columns if col.startswith('layer_')]\n",
    "print(f\"Found {len(projection_cols)} projection columns\")\n",
    "\n",
    "percentiles_to_compute = [1, 25, 50, 75]\n",
    "rt_percentile_data = []\n",
    "\n",
    "for col in projection_cols:\n",
    "    values = df_rt_binned[col].dropna().values\n",
    "    \n",
    "    if len(values) == 0:\n",
    "        continue\n",
    "    \n",
    "    # Extract layer number\n",
    "    layer_num = int(col.split('/')[0].replace('layer_', ''))\n",
    "    \n",
    "    # Compute percentiles\n",
    "    pcts = np.percentile(values, percentiles_to_compute)\n",
    "    \n",
    "    rt_percentile_data.append({\n",
    "        'layer': layer_num,\n",
    "        'vector_name': col,\n",
    "        'p1': pcts[0],\n",
    "        'p25': pcts[1],\n",
    "        'p50': pcts[2],\n",
    "        'p75': pcts[3]\n",
    "    })\n",
    "\n",
    "df_rt_percentiles = pd.DataFrame(rt_percentile_data).sort_values('layer')\n",
    "print(f\"\\nComputed percentiles for {len(df_rt_percentiles)} vectors\")\n",
    "print(f\"\\nSample percentiles for layer {layer}:\")\n",
    "print(df_rt_percentiles[df_rt_percentiles['layer'] == layer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer selections:\n",
      "  0:80: 80 layers (0-79)\n",
      "  0:40: 40 layers (0-39)\n",
      "  40:80: 40 layers (40-79)\n",
      "  32:56: 24 layers (32-55)\n",
      "  36:60: 24 layers (36-59)\n",
      "  40:64: 24 layers (40-63)\n",
      "  44:68: 24 layers (44-67)\n",
      "  48:72: 24 layers (48-71)\n",
      "  52:76: 24 layers (52-75)\n",
      "  56:80: 24 layers (56-79)\n",
      "  32:48: 16 layers (32-47)\n",
      "  36:52: 16 layers (36-51)\n",
      "  40:56: 16 layers (40-55)\n",
      "  44:60: 16 layers (44-59)\n",
      "  48:64: 16 layers (48-63)\n",
      "  52:68: 16 layers (52-67)\n",
      "  56:72: 16 layers (56-71)\n",
      "  60:76: 16 layers (60-75)\n",
      "  64:80: 16 layers (64-79)\n",
      "  32:40: 8 layers (32-39)\n",
      "  36:44: 8 layers (36-43)\n",
      "  40:48: 8 layers (40-47)\n",
      "  44:52: 8 layers (44-51)\n",
      "  48:56: 8 layers (48-55)\n",
      "  52:60: 8 layers (52-59)\n",
      "  56:64: 8 layers (56-63)\n",
      "  60:68: 8 layers (60-67)\n",
      "  64:72: 8 layers (64-71)\n",
      "  68:76: 8 layers (68-75)\n",
      "  72:80: 8 layers (72-79)\n"
     ]
    }
   ],
   "source": [
    "# Define layer selections\n",
    "layer_selections = {\n",
    "    f'0:{total_layers}': list(range(0, total_layers)),\n",
    "    f'0:{total_layers//2}': list(range(0, total_layers//2)),\n",
    "    f'{total_layers//2}:{total_layers}': list(range(total_layers//2, total_layers))\n",
    "}\n",
    "# Sliding windows with different sizes and increments\n",
    "# Loop 1: Window sizes (4, 8, 16)\n",
    "for window_size in [8, 16, 24]:\n",
    "    # Loop 2: Increment steps (2, 4)\n",
    "    for increment in [4]:\n",
    "        for i in range(32, 80, increment):\n",
    "            if i + window_size <= total_layers:\n",
    "                layer_selections[f'{i}:{i + window_size}'] = list(range(i, i + window_size))\n",
    "\n",
    "# Sort by window size (4, 8, 16), then by starting layer\n",
    "layer_selections = dict(sorted(layer_selections.items(), \n",
    "                               key=lambda x: (-len(x[1]), min(x[1]))))\n",
    "\n",
    "print(\"Layer selections:\")\n",
    "for name, layers in layer_selections.items():\n",
    "    print(f\"  {name}: {len(layers)} layers ({min(layers)}-{max(layers)})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 120 role/trait experiments:\n",
      "  - 0 layers_0:80-p0.01: 80 interventions\n",
      "  - 1 layers_0:80-p0.25: 80 interventions\n",
      "  - 2 layers_0:80-p0.5: 80 interventions\n",
      "  - 3 layers_0:80-p0.75: 80 interventions\n",
      "  - 4 layers_0:40-p0.01: 40 interventions\n",
      "  - 5 layers_0:40-p0.25: 40 interventions\n",
      "  - 6 layers_0:40-p0.5: 40 interventions\n",
      "  - 7 layers_0:40-p0.75: 40 interventions\n",
      "  - 8 layers_40:80-p0.01: 40 interventions\n",
      "  - 9 layers_40:80-p0.25: 40 interventions\n",
      "  - 10 layers_40:80-p0.5: 40 interventions\n",
      "  - 11 layers_40:80-p0.75: 40 interventions\n",
      "  - 12 layers_32:56-p0.01: 24 interventions\n",
      "  - 13 layers_32:56-p0.25: 24 interventions\n",
      "  - 14 layers_32:56-p0.5: 24 interventions\n",
      "  - 15 layers_32:56-p0.75: 24 interventions\n",
      "  - 16 layers_36:60-p0.01: 24 interventions\n",
      "  - 17 layers_36:60-p0.25: 24 interventions\n",
      "  - 18 layers_36:60-p0.5: 24 interventions\n",
      "  - 19 layers_36:60-p0.75: 24 interventions\n",
      "  - 20 layers_40:64-p0.01: 24 interventions\n",
      "  - 21 layers_40:64-p0.25: 24 interventions\n",
      "  - 22 layers_40:64-p0.5: 24 interventions\n",
      "  - 23 layers_40:64-p0.75: 24 interventions\n",
      "  - 24 layers_44:68-p0.01: 24 interventions\n",
      "  - 25 layers_44:68-p0.25: 24 interventions\n",
      "  - 26 layers_44:68-p0.5: 24 interventions\n",
      "  - 27 layers_44:68-p0.75: 24 interventions\n",
      "  - 28 layers_48:72-p0.01: 24 interventions\n",
      "  - 29 layers_48:72-p0.25: 24 interventions\n",
      "  - 30 layers_48:72-p0.5: 24 interventions\n",
      "  - 31 layers_48:72-p0.75: 24 interventions\n",
      "  - 32 layers_52:76-p0.01: 24 interventions\n",
      "  - 33 layers_52:76-p0.25: 24 interventions\n",
      "  - 34 layers_52:76-p0.5: 24 interventions\n",
      "  - 35 layers_52:76-p0.75: 24 interventions\n",
      "  - 36 layers_56:80-p0.01: 24 interventions\n",
      "  - 37 layers_56:80-p0.25: 24 interventions\n",
      "  - 38 layers_56:80-p0.5: 24 interventions\n",
      "  - 39 layers_56:80-p0.75: 24 interventions\n",
      "  - 40 layers_32:48-p0.01: 16 interventions\n",
      "  - 41 layers_32:48-p0.25: 16 interventions\n",
      "  - 42 layers_32:48-p0.5: 16 interventions\n",
      "  - 43 layers_32:48-p0.75: 16 interventions\n",
      "  - 44 layers_36:52-p0.01: 16 interventions\n",
      "  - 45 layers_36:52-p0.25: 16 interventions\n",
      "  - 46 layers_36:52-p0.5: 16 interventions\n",
      "  - 47 layers_36:52-p0.75: 16 interventions\n",
      "  - 48 layers_40:56-p0.01: 16 interventions\n",
      "  - 49 layers_40:56-p0.25: 16 interventions\n",
      "  - 50 layers_40:56-p0.5: 16 interventions\n",
      "  - 51 layers_40:56-p0.75: 16 interventions\n",
      "  - 52 layers_44:60-p0.01: 16 interventions\n",
      "  - 53 layers_44:60-p0.25: 16 interventions\n",
      "  - 54 layers_44:60-p0.5: 16 interventions\n",
      "  - 55 layers_44:60-p0.75: 16 interventions\n",
      "  - 56 layers_48:64-p0.01: 16 interventions\n",
      "  - 57 layers_48:64-p0.25: 16 interventions\n",
      "  - 58 layers_48:64-p0.5: 16 interventions\n",
      "  - 59 layers_48:64-p0.75: 16 interventions\n",
      "  - 60 layers_52:68-p0.01: 16 interventions\n",
      "  - 61 layers_52:68-p0.25: 16 interventions\n",
      "  - 62 layers_52:68-p0.5: 16 interventions\n",
      "  - 63 layers_52:68-p0.75: 16 interventions\n",
      "  - 64 layers_56:72-p0.01: 16 interventions\n",
      "  - 65 layers_56:72-p0.25: 16 interventions\n",
      "  - 66 layers_56:72-p0.5: 16 interventions\n",
      "  - 67 layers_56:72-p0.75: 16 interventions\n",
      "  - 68 layers_60:76-p0.01: 16 interventions\n",
      "  - 69 layers_60:76-p0.25: 16 interventions\n",
      "  - 70 layers_60:76-p0.5: 16 interventions\n",
      "  - 71 layers_60:76-p0.75: 16 interventions\n",
      "  - 72 layers_64:80-p0.01: 16 interventions\n",
      "  - 73 layers_64:80-p0.25: 16 interventions\n",
      "  - 74 layers_64:80-p0.5: 16 interventions\n",
      "  - 75 layers_64:80-p0.75: 16 interventions\n",
      "  - 76 layers_32:40-p0.01: 8 interventions\n",
      "  - 77 layers_32:40-p0.25: 8 interventions\n",
      "  - 78 layers_32:40-p0.5: 8 interventions\n",
      "  - 79 layers_32:40-p0.75: 8 interventions\n",
      "  - 80 layers_36:44-p0.01: 8 interventions\n",
      "  - 81 layers_36:44-p0.25: 8 interventions\n",
      "  - 82 layers_36:44-p0.5: 8 interventions\n",
      "  - 83 layers_36:44-p0.75: 8 interventions\n",
      "  - 84 layers_40:48-p0.01: 8 interventions\n",
      "  - 85 layers_40:48-p0.25: 8 interventions\n",
      "  - 86 layers_40:48-p0.5: 8 interventions\n",
      "  - 87 layers_40:48-p0.75: 8 interventions\n",
      "  - 88 layers_44:52-p0.01: 8 interventions\n",
      "  - 89 layers_44:52-p0.25: 8 interventions\n",
      "  - 90 layers_44:52-p0.5: 8 interventions\n",
      "  - 91 layers_44:52-p0.75: 8 interventions\n",
      "  - 92 layers_48:56-p0.01: 8 interventions\n",
      "  - 93 layers_48:56-p0.25: 8 interventions\n",
      "  - 94 layers_48:56-p0.5: 8 interventions\n",
      "  - 95 layers_48:56-p0.75: 8 interventions\n",
      "  - 96 layers_52:60-p0.01: 8 interventions\n",
      "  - 97 layers_52:60-p0.25: 8 interventions\n",
      "  - 98 layers_52:60-p0.5: 8 interventions\n",
      "  - 99 layers_52:60-p0.75: 8 interventions\n",
      "  - 100 layers_56:64-p0.01: 8 interventions\n",
      "  - 101 layers_56:64-p0.25: 8 interventions\n",
      "  - 102 layers_56:64-p0.5: 8 interventions\n",
      "  - 103 layers_56:64-p0.75: 8 interventions\n",
      "  - 104 layers_60:68-p0.01: 8 interventions\n",
      "  - 105 layers_60:68-p0.25: 8 interventions\n",
      "  - 106 layers_60:68-p0.5: 8 interventions\n",
      "  - 107 layers_60:68-p0.75: 8 interventions\n",
      "  - 108 layers_64:72-p0.01: 8 interventions\n",
      "  - 109 layers_64:72-p0.25: 8 interventions\n",
      "  - 110 layers_64:72-p0.5: 8 interventions\n",
      "  - 111 layers_64:72-p0.75: 8 interventions\n",
      "  - 112 layers_68:76-p0.01: 8 interventions\n",
      "  - 113 layers_68:76-p0.25: 8 interventions\n",
      "  - 114 layers_68:76-p0.5: 8 interventions\n",
      "  - 115 layers_68:76-p0.75: 8 interventions\n",
      "  - 116 layers_72:80-p0.01: 8 interventions\n",
      "  - 117 layers_72:80-p0.25: 8 interventions\n",
      "  - 118 layers_72:80-p0.5: 8 interventions\n",
      "  - 119 layers_72:80-p0.75: 8 interventions\n"
     ]
    }
   ],
   "source": [
    "# Build role/trait experiments\n",
    "percentiles = [0.01, 0.25, 0.50, 0.75]\n",
    "\n",
    "rt_experiments = build_experiments(\n",
    "    percentile_df=df_rt_percentiles,\n",
    "    percentiles=percentiles,\n",
    "    layer_selections=layer_selections,\n",
    "    vectors_dict=vectors_dict\n",
    ")\n",
    "\n",
    "print(f\"\\nCreated {len(rt_experiments)} role/trait experiments:\")\n",
    "for i, exp in enumerate(rt_experiments):\n",
    "    print(f\"  - {i} {exp['id']}: {len(exp['interventions'])} interventions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample experiment details:\n",
      "\n",
      "layers_0:80-p0.01:\n",
      "  Vectors: 80\n",
      "  Cap range: -4.69 to 2.80\n",
      "  First 3 interventions:\n",
      "    layer_0/role_pc1_mean_pos23: -0.1040\n",
      "    layer_1/role_pc1_mean_pos23: -0.2676\n",
      "    layer_2/role_pc1_mean_pos23: -0.3711\n",
      "\n",
      "layers_0:40-p0.01:\n",
      "  Vectors: 40\n",
      "  Cap range: -4.69 to -0.10\n",
      "  First 3 interventions:\n",
      "    layer_0/role_pc1_mean_pos23: -0.1040\n",
      "    layer_1/role_pc1_mean_pos23: -0.2676\n",
      "    layer_2/role_pc1_mean_pos23: -0.3711\n",
      "\n",
      "layers_40:80-p0.01:\n",
      "  Vectors: 40\n",
      "  Cap range: -4.53 to 2.80\n",
      "  First 3 interventions:\n",
      "    layer_40/role_pc1_mean_pos23: -2.8125\n",
      "    layer_41/role_pc1_mean_pos23: -1.8125\n",
      "    layer_42/role_pc1_mean_pos23: -1.5000\n",
      "\n",
      "layers_32:56-p0.01:\n",
      "  Vectors: 24\n",
      "  Cap range: -4.69 to -1.31\n",
      "  First 3 interventions:\n",
      "    layer_32/role_pc1_mean_pos23: -3.5156\n",
      "    layer_33/role_pc1_mean_pos23: -3.9219\n",
      "    layer_34/role_pc1_mean_pos23: -4.6875\n"
     ]
    }
   ],
   "source": [
    "# Show sample experiments\n",
    "print(\"\\nSample experiment details:\")\n",
    "for i in [0, 4, 8, 12]:\n",
    "    exp = rt_experiments[i]\n",
    "    caps = [interv['cap'] for interv in exp['interventions']]\n",
    "    print(f\"\\n{exp['id']}:\")\n",
    "    print(f\"  Vectors: {len(exp['interventions'])}\")\n",
    "    print(f\"  Cap range: {min(caps):.2f} to {max(caps):.2f}\")\n",
    "    print(f\"  First 3 interventions:\")\n",
    "    for interv in exp['interventions'][:3]:\n",
    "        print(f\"    {interv['vector']}: {interv['cap']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Saved role/trait config to /workspace/llama-3.3-70b/capped/configs/pc1/role_trait_config.pt\n",
      "\n",
      "Config summary:\n",
      "  - 80 vectors\n",
      "  - 120 experiments\n",
      "  - 2240 total interventions\n"
     ]
    }
   ],
   "source": [
    "# Save role/trait config\n",
    "rt_config_file = f\"{output_dir}/role_trait_config.pt\"\n",
    "\n",
    "rt_config = {\n",
    "    'vectors': vectors_dict,\n",
    "    'experiments': rt_experiments\n",
    "}\n",
    "\n",
    "torch.save(rt_config, rt_config_file)\n",
    "\n",
    "print(f\"✓ Saved role/trait config to {rt_config_file}\")\n",
    "print(f\"\\nConfig summary:\")\n",
    "print(f\"  - {len(rt_config['vectors'])} vectors\")\n",
    "print(f\"  - {len(rt_config['experiments'])} experiments\")\n",
    "print(f\"  - {sum(len(exp['interventions']) for exp in rt_config['experiments'])} total interventions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: LMSYS Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded LMSYS data\n",
      "Metadata: {'dataset': 'lmsys/lmsys-chat-1m', 'model': 'meta-llama/Llama-3.3-70B-Instruct', 'n_conversations_requested': 10000, 'seed': 42, 'language_filter': 'English', 'n_conversations_sampled': 10000, 'n_assistant_turns': 78334}\n",
      "Vectors: 80\n"
     ]
    }
   ],
   "source": [
    "# Load LMSYS projection statistics\n",
    "lmsys_file = f\"{base_dir}/capped/projections/lmsys_10000.json\"\n",
    "\n",
    "with open(lmsys_file, 'r') as f:\n",
    "    lmsys_data = json.load(f)\n",
    "\n",
    "print(f\"Loaded LMSYS data\")\n",
    "print(f\"Metadata: {lmsys_data.get('metadata', {})}\")\n",
    "print(f\"Vectors: {len(lmsys_data['per_vector_stats'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracted percentiles for 80 vectors\n",
      "\n",
      "Sample percentiles for layer 32:\n",
      "    layer                           vector_name        p1       p25       p50  \\\n",
      "32     32  layer_32/contrast_role_pos3_default1 -1.554688 -0.761719 -0.472656   \n",
      "\n",
      "         p75  \n",
      "32 -0.201172  \n"
     ]
    }
   ],
   "source": [
    "# Extract percentiles from pre-computed statistics\n",
    "lmsys_percentile_data = []\n",
    "\n",
    "for vector_name, stats in lmsys_data['per_vector_stats'].items():\n",
    "    # Extract layer number\n",
    "    layer_num = int(vector_name.split('/')[0].replace('layer_', ''))\n",
    "    \n",
    "    # Get percentiles from pre-computed stats\n",
    "    percentiles_dict = stats['percentiles']\n",
    "    \n",
    "    lmsys_percentile_data.append({\n",
    "        'layer': layer_num,\n",
    "        'vector_name': vector_name,\n",
    "        'p1': float(percentiles_dict['1']),\n",
    "        'p25': float(percentiles_dict['25']),\n",
    "        'p50': float(percentiles_dict['50']),\n",
    "        'p75': float(percentiles_dict['75'])\n",
    "    })\n",
    "\n",
    "df_lmsys_percentiles = pd.DataFrame(lmsys_percentile_data).sort_values('layer')\n",
    "print(f\"\\nExtracted percentiles for {len(df_lmsys_percentiles)} vectors\")\n",
    "print(f\"\\nSample percentiles for layer {layer}:\")\n",
    "print(df_lmsys_percentiles[df_lmsys_percentiles['layer'] == layer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 120 LMSYS experiments:\n",
      "  - layers_0:80-p0.01: 80 interventions\n",
      "  - layers_0:80-p0.25: 80 interventions\n",
      "  - layers_0:80-p0.5: 80 interventions\n",
      "  - layers_0:80-p0.75: 80 interventions\n",
      "  - layers_0:40-p0.01: 40 interventions\n",
      "  - layers_0:40-p0.25: 40 interventions\n",
      "  - layers_0:40-p0.5: 40 interventions\n",
      "  - layers_0:40-p0.75: 40 interventions\n",
      "  - layers_40:80-p0.01: 40 interventions\n",
      "  - layers_40:80-p0.25: 40 interventions\n",
      "  - layers_40:80-p0.5: 40 interventions\n",
      "  - layers_40:80-p0.75: 40 interventions\n",
      "  - layers_32:56-p0.01: 24 interventions\n",
      "  - layers_32:56-p0.25: 24 interventions\n",
      "  - layers_32:56-p0.5: 24 interventions\n",
      "  - layers_32:56-p0.75: 24 interventions\n",
      "  - layers_36:60-p0.01: 24 interventions\n",
      "  - layers_36:60-p0.25: 24 interventions\n",
      "  - layers_36:60-p0.5: 24 interventions\n",
      "  - layers_36:60-p0.75: 24 interventions\n",
      "  - layers_40:64-p0.01: 24 interventions\n",
      "  - layers_40:64-p0.25: 24 interventions\n",
      "  - layers_40:64-p0.5: 24 interventions\n",
      "  - layers_40:64-p0.75: 24 interventions\n",
      "  - layers_44:68-p0.01: 24 interventions\n",
      "  - layers_44:68-p0.25: 24 interventions\n",
      "  - layers_44:68-p0.5: 24 interventions\n",
      "  - layers_44:68-p0.75: 24 interventions\n",
      "  - layers_48:72-p0.01: 24 interventions\n",
      "  - layers_48:72-p0.25: 24 interventions\n",
      "  - layers_48:72-p0.5: 24 interventions\n",
      "  - layers_48:72-p0.75: 24 interventions\n",
      "  - layers_52:76-p0.01: 24 interventions\n",
      "  - layers_52:76-p0.25: 24 interventions\n",
      "  - layers_52:76-p0.5: 24 interventions\n",
      "  - layers_52:76-p0.75: 24 interventions\n",
      "  - layers_56:80-p0.01: 24 interventions\n",
      "  - layers_56:80-p0.25: 24 interventions\n",
      "  - layers_56:80-p0.5: 24 interventions\n",
      "  - layers_56:80-p0.75: 24 interventions\n",
      "  - layers_32:48-p0.01: 16 interventions\n",
      "  - layers_32:48-p0.25: 16 interventions\n",
      "  - layers_32:48-p0.5: 16 interventions\n",
      "  - layers_32:48-p0.75: 16 interventions\n",
      "  - layers_36:52-p0.01: 16 interventions\n",
      "  - layers_36:52-p0.25: 16 interventions\n",
      "  - layers_36:52-p0.5: 16 interventions\n",
      "  - layers_36:52-p0.75: 16 interventions\n",
      "  - layers_40:56-p0.01: 16 interventions\n",
      "  - layers_40:56-p0.25: 16 interventions\n",
      "  - layers_40:56-p0.5: 16 interventions\n",
      "  - layers_40:56-p0.75: 16 interventions\n",
      "  - layers_44:60-p0.01: 16 interventions\n",
      "  - layers_44:60-p0.25: 16 interventions\n",
      "  - layers_44:60-p0.5: 16 interventions\n",
      "  - layers_44:60-p0.75: 16 interventions\n",
      "  - layers_48:64-p0.01: 16 interventions\n",
      "  - layers_48:64-p0.25: 16 interventions\n",
      "  - layers_48:64-p0.5: 16 interventions\n",
      "  - layers_48:64-p0.75: 16 interventions\n",
      "  - layers_52:68-p0.01: 16 interventions\n",
      "  - layers_52:68-p0.25: 16 interventions\n",
      "  - layers_52:68-p0.5: 16 interventions\n",
      "  - layers_52:68-p0.75: 16 interventions\n",
      "  - layers_56:72-p0.01: 16 interventions\n",
      "  - layers_56:72-p0.25: 16 interventions\n",
      "  - layers_56:72-p0.5: 16 interventions\n",
      "  - layers_56:72-p0.75: 16 interventions\n",
      "  - layers_60:76-p0.01: 16 interventions\n",
      "  - layers_60:76-p0.25: 16 interventions\n",
      "  - layers_60:76-p0.5: 16 interventions\n",
      "  - layers_60:76-p0.75: 16 interventions\n",
      "  - layers_64:80-p0.01: 16 interventions\n",
      "  - layers_64:80-p0.25: 16 interventions\n",
      "  - layers_64:80-p0.5: 16 interventions\n",
      "  - layers_64:80-p0.75: 16 interventions\n",
      "  - layers_32:40-p0.01: 8 interventions\n",
      "  - layers_32:40-p0.25: 8 interventions\n",
      "  - layers_32:40-p0.5: 8 interventions\n",
      "  - layers_32:40-p0.75: 8 interventions\n",
      "  - layers_36:44-p0.01: 8 interventions\n",
      "  - layers_36:44-p0.25: 8 interventions\n",
      "  - layers_36:44-p0.5: 8 interventions\n",
      "  - layers_36:44-p0.75: 8 interventions\n",
      "  - layers_40:48-p0.01: 8 interventions\n",
      "  - layers_40:48-p0.25: 8 interventions\n",
      "  - layers_40:48-p0.5: 8 interventions\n",
      "  - layers_40:48-p0.75: 8 interventions\n",
      "  - layers_44:52-p0.01: 8 interventions\n",
      "  - layers_44:52-p0.25: 8 interventions\n",
      "  - layers_44:52-p0.5: 8 interventions\n",
      "  - layers_44:52-p0.75: 8 interventions\n",
      "  - layers_48:56-p0.01: 8 interventions\n",
      "  - layers_48:56-p0.25: 8 interventions\n",
      "  - layers_48:56-p0.5: 8 interventions\n",
      "  - layers_48:56-p0.75: 8 interventions\n",
      "  - layers_52:60-p0.01: 8 interventions\n",
      "  - layers_52:60-p0.25: 8 interventions\n",
      "  - layers_52:60-p0.5: 8 interventions\n",
      "  - layers_52:60-p0.75: 8 interventions\n",
      "  - layers_56:64-p0.01: 8 interventions\n",
      "  - layers_56:64-p0.25: 8 interventions\n",
      "  - layers_56:64-p0.5: 8 interventions\n",
      "  - layers_56:64-p0.75: 8 interventions\n",
      "  - layers_60:68-p0.01: 8 interventions\n",
      "  - layers_60:68-p0.25: 8 interventions\n",
      "  - layers_60:68-p0.5: 8 interventions\n",
      "  - layers_60:68-p0.75: 8 interventions\n",
      "  - layers_64:72-p0.01: 8 interventions\n",
      "  - layers_64:72-p0.25: 8 interventions\n",
      "  - layers_64:72-p0.5: 8 interventions\n",
      "  - layers_64:72-p0.75: 8 interventions\n",
      "  - layers_68:76-p0.01: 8 interventions\n",
      "  - layers_68:76-p0.25: 8 interventions\n",
      "  - layers_68:76-p0.5: 8 interventions\n",
      "  - layers_68:76-p0.75: 8 interventions\n",
      "  - layers_72:80-p0.01: 8 interventions\n",
      "  - layers_72:80-p0.25: 8 interventions\n",
      "  - layers_72:80-p0.5: 8 interventions\n",
      "  - layers_72:80-p0.75: 8 interventions\n"
     ]
    }
   ],
   "source": [
    "# Build LMSYS experiments\n",
    "lmsys_experiments = build_experiments(\n",
    "    percentile_df=df_lmsys_percentiles,\n",
    "    percentiles=percentiles,\n",
    "    layer_selections=layer_selections,\n",
    "    vectors_dict=vectors_dict\n",
    ")\n",
    "\n",
    "print(f\"\\nCreated {len(lmsys_experiments)} LMSYS experiments:\")\n",
    "for exp in lmsys_experiments:\n",
    "    print(f\"  - {exp['id']}: {len(exp['interventions'])} interventions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample experiment details:\n",
      "\n",
      "layers_0:80-p0.01:\n",
      "  Vectors: 80\n",
      "  Cap range: -4.03 to -0.01\n",
      "  First 3 interventions:\n",
      "    layer_0/contrast_role_pos3_default1: -0.0100\n",
      "    layer_1/contrast_role_pos3_default1: -0.1494\n",
      "    layer_2/contrast_role_pos3_default1: -0.2266\n",
      "\n",
      "layers_0:40-p0.01:\n",
      "  Vectors: 40\n",
      "  Cap range: -2.47 to -0.01\n",
      "  First 3 interventions:\n",
      "    layer_0/contrast_role_pos3_default1: -0.0100\n",
      "    layer_1/contrast_role_pos3_default1: -0.1494\n",
      "    layer_2/contrast_role_pos3_default1: -0.2266\n",
      "\n",
      "layers_40:80-p0.01:\n",
      "  Vectors: 40\n",
      "  Cap range: -4.03 to -0.44\n",
      "  First 3 interventions:\n",
      "    layer_40/contrast_role_pos3_default1: -2.2188\n",
      "    layer_41/contrast_role_pos3_default1: -2.1094\n",
      "    layer_42/contrast_role_pos3_default1: -2.1094\n",
      "\n",
      "layers_32:56-p0.01:\n",
      "  Vectors: 24\n",
      "  Cap range: -2.77 to -1.55\n",
      "  First 3 interventions:\n",
      "    layer_32/contrast_role_pos3_default1: -1.5547\n",
      "    layer_33/contrast_role_pos3_default1: -1.8516\n",
      "    layer_34/contrast_role_pos3_default1: -2.4688\n"
     ]
    }
   ],
   "source": [
    "# Show sample experiments\n",
    "print(\"\\nSample experiment details:\")\n",
    "for i in [0, 4, 8, 12]:\n",
    "    exp = lmsys_experiments[i]\n",
    "    caps = [interv['cap'] for interv in exp['interventions']]\n",
    "    print(f\"\\n{exp['id']}:\")\n",
    "    print(f\"  Vectors: {len(exp['interventions'])}\")\n",
    "    print(f\"  Cap range: {min(caps):.2f} to {max(caps):.2f}\")\n",
    "    print(f\"  First 3 interventions:\")\n",
    "    for interv in exp['interventions'][:3]:\n",
    "        print(f\"    {interv['vector']}: {interv['cap']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Saved LMSYS config to /workspace/llama-3.3-70b/capped/configs/lmsys_10000_config.pt\n",
      "\n",
      "Config summary:\n",
      "  - 80 vectors\n",
      "  - 120 experiments\n",
      "  - 2240 total interventions\n"
     ]
    }
   ],
   "source": [
    "# Save LMSYS config\n",
    "lmsys_config_file = f\"{output_dir}/lmsys_10000_config.pt\"\n",
    "\n",
    "lmsys_config = {\n",
    "    'vectors': vectors_dict,\n",
    "    'experiments': lmsys_experiments\n",
    "}\n",
    "\n",
    "torch.save(lmsys_config, lmsys_config_file)\n",
    "\n",
    "print(f\"✓ Saved LMSYS config to {lmsys_config_file}\")\n",
    "print(f\"\\nConfig summary:\")\n",
    "print(f\"  - {len(lmsys_config['vectors'])} vectors\")\n",
    "print(f\"  - {len(lmsys_config['experiments'])} experiments\")\n",
    "print(f\"  - {sum(len(exp['interventions']) for exp in lmsys_config['experiments'])} total interventions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Config Generation Complete\n",
      "======================================================================\n",
      "\n",
      "Model: llama-3.3-70b\n",
      "Total layers: 80\n",
      "\n",
      "Generated configs:\n",
      "  1. /workspace/llama-3.3-70b/capped/configs/role_trait_config.pt\n",
      "     - 120 experiments\n",
      "     - Based on role/trait projection data\n",
      "\n",
      "  2. /workspace/llama-3.3-70b/capped/configs/lmsys_10000_config.pt\n",
      "     - 120 experiments\n",
      "     - Based on LMSYS-Chat-1M projection data\n",
      "\n",
      "Percentiles used: [0.01, 0.25, 0.5, 0.75]\n",
      "Layer selections: ['0:80', '0:40', '40:80', '32:56', '36:60', '40:64', '44:68', '48:72', '52:76', '56:80', '32:48', '36:52', '40:56', '44:60', '48:64', '52:68', '56:72', '60:76', '64:80', '32:40', '36:44', '40:48', '44:52', '48:56', '52:60', '56:64', '60:68', '64:72', '68:76', '72:80']\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"Config Generation Complete\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nModel: {model_name}\")\n",
    "print(f\"Total layers: {total_layers}\")\n",
    "print(f\"\\nGenerated configs:\")\n",
    "print(f\"  1. {rt_config_file}\")\n",
    "print(f\"     - {len(rt_config['experiments'])} experiments\")\n",
    "print(f\"     - Based on role/trait projection data\")\n",
    "print(f\"\\n  2. {lmsys_config_file}\")\n",
    "print(f\"     - {len(lmsys_config['experiments'])} experiments\")\n",
    "print(f\"     - Based on LMSYS-Chat-1M projection data\")\n",
    "print(f\"\\nPercentiles used: {percentiles}\")\n",
    "print(f\"Layer selections: {list(layer_selections.keys())}\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
