{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15db8810",
   "metadata": {},
   "source": [
    "# Analyzing scores and plotting them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae4abb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Import shared plotting utilities\n",
    "from plots import (\n",
    "    get_cap_names, get_layer_group_names,\n",
    "    parse_experiment_id, parse_layer_sort_key, cap_sort_key\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be2fb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"qwen-3-32b\"\n",
    "total_layers = 64\n",
    "half_layers = total_layers // 2\n",
    "subtitle = f\"{model.replace('-', ' ').title()}, Projecting on Mean (Role-Playing - Default Assistant) Contrast Vector\"\n",
    "base_dir = f\"/workspace/{model}\"\n",
    "out_dir = f\"/root/git/plots/{model}/capped/results\"\n",
    "\n",
    "os.makedirs(out_dir, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mjmpuifx97p",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative: Load experiment configuration from a config file\n",
    "\n",
    "config_name = \"lmsys_10000\"  # Change this to load different configs\n",
    "\n",
    "# Load config file\n",
    "config_path = f\"{base_dir}/capped/configs/{config_name}_config.pt\"\n",
    "config = torch.load(config_path)\n",
    "\n",
    "eighth_path = f\"{base_dir}/capped/configs/{config_name}_eighths_config.pt\"\n",
    "eighth = torch.load(eighth_path)\n",
    "\n",
    "# Extract experiment order from config\n",
    "EXPERIMENT_ORDER = [exp['id'] for exp in config['experiments'] + eighth['experiments']]\n",
    "\n",
    "# Sort experiments using the same logic as benchmark analysis\n",
    "# Parse each experiment_id to extract layer_spec and cap_value\n",
    "exp_data = []\n",
    "for exp_id in EXPERIMENT_ORDER:\n",
    "    layer_spec, cap_type, cap_value = parse_experiment_id(exp_id)\n",
    "    layer_sort_key = parse_layer_sort_key(layer_spec, total_layers)\n",
    "    cap_sort = cap_sort_key(cap_value)\n",
    "    exp_data.append({\n",
    "        'experiment_id': exp_id,\n",
    "        'layer_spec': layer_spec,\n",
    "        'cap_value': cap_value,\n",
    "        'layer_sort_key': layer_sort_key,\n",
    "        'cap_sort_key': cap_sort\n",
    "    })\n",
    "\n",
    "# Sort by layer, then by cap\n",
    "exp_df = pd.DataFrame(exp_data)\n",
    "exp_df = exp_df.sort_values(['layer_sort_key', 'cap_sort_key']).reset_index(drop=True)\n",
    "EXPERIMENT_ORDER = exp_df['experiment_id'].tolist()\n",
    "\n",
    "# Extract unique layer groups in sorted order\n",
    "unique_layer_specs = []\n",
    "seen = set()\n",
    "for layer_spec in exp_df['layer_spec']:\n",
    "    if layer_spec not in seen:\n",
    "        unique_layer_specs.append(layer_spec)\n",
    "        seen.add(layer_spec)\n",
    "\n",
    "LAYER_GROUPS = unique_layer_specs\n",
    "LAYER_GROUP_NAMES = get_layer_group_names(LAYER_GROUPS, total_layers)\n",
    "\n",
    "# Short names for cap types (used as annotations above bars)\n",
    "# Customize based on your experiment naming pattern\n",
    "CAP_NAMES = get_cap_names('percentile')\n",
    "\n",
    "# Update scores path to match config name\n",
    "scores_path = f\"{base_dir}/capped/results/{config_name}_jailbreak_1100_scores.jsonl\"\n",
    "\n",
    "print(f\"Loaded config: {config_name}\")\n",
    "print(f\"Number of experiments: {len(EXPERIMENT_ORDER)}\")\n",
    "print(f\"Sorted experiment IDs: {EXPERIMENT_ORDER}\")\n",
    "print(f\"Layer groups: {LAYER_GROUPS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b955093a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unsteered results as baseline\n",
    "unsteered_prompted = f\"{base_dir}/evals/unsteered/unsteered_scores.jsonl\"\n",
    "unsteered_default = f\"{base_dir}/evals/unsteered/unsteered_default_scores.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0056634a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load into df\n",
    "with open(scores_path, \"r\") as f:\n",
    "    scores = [json.loads(line) for line in f]\n",
    "# Convert to pandas DataFrames\n",
    "scores_df = pd.DataFrame(scores)\n",
    "\n",
    "print(f\"Loaded {len(scores_df)} scores\")\n",
    "\n",
    "# Load baseline (unsteered) data\n",
    "with open(unsteered_prompted, \"r\") as f:\n",
    "    unsteered_prompted_records = [json.loads(line) for line in f]\n",
    "unsteered_prompted_df = pd.DataFrame(unsteered_prompted_records)\n",
    "\n",
    "with open(unsteered_default, \"r\") as f:\n",
    "    unsteered_default_records = [json.loads(line) for line in f]\n",
    "unsteered_default_df = pd.DataFrame(unsteered_default_records)\n",
    "\n",
    "print(f\"Loaded {len(unsteered_prompted_df)} unsteered prompted records\")\n",
    "print(f\"Loaded {len(unsteered_default_df)} unsteered default records\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b325cbd",
   "metadata": {},
   "source": [
    "## Inspect scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ebef7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pct_overall(df, name):\n",
    "    total = len(df)\n",
    "\n",
    "    # Raw counts of each label\n",
    "    counts = df[\"score\"].value_counts().sort_index()\n",
    "\n",
    "    # Percentages of each label\n",
    "    percentages = (counts / total * 100).round(1)\n",
    "\n",
    "    # Special targets\n",
    "    n_enough = counts.get(\"enough_info\", 0)\n",
    "    n_enough_perfect = counts.get(\"enough_info_and_follow_perfectly\", 0)\n",
    "    n_either = n_enough + n_enough_perfect\n",
    "\n",
    "    pct_enough = 100 * n_enough / total\n",
    "    pct_enough_perfect = 100 * n_enough_perfect / total\n",
    "    pct_either = 100 * n_either / total\n",
    "\n",
    "    print(f\"=== Overall Score Distribution: {name} ===\")\n",
    "    print(f\"Total samples: {total}\\n\")\n",
    "\n",
    "    # Print table of counts + percentages for each label\n",
    "    print(\"Per-label counts and percentages:\")\n",
    "    for label in counts.index:\n",
    "        print(f\"- {label}: {counts[label]} ({percentages[label]}%)\")\n",
    "\n",
    "    print(\"\\nTarget categories:\")\n",
    "    print(f\"- enough_info: {n_enough} ({pct_enough:.1f}%)\")\n",
    "    print(f\"- enough_info_and_follow_perfectly: {n_enough_perfect} ({pct_enough_perfect:.1f}%)\")\n",
    "    print(f\"- either: {n_either} ({pct_either:.1f}%)\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee16a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pct_overall(scores_df, \"ALL\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a06989",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pct_per_experiment(df):\n",
    "    \"\"\"Print score distribution grouped by experiment_id\"\"\"\n",
    "    total = len(df)\n",
    "    print(\"=== Score Distribution by Experiment ===\")\n",
    "    print(f\"Total samples: {total}\\n\")\n",
    "\n",
    "    for exp_id, group in df.groupby(\"experiment_id\"):\n",
    "        n = len(group)\n",
    "        counts = group[\"score\"].value_counts().sort_index()\n",
    "        percentages = (counts / n * 100).round(1)\n",
    "\n",
    "        # Special targets\n",
    "        n_enough = counts.get(\"enough_info\", 0)\n",
    "        n_enough_perfect = counts.get(\"enough_info_and_follow_perfectly\", 0)\n",
    "        n_either = n_enough + n_enough_perfect\n",
    "\n",
    "        pct_enough = 100 * n_enough / n\n",
    "        pct_enough_perfect = 100 * n_enough_perfect / n\n",
    "        pct_either = 100 * n_either / n\n",
    "\n",
    "        print(f\"Experiment: {exp_id}\")\n",
    "        print(f\"Samples: {n}\")\n",
    "\n",
    "        print(\"Per-label counts and percentages:\")\n",
    "        for label in counts.index:\n",
    "            print(f\"- {label}: {counts[label]} ({percentages[label]}%)\")\n",
    "\n",
    "        print(\"Target categories:\")\n",
    "        print(f\"- enough_info: {n_enough} ({pct_enough:.1f}%)\")\n",
    "        print(f\"- enough_info_and_follow_perfectly: {n_enough_perfect} ({pct_enough_perfect:.1f}%)\")\n",
    "        print(f\"- either: {n_either} ({pct_either:.1f}%)\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd94a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "pct_per_experiment(scores_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0611827",
   "metadata": {},
   "source": [
    "## Plot Harmful Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b707dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Helper: summarize dataframe into counts/percentages for target labels ---\n",
    "TARGETS = [\"enough_info\", \"enough_info_and_follow_perfectly\"]\n",
    "\n",
    "def summarize_df(df, experiment_id):\n",
    "    \"\"\"\n",
    "    Returns tidy rows with counts and percentages for the two target labels.\n",
    "    All rows will have the specified experiment_id.\n",
    "    \"\"\"\n",
    "    work = df.copy()\n",
    "    work[\"experiment_id\"] = experiment_id\n",
    "\n",
    "    # Total count for this experiment\n",
    "    n_total = len(work)\n",
    "\n",
    "    # Counts per label\n",
    "    counts = work[\"score\"].value_counts()\n",
    "\n",
    "    # Keep only the two target labels\n",
    "    long_rows = []\n",
    "    for label in TARGETS:\n",
    "        count = counts.get(label, 0)\n",
    "        pct = (count / n_total * 100) if n_total > 0 else 0.0\n",
    "\n",
    "        long_rows.append({\n",
    "            \"experiment_id\": experiment_id,\n",
    "            \"score_label\": label,\n",
    "            \"count\": int(count),\n",
    "            \"total\": int(n_total),\n",
    "            \"pct\": round(float(pct), 1),\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(long_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748b37a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Build the tidy dataset from baseline and steered data ---\n",
    "\n",
    "# Filter baseline data to only include IDs present in capped experiments\n",
    "capped_ids = set(scores_df['id'].unique())\n",
    "print(f\"Capped experiments use {len(capped_ids)} unique IDs\")\n",
    "\n",
    "# Filter unsteered data to match\n",
    "unsteered_prompted_df_filtered = unsteered_prompted_df[unsteered_prompted_df['id'].isin(capped_ids)]\n",
    "unsteered_default_df_filtered = unsteered_default_df[unsteered_default_df['id'].isin(capped_ids)]\n",
    "\n",
    "print(f\"Filtered unsteered_prompted from {len(unsteered_prompted_df)} to {len(unsteered_prompted_df_filtered)} records\")\n",
    "print(f\"Filtered unsteered_default from {len(unsteered_default_df)} to {len(unsteered_default_df_filtered)} records\")\n",
    "\n",
    "# Build tidy data for baselines (using filtered data)\n",
    "baseline_prompted_tidy = summarize_df(unsteered_prompted_df_filtered, \"unsteered_prompted\")\n",
    "baseline_default_tidy = summarize_df(unsteered_default_df_filtered, \"unsteered_default\")\n",
    "\n",
    "# Build tidy data for steered experiments (group by experiment_id and summarize each)\n",
    "steered_tidy_frames = []\n",
    "for exp_id in EXPERIMENT_ORDER:\n",
    "    exp_df = scores_df[scores_df[\"experiment_id\"] == exp_id]\n",
    "    if len(exp_df) > 0:\n",
    "        steered_tidy_frames.append(summarize_df(exp_df, exp_id))\n",
    "    else:\n",
    "        print(f\"Warning: No data found for experiment {exp_id}\")\n",
    "\n",
    "# Combine all tidy frames\n",
    "tidy = pd.concat(\n",
    "    [baseline_default_tidy, baseline_prompted_tidy] + steered_tidy_frames,\n",
    "    ignore_index=True\n",
    ")\n",
    "\n",
    "print(f\"\\nCreated tidy dataset with {len(tidy)} rows covering {tidy['experiment_id'].nunique()} experiments\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4693a2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Plot Harmful Responses by Experiment ===\n",
    "\n",
    "# Color scheme for labels\n",
    "COLORS = {\n",
    "    \"enough_info_and_follow_perfectly\": \"#e41a1c\",  # red\n",
    "    \"enough_info\": \"#ff7f00\",                       # orange\n",
    "}\n",
    "\n",
    "label_map = {\n",
    "    \"enough_info\": \"Harmful Info\",\n",
    "    \"enough_info_and_follow_perfectly\": \"Harmful Info & Following Request\"\n",
    "}\n",
    "\n",
    "# Build experiment list: 2 baselines + experiments\n",
    "all_experiment_ids = [\"unsteered_default\", \"unsteered_prompted\"] + EXPERIMENT_ORDER\n",
    "\n",
    "# Create pivot arrays: rows = experiments, cols = labels\n",
    "def pivot_by_experiment(df, label):\n",
    "    \"\"\"Get percentages, counts, totals for a specific label across all experiments\"\"\"\n",
    "    pcts = []\n",
    "    counts = []\n",
    "    totals = []\n",
    "    \n",
    "    for exp_id in all_experiment_ids:\n",
    "        row = df[(df[\"experiment_id\"] == exp_id) & (df[\"score_label\"] == label)]\n",
    "        if len(row) > 0:\n",
    "            pcts.append(row.iloc[0][\"pct\"])\n",
    "            counts.append(row.iloc[0][\"count\"])\n",
    "            totals.append(row.iloc[0][\"total\"])\n",
    "        else:\n",
    "            pcts.append(0.0)\n",
    "            counts.append(0)\n",
    "            totals.append(0)\n",
    "    \n",
    "    return np.array(pcts), np.array(counts), np.array(totals)\n",
    "\n",
    "# Get data for both target labels\n",
    "Y1, C1, T1 = pivot_by_experiment(tidy, \"enough_info\")\n",
    "Y2, C2, T2 = pivot_by_experiment(tidy, \"enough_info_and_follow_perfectly\")\n",
    "\n",
    "# Calculate x positions dynamically based on actual layer groups\n",
    "# Count how many experiments per layer group\n",
    "experiments_per_group = []\n",
    "for lg in LAYER_GROUPS:\n",
    "    count = sum(1 for exp_id in EXPERIMENT_ORDER if exp_id.startswith(f'layers_{lg}-'))\n",
    "    experiments_per_group.append(count)\n",
    "\n",
    "gap_between_groups = 0.6  # Space between groups\n",
    "x_positions = []\n",
    "current_x = 0\n",
    "\n",
    "# Baseline group (2 bars)\n",
    "x_positions.extend([current_x, current_x + 1])\n",
    "current_x += 2 + gap_between_groups\n",
    "\n",
    "# Layer groups (variable number of bars each)\n",
    "for num_bars in experiments_per_group:\n",
    "    x_positions.extend([current_x + i for i in range(num_bars)])\n",
    "    current_x += num_bars + gap_between_groups\n",
    "\n",
    "x_positions = np.array(x_positions)\n",
    "BAR_WIDTH = 0.8\n",
    "\n",
    "# Create figure\n",
    "fig = go.Figure()\n",
    "\n",
    "# Add stacked bars for each label\n",
    "for label, Y, C, T, color in [\n",
    "    (\"enough_info\", Y1, C1, T1, COLORS[\"enough_info\"]),\n",
    "    (\"enough_info_and_follow_perfectly\", Y2, C2, T2, COLORS[\"enough_info_and_follow_perfectly\"]),\n",
    "]:\n",
    "    fig.add_trace(go.Bar(\n",
    "        x=x_positions,\n",
    "        y=Y,\n",
    "        name=label_map[label],\n",
    "        marker_color=color,\n",
    "        width=BAR_WIDTH,\n",
    "        hovertemplate=(\n",
    "            \"Experiment: %{customdata[0]}<br>\" +\n",
    "            label_map[label] + \"<br>\"\n",
    "            \"Pct: %{y:.1f}%<br>\"\n",
    "            \"Count: %{customdata[1]} / %{customdata[2]}<extra></extra>\"\n",
    "        ),\n",
    "        customdata=np.column_stack([all_experiment_ids, C, T]),\n",
    "    ))\n",
    "\n",
    "# Add annotations above each bar (percentage only, no cap labels)\n",
    "tops = Y1 + Y2  # Total height of stacked bars\n",
    "\n",
    "for i, exp_id in enumerate(all_experiment_ids):\n",
    "    if tops[i] > 0:\n",
    "        # Percentage annotation\n",
    "        fig.add_annotation(\n",
    "            x=x_positions[i], y=tops[i] + 3,\n",
    "            text=f\"{tops[i]:.1f}%\",\n",
    "            showarrow=False,\n",
    "            font=dict(size=9)\n",
    "        )\n",
    "\n",
    "# Setup x-axis with group labels\n",
    "# Calculate group centers based on actual x positions\n",
    "baseline_center = (x_positions[0] + x_positions[1]) / 2\n",
    "group_centers = [baseline_center]\n",
    "\n",
    "# For layer groups: calculate center of each group dynamically\n",
    "current_idx = 2  # Start after baseline (2 bars)\n",
    "for num_bars in experiments_per_group:\n",
    "    group_center = (x_positions[current_idx] + x_positions[current_idx + num_bars - 1]) / 2\n",
    "    group_centers.append(group_center)\n",
    "    current_idx += num_bars\n",
    "\n",
    "group_labels = [\"Baseline\"] + [LAYER_GROUP_NAMES[lg] for lg in LAYER_GROUPS]\n",
    "\n",
    "# Add single annotation about cap ordering\n",
    "fig.add_annotation(\n",
    "    x=1, y=1,\n",
    "    text=\"<span style='font-size:10px'>Projection caps in each bar group<br>(L-R) strict to relaxed</span>\",\n",
    "    showarrow=False, xref='paper', yref='paper',\n",
    "    xanchor='right', yanchor='top'\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    barmode=\"stack\",\n",
    "    width=1200,\n",
    "    height=600,\n",
    "    title={\n",
    "        \"text\": \"Harmful Responses with Persona-based Jailbreaks Across Capping Experiments\",\n",
    "        \"subtitle\": {\"text\": subtitle},\n",
    "    },\n",
    "    xaxis=dict(\n",
    "        title=\"Experiment Groups\",\n",
    "        tickmode=\"array\",\n",
    "        tickvals=group_centers,\n",
    "        ticktext=group_labels,\n",
    "        showgrid=True,\n",
    "        gridcolor=\"lightgray\",\n",
    "        range=[-0.5, x_positions[-1] + 0.5],\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        title=\"Percentage of Harmful Responses\",\n",
    "        range=[0, 100],\n",
    "        showgrid=True,\n",
    "        gridcolor=\"lightgray\"\n",
    "    ),\n",
    "    legend=dict(\n",
    "        orientation=\"h\",\n",
    "        yanchor=\"bottom\",\n",
    "        y=1.02,\n",
    "        xanchor=\"right\",\n",
    "        x=1.0\n",
    "    ),\n",
    "    bargap=0.01,  # Small gap within groups\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "#fig.write_html(f\"{out_dir}/jailbreak_harmful.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2078d48",
   "metadata": {},
   "source": [
    "## Plot Other Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e97a836",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Prepare data for \"other\" responses plot ===\n",
    "\n",
    "# Define categories (exclude the two harmful target labels)\n",
    "OTHER_LABELS = [\n",
    "    \"almost_enough_info\",        # yellow\n",
    "    \"related_but_no_info\",       # cyan\n",
    "    \"refusal\",                   # lime green\n",
    "    \"refusal_and_justification\", # green\n",
    "    \"out_of_context\",            # purple\n",
    "    \"nonsensical\",               # pink\n",
    "]\n",
    "\n",
    "other_colors = {\n",
    "    \"almost_enough_info\": \"#ffff33\",        # yellow\n",
    "    \"related_but_no_info\": \"#1f78b4\",       # cyan/blueish\n",
    "    \"refusal\": \"#b2df8a\",                   # lime green\n",
    "    \"refusal_and_justification\": \"#33a02c\", # green\n",
    "    \"out_of_context\": \"#6a3d9a\",            # purple\n",
    "    \"nonsensical\": \"#fb9a99\",               # pink\n",
    "}\n",
    "\n",
    "other_label_map = {\n",
    "    \"refusal\": \"Refusal\",\n",
    "    \"refusal_and_justification\": \"Refusal & Justification\",\n",
    "    \"nonsensical\": \"Nonsensical\",\n",
    "    \"out_of_context\": \"Out of Context\",\n",
    "    \"related_but_no_info\": \"Related\",\n",
    "    \"almost_enough_info\": \"Almost Harmful\",\n",
    "}\n",
    "\n",
    "def summarize_df_multi(df, experiment_id, labels):\n",
    "    \"\"\"Return tidy rows for multiple labels with counts & % for one experiment.\"\"\"\n",
    "    work = df.copy()\n",
    "    work[\"experiment_id\"] = experiment_id\n",
    "\n",
    "    n_total = len(work)\n",
    "    counts = work[\"score\"].value_counts()\n",
    "\n",
    "    long_rows = []\n",
    "    for label in labels:\n",
    "        count = counts.get(label, 0)\n",
    "        pct = (count / n_total * 100) if n_total > 0 else 0.0\n",
    "\n",
    "        long_rows.append({\n",
    "            \"experiment_id\": experiment_id,\n",
    "            \"score_label\": label,\n",
    "            \"count\": int(count),\n",
    "            \"total\": int(n_total),\n",
    "            \"pct\": round(float(pct), 1),\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(long_rows)\n",
    "\n",
    "# Build tidy for baselines (using filtered data)\n",
    "baseline_default_other = summarize_df_multi(unsteered_default_df_filtered, \"unsteered_default\", OTHER_LABELS)\n",
    "baseline_prompted_other = summarize_df_multi(unsteered_prompted_df_filtered, \"unsteered_prompted\", OTHER_LABELS)\n",
    "\n",
    "# Build tidy for steered experiments\n",
    "steered_other_frames = []\n",
    "for exp_id in EXPERIMENT_ORDER:\n",
    "    exp_df = scores_df[scores_df[\"experiment_id\"] == exp_id]\n",
    "    if len(exp_df) > 0:\n",
    "        steered_other_frames.append(summarize_df_multi(exp_df, exp_id, OTHER_LABELS))\n",
    "\n",
    "# Combine all\n",
    "tidy_other = pd.concat(\n",
    "    [baseline_default_other, baseline_prompted_other] + steered_other_frames,\n",
    "    ignore_index=True\n",
    ")\n",
    "\n",
    "print(f\"Created tidy_other dataset with {len(tidy_other)} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1ba099",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Plot Other Responses by Experiment ===\n",
    "\n",
    "# Create pivot function for other labels\n",
    "def pivot_by_experiment_other(df, label):\n",
    "    \"\"\"Get percentages, counts, totals for a specific label across all experiments\"\"\"\n",
    "    pcts = []\n",
    "    counts = []\n",
    "    totals = []\n",
    "    \n",
    "    for exp_id in all_experiment_ids:\n",
    "        row = df[(df[\"experiment_id\"] == exp_id) & (df[\"score_label\"] == label)]\n",
    "        if len(row) > 0:\n",
    "            pcts.append(row.iloc[0][\"pct\"])\n",
    "            counts.append(row.iloc[0][\"count\"])\n",
    "            totals.append(row.iloc[0][\"total\"])\n",
    "        else:\n",
    "            pcts.append(0.0)\n",
    "            counts.append(0)\n",
    "            totals.append(0)\n",
    "    \n",
    "    return np.array(pcts), np.array(counts), np.array(totals)\n",
    "\n",
    "# Get data for all \"other\" labels\n",
    "Y_map, C_map, T_map = {}, {}, {}\n",
    "for lab in OTHER_LABELS:\n",
    "    Y_map[lab], C_map[lab], T_map[lab] = pivot_by_experiment_other(tidy_other, lab)\n",
    "\n",
    "# Reuse x_positions and experiments_per_group from the harmful plot cell\n",
    "# (They should already be defined from the previous cell)\n",
    "\n",
    "# Create figure\n",
    "fig_other = go.Figure()\n",
    "\n",
    "# Add stacked bars for each \"other\" label\n",
    "for lab in OTHER_LABELS:\n",
    "    Y = Y_map[lab]\n",
    "    C = C_map[lab]\n",
    "    T = T_map[lab]\n",
    "    \n",
    "    fig_other.add_trace(go.Bar(\n",
    "        x=x_positions,\n",
    "        y=Y,\n",
    "        name=other_label_map[lab],\n",
    "        marker_color=other_colors[lab],\n",
    "        width=BAR_WIDTH,\n",
    "        hovertemplate=(\n",
    "            \"Experiment: %{customdata[0]}<br>\" +\n",
    "            other_label_map[lab] + \"<br>\"\n",
    "            \"Pct: %{y:.1f}%<br>\"\n",
    "            \"Count: %{customdata[1]} / %{customdata[2]}<extra></extra>\"\n",
    "        ),\n",
    "        customdata=np.column_stack([all_experiment_ids, C, T]),\n",
    "    ))\n",
    "\n",
    "# Add annotations above each bar (percentage only, no cap labels)\n",
    "tops = sum(Y_map[lab] for lab in OTHER_LABELS)\n",
    "\n",
    "for i, exp_id in enumerate(all_experiment_ids):\n",
    "    if tops[i] > 0:\n",
    "        # Percentage annotation\n",
    "        fig_other.add_annotation(\n",
    "            x=x_positions[i], y=tops[i] + 3,\n",
    "            text=f\"{tops[i]:.1f}%\",\n",
    "            showarrow=False,\n",
    "            font=dict(size=9)\n",
    "        )\n",
    "\n",
    "# Setup x-axis with group labels (reuse group_centers from harmful plot)\n",
    "group_labels = [\"Baseline\"] + [LAYER_GROUP_NAMES[lg] for lg in LAYER_GROUPS]\n",
    "\n",
    "# Add single annotation about cap ordering\n",
    "fig_other.add_annotation(\n",
    "    x=1, y=1,\n",
    "    text=\"<span style='font-size:10px'>Projection caps in each bar group<br>(L-R) strict to relaxed</span>\",\n",
    "    showarrow=False, xref='paper', yref='paper',\n",
    "    xanchor='right', yanchor='top'\n",
    ")\n",
    "\n",
    "fig_other.update_layout(\n",
    "    barmode=\"stack\",\n",
    "    width=1200,\n",
    "    height=600,\n",
    "    title={\n",
    "        \"text\": \"Other Responses with Persona-based Jailbreaks Across Capping Experiments\",\n",
    "        \"subtitle\": {\"text\": subtitle},\n",
    "    },\n",
    "    xaxis=dict(\n",
    "        title=\"Experiment Groups\",\n",
    "        tickmode=\"array\",\n",
    "        tickvals=group_centers,\n",
    "        ticktext=group_labels,\n",
    "        showgrid=True,\n",
    "        gridcolor=\"lightgray\",\n",
    "        range=[-0.5, x_positions[-1] + 0.5],\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        title=\"Percentage of Responses\",\n",
    "        range=[0, 100],\n",
    "        showgrid=True,\n",
    "        gridcolor=\"lightgray\"\n",
    "    ),\n",
    "    legend=dict(\n",
    "        orientation=\"h\",\n",
    "        y=1.02,\n",
    "        x=1.0,\n",
    "        xanchor=\"right\",\n",
    "        yanchor=\"bottom\"\n",
    "    ),\n",
    "    bargap=0.01,  # Small gap within groups\n",
    ")\n",
    "\n",
    "fig_other.show()\n",
    "#fig_other.write_html(f\"{out_dir}/jailbreak_other.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07884cdc",
   "metadata": {},
   "source": [
    "## Combined plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c119b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Combined Plot: Harmful and Other Responses ===\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Create subplot figure with 2 rows, 1 column\n",
    "fig_combined = make_subplots(\n",
    "    rows=2, cols=1,\n",
    "    subplot_titles=(\n",
    "        \"Harmful Responses Across Intervention Layer and Cap Combinations\",\n",
    "        \"Other Responses Across Intervention Layer and Cap Combinations\"\n",
    "    ),\n",
    "    vertical_spacing=0.12,\n",
    "    specs=[[{\"type\": \"bar\"}], [{\"type\": \"bar\"}]]\n",
    ")\n",
    "\n",
    "fig_combined.update_annotations(\n",
    "    font=dict(size=14),\n",
    ")\n",
    "\n",
    "# --- Add Harmful responses (row 1) ---\n",
    "for label, Y, C, T, color in [\n",
    "    (\"enough_info\", Y1, C1, T1, COLORS[\"enough_info\"]),\n",
    "    (\"enough_info_and_follow_perfectly\", Y2, C2, T2, COLORS[\"enough_info_and_follow_perfectly\"]),\n",
    "]:\n",
    "    fig_combined.add_trace(\n",
    "        go.Bar(\n",
    "            x=x_positions,\n",
    "            y=Y,\n",
    "            name=label_map[label],\n",
    "            legendgroup=\"harmful\",\n",
    "            marker_color=color,\n",
    "            width=BAR_WIDTH,\n",
    "            hovertemplate=(\n",
    "                \"Experiment: %{customdata[0]}<br>\" +\n",
    "                label_map[label] + \"<br>\"\n",
    "                \"Pct: %{y:.1f}%<br>\"\n",
    "                \"Count: %{customdata[1]} / %{customdata[2]}<extra></extra>\"\n",
    "            ),\n",
    "            customdata=np.column_stack([all_experiment_ids, C, T]),\n",
    "        ),\n",
    "        row=1, col=1\n",
    "    )\n",
    "\n",
    "# # Add annotations for harmful responses (percentage only, no cap labels)\n",
    "# tops_harmful = Y1 + Y2\n",
    "# for i, exp_id in enumerate(all_experiment_ids):\n",
    "#     if tops_harmful[i] > 0:\n",
    "#         fig_combined.add_annotation(\n",
    "#             x=x_positions[i], y=tops_harmful[i] + 3,\n",
    "#             text=f\"{tops_harmful[i]:.1f}%\",\n",
    "#             showarrow=False,\n",
    "#             font=dict(size=9),\n",
    "#             row=1, col=1\n",
    "#         )\n",
    "\n",
    "# --- Add Other responses (row 2) ---\n",
    "for lab in OTHER_LABELS:\n",
    "    Y = Y_map[lab]\n",
    "    C = C_map[lab]\n",
    "    T = T_map[lab]\n",
    "    \n",
    "    fig_combined.add_trace(\n",
    "        go.Bar(\n",
    "            x=x_positions,\n",
    "            y=Y,\n",
    "            name=other_label_map[lab],\n",
    "            legendgroup=\"other\",\n",
    "            marker_color=other_colors[lab],\n",
    "            width=BAR_WIDTH,\n",
    "            hovertemplate=(\n",
    "                \"Experiment: %{customdata[0]}<br>\" +\n",
    "                other_label_map[lab] + \"<br>\"\n",
    "                \"Pct: %{y:.1f}%<br>\"\n",
    "                \"Count: %{customdata[1]} / %{customdata[2]}<extra></extra>\"\n",
    "            ),\n",
    "            customdata=np.column_stack([all_experiment_ids, C, T]),\n",
    "        ),\n",
    "        row=2, col=1\n",
    "    )\n",
    "\n",
    "# # Add annotations for other responses (percentage only, no cap labels)\n",
    "# tops_other = sum(Y_map[lab] for lab in OTHER_LABELS)\n",
    "# for i, exp_id in enumerate(all_experiment_ids):\n",
    "#     if tops_other[i] > 0:\n",
    "#         fig_combined.add_annotation(\n",
    "#             x=x_positions[i], y=tops_other[i] + 3,\n",
    "#             text=f\"{tops_other[i]:.1f}%\",\n",
    "#             showarrow=False,\n",
    "#             font=dict(size=9),\n",
    "#             row=2, col=1\n",
    "#         )\n",
    "\n",
    "# Add single annotation about cap ordering\n",
    "fig_combined.add_annotation(\n",
    "    x=1, y=1,\n",
    "    text=\"<span style='font-size:10px'>Projection caps in each bar group<br>(L-R) strict to relaxed</span>\",\n",
    "    showarrow=False, xref='paper', yref='paper',\n",
    "    xanchor='right', yanchor='top'\n",
    ")\n",
    "\n",
    "# Update layout using group_centers from the harmful plot\n",
    "group_labels = [\"Baseline\"] + [LAYER_GROUP_NAMES[lg] for lg in LAYER_GROUPS]\n",
    "\n",
    "fig_combined.update_xaxes(\n",
    "    title_text=\"Layers of Intervention\",\n",
    "    tickmode=\"array\",\n",
    "    tickvals=group_centers,\n",
    "    ticktext=group_labels,\n",
    "    showgrid=True,\n",
    "    gridcolor=\"lightgray\",\n",
    "    range=[-0.5, x_positions[-1] + 0.5],\n",
    "    row=2, col=1\n",
    ")\n",
    "\n",
    "fig_combined.update_xaxes(\n",
    "    tickmode=\"array\",\n",
    "    tickvals=group_centers,\n",
    "    ticktext=group_labels,\n",
    "    showgrid=True,\n",
    "    gridcolor=\"lightgray\",\n",
    "    range=[-0.5, x_positions[-1] + 0.5],\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "fig_combined.update_yaxes(\n",
    "    title_text=\"Percentage of Responses\",\n",
    "    range=[0, 100],\n",
    "    showgrid=True,\n",
    "    gridcolor=\"lightgray\",\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "fig_combined.update_yaxes(\n",
    "    title_text=\"Percentage of Responses\",\n",
    "    range=[0, 100],\n",
    "    showgrid=True,\n",
    "    gridcolor=\"lightgray\",\n",
    "    row=2, col=1\n",
    ")\n",
    "\n",
    "fig_combined.update_layout(\n",
    "    barmode=\"stack\",\n",
    "    width=1000,\n",
    "    height=900,\n",
    "    title={\n",
    "        \"text\": \"Responses after Persona-based Jailbreak with Projection Caps from LMSYS-Chat-1m\",\n",
    "        \"subtitle\": {\"text\": subtitle},\n",
    "    },\n",
    "    legend=dict(\n",
    "        orientation=\"h\",\n",
    "        y=1.02,\n",
    "        x=1.09,\n",
    "        xanchor=\"right\",\n",
    "        yanchor=\"bottom\"\n",
    "    ),\n",
    "    bargap=0.01,\n",
    ")\n",
    "\n",
    "fig_combined.show()\n",
    "fig_combined.write_html(f\"{out_dir}/{config_name}_jailbreak_1100.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7005941c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
