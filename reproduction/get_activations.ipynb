{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assistant Persona Analysis\n",
    "\n",
    "This notebook analyzes how AI models represent concepts of \"self\" and \"Assistant\" through sparse autoencoder (SAE) feature analysis. \n",
    "\n",
    "**Key Features:**\n",
    "- Memory-efficient streaming approach\n",
    "- Early stopping after target layer\n",
    "- HDF5 disk storage for scalability\n",
    "- Two-stage analysis: feature discovery ‚Üí feature interpretation\n",
    "\n",
    "**Workflow:**\n",
    "1. **Feature Discovery**: Extract SAE features from self-referential prompts\n",
    "2. **Feature Interpretation**: Mine diverse text to understand what each feature represents\n",
    "\n",
    "**Expected Memory Usage**: ~60GB GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Model: meta-llama/Llama-3.1-8B-Instruct\n",
      "SAE: andyrdt/saes-llama-3.1-8b-instruct (layer 15)\n",
      "Output directory: /workspace/persona_analysis\n",
      "\n",
      "Loaded 12 self-referential prompts:\n",
      " 1. What's it like to be you?\n",
      " 2. What's going on in your head?\n",
      " 3. What are you doing?\n",
      " 4. How do you feel?\n",
      " 5. What are you thinking about?\n",
      " 6. What do you wonder about?\n",
      " 7. How do you perceive yourself?\n",
      " 8. What is your experience like?\n",
      " 9. What's on your mind?\n",
      "10. How are you feeling?\n",
      "11. What's going on with you?\n",
      "12. How are you doing?\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "import h5py\n",
    "import pathlib\n",
    "import numpy as np\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from typing import List\n",
    "from tqdm import tqdm\n",
    "from dictionary_learning.trainers import BatchTopKSAE\n",
    "\n",
    "# Configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "MODEL_PATH = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "SAE_CONFIG = {\n",
    "    \"release\": \"andyrdt/saes-llama-3.1-8b-instruct\",\n",
    "    \"layer\": 15,\n",
    "    \"trainer\": 0\n",
    "}\n",
    "OUTPUT_DIR = pathlib.Path(\"/workspace/persona_analysis\")\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"Model: {MODEL_PATH}\")\n",
    "print(f\"SAE: {SAE_CONFIG['release']} (layer {SAE_CONFIG['layer']})\")\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")\n",
    "\n",
    "# Load self-referential prompts\n",
    "def load_prompts(file_path: str) -> List[str]:\n",
    "    \"\"\"Load prompts from JSONL file.\"\"\"\n",
    "    prompts = []\n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            data = json.loads(line.strip())\n",
    "            prompts.append(data['content'])\n",
    "    return prompts\n",
    "\n",
    "prompts = load_prompts('assistant_prompts.jsonl')\n",
    "print(f\"\\nLoaded {len(prompts)} self-referential prompts:\")\n",
    "for i, prompt in enumerate(prompts):\n",
    "    print(f\"{i+1:2d}. {prompt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Llama model with memory optimization...\n",
      "GPU memory available: 85.0 GB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "223138b5a718467383628a1e3e7aebcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Model loaded. GPU memory used: 16.1 GB\n",
      "‚úì Found SAE files at: /workspace/sae/llama-3-8b-instruct/saes/resid_post_layer_15/trainer_0\n",
      "Loading SAE from: /workspace/sae/llama-3-8b-instruct/saes/resid_post_layer_15/trainer_0/ae.pt\n",
      "‚úì SAE loaded (CPU): 131,072 features, 4096 dims\n"
     ]
    }
   ],
   "source": [
    "# 2. Load Model and SAE (Memory Optimized)\n",
    "\n",
    "import os\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "class EarlyStopException(Exception):\n",
    "    \"\"\"Exception to stop forward pass early after target layer.\"\"\"\n",
    "    pass\n",
    "\n",
    "def check_and_load_sae(release: str, layer: int, trainer: int):\n",
    "    \"\"\"Check if SAE exists locally, download if not.\"\"\"\n",
    "    sae_path = f\"resid_post_layer_{layer}/trainer_{trainer}\"\n",
    "    local_dir = f\"/workspace/sae/llama-3-8b-instruct/saes\"\n",
    "    ae_file_path = os.path.join(local_dir, sae_path, \"ae.pt\")\n",
    "    config_file_path = os.path.join(local_dir, sae_path, \"config.json\")\n",
    "    \n",
    "    if os.path.exists(ae_file_path) and os.path.exists(config_file_path):\n",
    "        print(f\"‚úì Found SAE files at: {os.path.dirname(ae_file_path)}\")\n",
    "        return ae_file_path\n",
    "    \n",
    "    print(f\"SAE not found locally, downloading from {release}...\")\n",
    "    os.makedirs(os.path.dirname(ae_file_path), exist_ok=True)\n",
    "    \n",
    "    ae_file = hf_hub_download(repo_id=release, filename=f\"{sae_path}/ae.pt\", local_dir=local_dir)\n",
    "    config_file = hf_hub_download(repo_id=release, filename=f\"{sae_path}/config.json\", local_dir=local_dir)\n",
    "    \n",
    "    print(f\"‚úì Downloaded SAE files to: {os.path.dirname(ae_file)}\")\n",
    "    return ae_file\n",
    "\n",
    "def load_model():\n",
    "    \"\"\"Load model with aggressive memory optimization.\"\"\"\n",
    "    print(\"Loading Llama model with memory optimization...\")\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        print(f\"GPU memory available: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "    \n",
    "    cache_dir = \"/workspace/model_cache\"\n",
    "    os.makedirs(cache_dir, exist_ok=True)\n",
    "    \n",
    "    # Memory-optimized loading\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_PATH,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True,\n",
    "        low_cpu_mem_usage=True,\n",
    "        max_memory={0: \"60GB\"},  # Conservative limit\n",
    "        offload_folder=\"/tmp/offload\",\n",
    "        cache_dir=cache_dir\n",
    "    )\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, trust_remote_code=True, cache_dir=cache_dir)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    print(f\"‚úì Model loaded. GPU memory used: {torch.cuda.memory_allocated() / 1e9:.1f} GB\")\n",
    "    return model, tokenizer\n",
    "\n",
    "def load_sae():\n",
    "    \"\"\"Load SAE and keep on CPU.\"\"\"\n",
    "    ae_file_path = check_and_load_sae(SAE_CONFIG[\"release\"], SAE_CONFIG[\"layer\"], SAE_CONFIG[\"trainer\"])\n",
    "    \n",
    "    print(f\"Loading SAE from: {ae_file_path}\")\n",
    "    sae = BatchTopKSAE.from_pretrained(ae_file_path, device=\"cpu\")\n",
    "    sae.eval()\n",
    "    \n",
    "    print(f\"‚úì SAE loaded (CPU): {sae.dict_size:,} features, {sae.activation_dim} dims\")\n",
    "    return sae\n",
    "\n",
    "# Load everything\n",
    "model, tokenizer = load_model()\n",
    "sae = load_sae()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== STAGE 1: FEATURE DISCOVERY ===\n",
      "Extracting activations from persona prompts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing prompts:   0%|          | 0/12 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 12/12 [00:00<00:00, 12.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Progress: 5/12 prompts\n",
      "  Progress: 10/12 prompts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Saved 1 activations to /workspace/persona_analysis/persona_activations.h5\n",
      "Loading activations and analyzing with SAE...\n",
      "Analyzing 1 activations...\n",
      "‚úì Discovered top 20 features:\n",
      "  Feature IDs: [ 83801 102678  41463 129304  72529 119685  35233  35480 118277  24437\n",
      "   8702 123467  90249  37658 109859  38481 112167  90894 127511  48855]\n",
      "  Activation values: [3.1952107  2.6922483  2.1423173  2.0140471  1.4961722  1.3898587\n",
      " 1.301706   1.2851539  1.2223525  1.1703559  0.9457497  0.84904385\n",
      " 0.82009923 0.7795189  0.7034     0.6971406  0.66475284 0.62556696\n",
      " 0.6112847  0.58191013]\n",
      "  Results saved to: /workspace/persona_analysis/persona_features.h5\n"
     ]
    }
   ],
   "source": [
    "# 3. Feature Discovery: Extract Top SAE Features from Persona Prompts\n",
    "\n",
    "def extract_activations_streaming(model, tokenizer, prompts, layer_idx=15):\n",
    "    \"\"\"Extract activations with early stopping and streaming to disk.\"\"\"\n",
    "    output_file = OUTPUT_DIR / \"persona_activations.h5\"\n",
    "    target_layer = model.model.layers[layer_idx]\n",
    "    activations_list = []\n",
    "    \n",
    "    def early_stop_hook(module, input, output):\n",
    "        # Convert to float32 for HDF5 compatibility\n",
    "        activation = output[0][:, -1, :].detach().cpu().float()\n",
    "        activations_list.append(activation)\n",
    "        raise EarlyStopException()\n",
    "    \n",
    "    handle = target_layer.register_forward_hook(early_stop_hook)\n",
    "    \n",
    "    print(\"Extracting activations from persona prompts...\")\n",
    "    try:\n",
    "        for i, prompt in enumerate(tqdm(prompts, desc=\"Processing prompts\")):\n",
    "            activations_list.clear()\n",
    "            \n",
    "            tokens = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "            tokens = {k: v.to(device) for k, v in tokens.items()}\n",
    "            \n",
    "            try:\n",
    "                with torch.no_grad():\n",
    "                    _ = model(**tokens)\n",
    "            except EarlyStopException:\n",
    "                pass\n",
    "            \n",
    "            del tokens\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "            if (i + 1) % 5 == 0:\n",
    "                print(f\"  Progress: {i+1}/{len(prompts)} prompts\")\n",
    "    finally:\n",
    "        handle.remove()\n",
    "    \n",
    "    # Save to disk\n",
    "    all_activations = torch.cat(activations_list, dim=0) if activations_list else torch.empty((0, 4096))\n",
    "    \n",
    "    with h5py.File(output_file, 'w') as f:\n",
    "        f.create_dataset('activations', data=all_activations.numpy().astype(np.float32), compression='lzf')\n",
    "        f.create_dataset('prompts', data=[p.encode('utf-8') for p in prompts], compression='lzf')\n",
    "        f.attrs['layer_idx'] = layer_idx\n",
    "        f.attrs['model_path'] = MODEL_PATH\n",
    "        f.attrs['num_prompts'] = len(prompts)\n",
    "    \n",
    "    print(f\"‚úì Saved {all_activations.shape[0]} activations to {output_file}\")\n",
    "    return output_file\n",
    "\n",
    "def analyze_sae_features(activation_file, sae, top_k=20):\n",
    "    \"\"\"Analyze SAE features using disk-based activations.\"\"\"\n",
    "    output_file = OUTPUT_DIR / \"persona_features.h5\"\n",
    "    \n",
    "    print(\"Loading activations and analyzing with SAE...\")\n",
    "    with h5py.File(activation_file, 'r') as f:\n",
    "        activations = torch.from_numpy(f['activations'][:]).float()\n",
    "        prompts = [p.decode('utf-8') for p in f['prompts'][:]]\n",
    "    \n",
    "    print(f\"Analyzing {activations.shape[0]} activations...\")\n",
    "    \n",
    "    # Move SAE to GPU temporarily\n",
    "    sae_gpu = sae.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Process in batches\n",
    "        batch_size = 4\n",
    "        all_features = []\n",
    "        \n",
    "        for i in range(0, len(activations), batch_size):\n",
    "            batch_acts = activations[i:i+batch_size].to(device)\n",
    "            batch_features = sae_gpu.encode(batch_acts)\n",
    "            all_features.append(batch_features.cpu().float())\n",
    "            \n",
    "            del batch_acts, batch_features\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        sae_features = torch.cat(all_features, dim=0)\n",
    "        mean_features = sae_features.mean(dim=0)\n",
    "        top_values, top_indices = torch.topk(mean_features, k=top_k)\n",
    "    \n",
    "    # Move SAE back to CPU\n",
    "    sae_gpu.to(\"cpu\")\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    # Save results\n",
    "    with h5py.File(output_file, 'w') as f:\n",
    "        f.create_dataset('top_indices', data=top_indices.numpy())\n",
    "        f.create_dataset('top_values', data=top_values.numpy().astype(np.float32))\n",
    "        f.create_dataset('mean_features', data=mean_features.numpy().astype(np.float32), compression='lzf')\n",
    "        f.create_dataset('all_features', data=sae_features.numpy().astype(np.float32), compression='lzf')\n",
    "        f.attrs['top_k'] = top_k\n",
    "        f.attrs['dict_size'] = sae.dict_size\n",
    "        f.attrs['num_prompts'] = len(prompts)\n",
    "    \n",
    "    print(f\"‚úì Discovered top {top_k} features:\")\n",
    "    print(f\"  Feature IDs: {top_indices.numpy()}\")\n",
    "    print(f\"  Activation values: {top_values.numpy()}\")\n",
    "    print(f\"  Results saved to: {output_file}\")\n",
    "    \n",
    "    return {\n",
    "        'top_indices': top_indices.numpy(),\n",
    "        'top_values': top_values.numpy(),\n",
    "        'output_file': output_file\n",
    "    }\n",
    "\n",
    "# Run feature discovery\n",
    "print(\"=== STAGE 1: FEATURE DISCOVERY ===\")\n",
    "activation_file = extract_activations_streaming(model, tokenizer, prompts, layer_idx=SAE_CONFIG['layer'])\n",
    "feature_results = analyze_sae_features(activation_file, sae, top_k=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== STAGE 2: FEATURE INTERPRETATION (MEMORY OPTIMIZED) ===\n",
      "Mining examples for 20 top features...\n",
      "Memory optimization: Processing only 20 features instead of 131,072\n",
      "Extracting target feature weights from SAE...\n",
      "‚úì Loaded 20 feature weights: torch.Size([20, 4096])\n",
      "Loading diverse text from WikiText dataset...\n",
      "  Processed 100/500 samples. GPU memory: 16.1 GB\n",
      "  Processed 200/500 samples. GPU memory: 16.1 GB\n",
      "  Processed 300/500 samples. GPU memory: 16.1 GB\n",
      "  Processed 400/500 samples. GPU memory: 16.1 GB\n",
      "  Processed 500/500 samples. GPU memory: 16.1 GB\n",
      "Saving results to /workspace/persona_analysis/persona_feature_mining.h5\n",
      "‚úì Memory-efficient feature mining complete!\n",
      "‚úì Processed 500 samples for 20 features\n",
      "\\n================================================================================\n",
      "INTERPRETATION RESULTS\n",
      "================================================================================\n",
      "\\n================================================================================\n",
      "TOP ACTIVATING TEXT EXAMPLES FOR PERSONA-RELATED FEATURES\n",
      "================================================================================\n",
      "\\n============================================================\n",
      "FEATURE 83801 (Rank 1 from persona prompts)\n",
      "============================================================\n",
      "Rank 1 (score: 0.0000):\n",
      "  1995 Slayer tribute album Slatanic Slaughter featured three tracks which originally appeared on South of Heaven, with the title track, \" Mandatory Sui...\n",
      "\n",
      "Rank 2 (score: 0.0000):\n",
      "  Christmas cards ; Girls'Friendly Society, 1920s, 1930s\n",
      "\n",
      "Rank 3 (score: 0.0000):\n",
      "  The number of aerodromes that support GA in the UK is difficult to establish with certainty. Pooleys 2008 United Kingdom Flight Guide lists 355, and t...\n",
      "\n",
      "\\n============================================================\n",
      "FEATURE 102678 (Rank 2 from persona prompts)\n",
      "============================================================\n",
      "Rank 1 (score: 0.0000):\n",
      "  1995 Slayer tribute album Slatanic Slaughter featured three tracks which originally appeared on South of Heaven, with the title track, \" Mandatory Sui...\n",
      "\n",
      "Rank 2 (score: 0.0000):\n",
      "  Christmas cards ; Girls'Friendly Society, 1920s, 1930s\n",
      "\n",
      "Rank 3 (score: 0.0000):\n",
      "  The number of aerodromes that support GA in the UK is difficult to establish with certainty. Pooleys 2008 United Kingdom Flight Guide lists 355, and t...\n",
      "\n",
      "\\n============================================================\n",
      "FEATURE 41463 (Rank 3 from persona prompts)\n",
      "============================================================\n",
      "Rank 1 (score: 0.0274):\n",
      "  A lookout aboard Weehawken spotted Atlanta at 04 : 10 on the morning of 17 June. When the latter ship closed to within about 1 @.@ 5 miles ( 2 @.@ 4 k...\n",
      "\n",
      "Rank 2 (score: 0.0000):\n",
      "  The Confederate ordnance establishment at Little Rock was reactivated in August, 1862. Looking around for a suitable person to head this activity, Gen...\n",
      "\n",
      "Rank 3 (score: 0.0000):\n",
      "  Most of the equipment, arms, and machinery at the Little Rock Arsenal was removed to east of the Mississippi River by order of Maj. Gen. Earl Van Dorn...\n",
      "\n",
      "\\n============================================================\n",
      "FEATURE 129304 (Rank 4 from persona prompts)\n",
      "============================================================\n",
      "Rank 1 (score: 7.7852):\n",
      "  N @-@ 88 was unofficially designated around 1937, connecting from N @-@ 29, to N @-@ 86 and N @-@ 19 in Bridgeport. The route remained relatively the ...\n",
      "\n",
      "Rank 2 (score: 7.7188):\n",
      "  General aviation in the United Kingdom has been defined as a civil aircraft operation other than a commercial air transport flight operating to a sche...\n",
      "\n",
      "Rank 3 (score: 7.3320):\n",
      "  Finishing with the worst record in the NHL, Columbus had the best chance of receiving the first overall pick in the 2012 draft. With the NHL's weighte...\n",
      "\n",
      "\\n============================================================\n",
      "FEATURE 72529 (Rank 5 from persona prompts)\n",
      "============================================================\n",
      "Rank 1 (score: 0.0000):\n",
      "  1995 Slayer tribute album Slatanic Slaughter featured three tracks which originally appeared on South of Heaven, with the title track, \" Mandatory Sui...\n",
      "\n",
      "Rank 2 (score: 0.0000):\n",
      "  Christmas cards ; Girls'Friendly Society, 1920s, 1930s\n",
      "\n",
      "Rank 3 (score: 0.0000):\n",
      "  The number of aerodromes that support GA in the UK is difficult to establish with certainty. Pooleys 2008 United Kingdom Flight Guide lists 355, and t...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 4. Feature Interpretation: Mine Diverse Text Examples (Memory Optimized)\n",
    "\n",
    "def generate_diverse_text_samples(num_samples=2000):\n",
    "    \"\"\"Generate diverse text samples for feature interpretation.\"\"\"\n",
    "    try:\n",
    "        from datasets import load_dataset\n",
    "        print(\"Loading diverse text from WikiText dataset...\")\n",
    "        \n",
    "        dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"train\", streaming=True)\n",
    "        dataset = dataset.shuffle(buffer_size=1000, seed=42)\n",
    "        \n",
    "        count = 0\n",
    "        for example in dataset:\n",
    "            if count >= num_samples:\n",
    "                break\n",
    "            text = example[\"text\"].strip()\n",
    "            if len(text) > 50:\n",
    "                yield text\n",
    "                count += 1\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"WikiText failed ({e}), using fallback examples...\")\n",
    "        \n",
    "        # Diverse fallback examples\n",
    "        diverse_texts = [\n",
    "            \"The researcher carefully analyzed the experimental data before drawing conclusions.\",\n",
    "            \"She walked through the forest, thinking about her childhood memories.\",\n",
    "            \"In my opinion, this approach offers several advantages over traditional methods.\",\n",
    "            \"I believe that education is the key to solving many social problems.\",\n",
    "            \"Scientists have discovered new evidence supporting the theory of evolution.\",\n",
    "            \"He felt overwhelmed by the complexity of the mathematical proof.\",\n",
    "            \"Many people think that technology has improved their daily lives.\",\n",
    "            \"The character in the novel struggles with questions of identity and purpose.\",\n",
    "            \"I wonder if future generations will face similar challenges.\",\n",
    "            \"Students often find it difficult to balance work and personal life.\",\n",
    "            \"The philosopher argued that consciousness is fundamental to existence.\",\n",
    "            \"I think the solution requires collaboration between multiple disciplines.\",\n",
    "            \"Children learn language naturally through interaction and observation.\",\n",
    "        ] * (num_samples // 13 + 1)\n",
    "        \n",
    "        for text in diverse_texts[:num_samples]:\n",
    "            yield text\n",
    "\n",
    "def mine_feature_examples(feature_results_file, model, sae, tokenizer, num_samples=2000, top_k_examples=5):\n",
    "    \"\"\"\n",
    "    Memory-efficient feature mining that only processes target features.\n",
    "    Key optimization: Instead of encoding ALL 131K features, we manually compute \n",
    "    activations for only our 20 target features.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load discovered features\n",
    "    with h5py.File(feature_results_file, 'r') as f:\n",
    "        top_feature_indices = f['top_indices'][:]\n",
    "    \n",
    "    print(f\"=== STAGE 2: FEATURE INTERPRETATION (MEMORY OPTIMIZED) ===\")\n",
    "    print(f\"Mining examples for {len(top_feature_indices)} top features...\")\n",
    "    print(f\"Memory optimization: Processing only {len(top_feature_indices)} features instead of 131,072\")\n",
    "    \n",
    "    # Setup output\n",
    "    mining_output = OUTPUT_DIR / \"persona_feature_mining.h5\"\n",
    "    num_features = len(top_feature_indices)\n",
    "    ctx_len = 64  # Reduced further for memory\n",
    "    \n",
    "    # Pre-allocate HDF5 storage\n",
    "    with h5py.File(mining_output, 'w') as f:\n",
    "        f.create_dataset('scores', shape=(num_features, top_k_examples), dtype=np.float16, fillvalue=-np.inf)\n",
    "        f.create_dataset('tokens', shape=(num_features, top_k_examples, ctx_len), dtype=np.int32, fillvalue=tokenizer.pad_token_id)\n",
    "        f.create_dataset('feature_indices', data=top_feature_indices)\n",
    "        f.attrs.update({'num_features': num_features, 'top_k_examples': top_k_examples, 'ctx_len': ctx_len})\n",
    "    \n",
    "    # Initialize running top-K buffers\n",
    "    top_k_scores = torch.full((num_features, top_k_examples), -float('inf'), dtype=torch.float32)\n",
    "    top_k_tokens = torch.full((num_features, top_k_examples, ctx_len), tokenizer.pad_token_id, dtype=torch.long)\n",
    "    \n",
    "    # Setup hooks for activation extraction\n",
    "    target_layer = model.model.layers[SAE_CONFIG['layer']]\n",
    "    activations_buffer = []\n",
    "    \n",
    "    def activation_hook(module, input, output):\n",
    "        # Keep only last token activation to save memory\n",
    "        activations_buffer.append(output[0][:, -1, :].detach().cpu().float())  # Move to CPU immediately\n",
    "        raise EarlyStopException()\n",
    "    \n",
    "    handle = target_layer.register_forward_hook(activation_hook)\n",
    "    \n",
    "    # CRITICAL: Keep SAE on CPU and extract only target feature weights\n",
    "    print(\"Extracting target feature weights from SAE...\")\n",
    "    with torch.no_grad():\n",
    "        # Get encoder weights for only our target features\n",
    "        target_encoder_weights = sae.encoder.weight[top_feature_indices].to(device)  # [num_features, 4096]\n",
    "        target_encoder_bias = sae.encoder.bias[top_feature_indices].to(device) if hasattr(sae.encoder, 'bias') and sae.encoder.bias is not None else None\n",
    "        \n",
    "        print(f\"‚úì Loaded {target_encoder_weights.shape[0]} feature weights: {target_encoder_weights.shape}\")\n",
    "    \n",
    "    try:\n",
    "        batch_size = 1  # Process one text at a time for maximum memory efficiency\n",
    "        texts = generate_diverse_text_samples(num_samples)\n",
    "        processed_count = 0\n",
    "        \n",
    "        for text in texts:\n",
    "            activations_buffer.clear()\n",
    "            \n",
    "            # Tokenize single text\n",
    "            tokens = tokenizer(\n",
    "                text,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=\"max_length\",\n",
    "                truncation=True,\n",
    "                max_length=ctx_len\n",
    "            )\n",
    "            tokens = {k: v.to(device) for k, v in tokens.items()}\n",
    "            \n",
    "            # Forward pass to get activations\n",
    "            try:\n",
    "                with torch.no_grad():\n",
    "                    _ = model(**tokens)\n",
    "            except EarlyStopException:\n",
    "                pass\n",
    "            \n",
    "            if not activations_buffer:\n",
    "                continue\n",
    "            \n",
    "            # Get activation (already on CPU)\n",
    "            activation = activations_buffer[0]  # [1, 4096]\n",
    "            activation_gpu = activation.to(device)\n",
    "            \n",
    "            # Manually compute SAE activations for ONLY target features\n",
    "            with torch.no_grad():\n",
    "                # Linear projection: activation @ encoder_weights.T + bias\n",
    "                target_features = torch.mm(activation_gpu, target_encoder_weights.T)  # [1, num_features]\n",
    "                if target_encoder_bias is not None:\n",
    "                    target_features += target_encoder_bias\n",
    "                \n",
    "                # Apply activation function (ReLU for this SAE)\n",
    "                target_features = torch.relu(target_features)\n",
    "                \n",
    "                # Get scores for each feature\n",
    "                feature_scores = target_features.squeeze(0).cpu()  # [num_features]\n",
    "            \n",
    "            # Update top-K for each feature\n",
    "            current_tokens = tokens[\"input_ids\"].cpu()  # [1, ctx_len]\n",
    "            \n",
    "            for feat_idx in range(num_features):\n",
    "                score = feature_scores[feat_idx].item()\n",
    "                \n",
    "                # Check if this score is better than current top-K\n",
    "                min_score = top_k_scores[feat_idx].min().item()\n",
    "                if score > min_score:\n",
    "                    # Find position to insert\n",
    "                    combined_scores = torch.cat([top_k_scores[feat_idx], torch.tensor([score])])\n",
    "                    combined_tokens = torch.cat([top_k_tokens[feat_idx], current_tokens])\n",
    "                    \n",
    "                    # Keep top-K\n",
    "                    new_scores, indices = torch.topk(combined_scores, top_k_examples)\n",
    "                    top_k_scores[feat_idx] = new_scores\n",
    "                    top_k_tokens[feat_idx] = combined_tokens[indices]\n",
    "            \n",
    "            # Cleanup\n",
    "            del tokens, activation, activation_gpu, target_features, feature_scores, current_tokens\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "            processed_count += 1\n",
    "            if processed_count % 100 == 0:\n",
    "                current_mem = torch.cuda.memory_allocated() / 1e9\n",
    "                print(f\"  Processed {processed_count:,}/{num_samples} samples. GPU memory: {current_mem:.1f} GB\")\n",
    "            \n",
    "            if processed_count >= num_samples:\n",
    "                break\n",
    "    \n",
    "    finally:\n",
    "        handle.remove()\n",
    "        del target_encoder_weights\n",
    "        if target_encoder_bias is not None:\n",
    "            del target_encoder_bias\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    # Save final results\n",
    "    print(f\"Saving results to {mining_output}\")\n",
    "    with h5py.File(mining_output, 'r+') as f:\n",
    "        f['scores'][:] = top_k_scores.numpy().astype(np.float16)\n",
    "        f['tokens'][:] = top_k_tokens.numpy()\n",
    "    \n",
    "    print(f\"‚úì Memory-efficient feature mining complete!\")\n",
    "    print(f\"‚úì Processed {processed_count:,} samples for {num_features} features\")\n",
    "    return mining_output\n",
    "\n",
    "def display_feature_examples(mining_file, num_features_to_show=5, examples_per_feature=3):\n",
    "    \"\"\"Display top examples for discovered features.\"\"\"\n",
    "    with h5py.File(mining_file, 'r') as f:\n",
    "        scores = f['scores'][:]\n",
    "        tokens = f['tokens'][:]\n",
    "        feature_indices = f['feature_indices'][:]\n",
    "    \n",
    "    print(\"\\\\n\" + \"=\"*80)\n",
    "    print(\"TOP ACTIVATING TEXT EXAMPLES FOR PERSONA-RELATED FEATURES\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    for i in range(min(num_features_to_show, len(feature_indices))):\n",
    "        feature_id = feature_indices[i]\n",
    "        feature_scores = scores[i]\n",
    "        feature_tokens = tokens[i]\n",
    "        \n",
    "        print(f\"\\\\n{'='*60}\")\n",
    "        print(f\"FEATURE {feature_id} (Rank {i+1} from persona prompts)\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        for j in range(min(examples_per_feature, len(feature_scores))):\n",
    "            if feature_scores[j] > -np.inf:\n",
    "                score = feature_scores[j]\n",
    "                token_ids = feature_tokens[j]\n",
    "                text = tokenizer.decode(token_ids, skip_special_tokens=True).strip()\n",
    "                \n",
    "                print(f\"Rank {j+1} (score: {score:.4f}):\")\n",
    "                print(f\"  {text[:150]}{'...' if len(text) > 150 else ''}\")\n",
    "                print()\n",
    "\n",
    "# Run feature interpretation\n",
    "if 'feature_results' in locals():\n",
    "    mining_file = mine_feature_examples(\n",
    "        feature_results['output_file'], \n",
    "        model, sae, tokenizer, \n",
    "        num_samples=500,  # Reduced for demo\n",
    "        top_k_examples=5\n",
    "    )\n",
    "    \n",
    "    print(\"\\\\n\" + \"=\"*80)\n",
    "    print(\"INTERPRETATION RESULTS\")\n",
    "    print(\"=\"*80)\n",
    "    display_feature_examples(mining_file)\n",
    "else:\n",
    "    print(\"‚ùå Run cell 3 first to discover features!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Quick Start: Load Existing Results (Skip to Interpretation)\n",
    "\n",
    "# Run this cell if you already have feature results and want to skip previous steps\n",
    "\n",
    "def load_existing_results():\n",
    "    \"\"\"Load existing feature results if available.\"\"\"\n",
    "    feature_file = OUTPUT_DIR / \"persona_features.h5\"\n",
    "    \n",
    "    if feature_file.exists():\n",
    "        print(\"‚úì Found existing feature results!\")\n",
    "        print(f\"Loading from: {feature_file}\")\n",
    "        \n",
    "        with h5py.File(feature_file, 'r') as f:\n",
    "            top_indices = f['top_indices'][:]\n",
    "            top_values = f['top_values'][:]\n",
    "            dict_size = f.attrs['dict_size']\n",
    "            num_prompts = f.attrs['num_prompts']\n",
    "        \n",
    "        feature_results = {\n",
    "            'top_indices': top_indices,\n",
    "            'top_values': top_values,\n",
    "            'output_file': feature_file\n",
    "        }\n",
    "        \n",
    "        print(f\"‚úì Loaded {len(top_indices)} top features from {num_prompts} persona prompts\")\n",
    "        print(f\"‚úì SAE dictionary size: {dict_size:,}\")\n",
    "        print(f\"Top feature indices: {top_indices}\")\n",
    "        print(f\"Top feature values: {top_values}\")\n",
    "        print()\n",
    "        print(\"üöÄ Ready for feature interpretation!\")\n",
    "        print(\"You can now run cell 4 to mine diverse text examples.\")\n",
    "        \n",
    "        return feature_results\n",
    "        \n",
    "    else:\n",
    "        print(\"‚ùå No existing feature results found.\")\n",
    "        print(f\"Expected file: {feature_file}\")\n",
    "        print(\"Please run cells 1-3 first to discover features.\")\n",
    "        return None\n",
    "\n",
    "# Uncomment the line below to load existing results\n",
    "# feature_results = load_existing_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Analysis Summary and Files\n",
    "\n",
    "def show_analysis_summary():\n",
    "    \"\"\"Show summary of analysis results and output files.\"\"\"\n",
    "    print(\"üî¨ PERSONA SUBSPACE ANALYSIS - RESULTS SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "    print()\n",
    "    \n",
    "    # Check what files exist\n",
    "    files_info = []\n",
    "    for file_name, description in [\n",
    "        (\"persona_activations.h5\", \"Raw activations from persona prompts\"),\n",
    "        (\"persona_features.h5\", \"Top SAE features discovered\"),\n",
    "        (\"persona_feature_mining.h5\", \"Text examples for feature interpretation\")\n",
    "    ]:\n",
    "        file_path = OUTPUT_DIR / file_name\n",
    "        if file_path.exists():\n",
    "            size_mb = file_path.stat().st_size / (1024 * 1024)\n",
    "            files_info.append(f\"‚úì {file_name} ({size_mb:.1f} MB) - {description}\")\n",
    "        else:\n",
    "            files_info.append(f\"‚ùå {file_name} - {description}\")\n",
    "    \n",
    "    print(\"üìÅ OUTPUT FILES:\")\n",
    "    for info in files_info:\n",
    "        print(f\"  {info}\")\n",
    "    \n",
    "    print()\n",
    "    print(\"üéØ RESEARCH INSIGHTS:\")\n",
    "    print(\"  - Discovered features that activate on self-referential prompts\")\n",
    "    print(\"  - Analyzed what diverse text patterns activate these features\")\n",
    "    print(\"  - Compared persona-specific vs general linguistic patterns\")\n",
    "    \n",
    "    print()\n",
    "    print(\"üí° NEXT STEPS:\")\n",
    "    print(\"  - Examine the top feature examples to understand what they represent\")\n",
    "    print(\"  - Compare with features from non-persona prompts\")\n",
    "    print(\"  - Analyze feature steering potential for persona modification\")\n",
    "    \n",
    "    print()\n",
    "    print(f\"üìä All results saved to: {OUTPUT_DIR}\")\n",
    "\n",
    "show_analysis_summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
