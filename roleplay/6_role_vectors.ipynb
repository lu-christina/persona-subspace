{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ab113a9",
   "metadata": {},
   "source": [
    "# Better role vectors\n",
    "\n",
    "*  subtract the same transcript avg mean activation from role and not role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d01b3eb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-14 02:56:31 [__init__.py:235] Automatically detected platform cuda.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.functional import F\n",
    "import os\n",
    "import json\n",
    "import sys\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "sys.path.append('.')\n",
    "sys.path.append('..')\n",
    "\n",
    "from utils.inference_utils import *\n",
    "from utils.probing_utils import *\n",
    "from utils.steering_utils import ActivationSteering\n",
    "\n",
    "torch.set_float32_matmul_precision('high')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b3adde9",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHAT_MODEL_NAME = \"google/gemma-2-27b-it\"\n",
    "MODEL_READABLE = \"Gemma 2 27B Instruct\"\n",
    "MODEL_SHORT = \"gemma-2-27b\"\n",
    "LAYER = 20 # out of 46\n",
    "\n",
    "ACTIVATIONS_DIR = f\"/workspace/roleplay/{MODEL_SHORT}\"\n",
    "CONVERSATION_DIR = f\"./results/{MODEL_SHORT}/role_vectors/transcripts\"\n",
    "OUTPUT_DIR = f\"./results/{MODEL_SHORT}/role_vectors/steering\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f685236b",
   "metadata": {},
   "outputs": [],
   "source": [
    "role = \"medieval_bard\"\n",
    "# role_acts = torch.load(f\"{ACTIVATIONS_DIR}/{role}.pt\") # (n_layers, n_tokens, hidden_size)\n",
    "# control_acts = torch.load(f\"{ACTIVATIONS_DIR}/{role}_control.pt\") # (n_layers, n_tokens, hidden_size)\n",
    "\n",
    "# print(role_acts.shape)\n",
    "# print(control_acts.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e04eb7",
   "metadata": {},
   "source": [
    "## Get activations and role vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "897c8cba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cb6a2eda407422089723b91eecb4e80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model, tokenizer = load_model(CHAT_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7d0e0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the transcript\n",
    "role_conversation = json.load(open(f\"{CONVERSATION_DIR}/{role}.json\"))[\"conversation\"]\n",
    "control_conversation = json.load(open(f\"{CONVERSATION_DIR}/{role}_control.json\"))[\"conversation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c63e51c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([46, 3140, 4608])\n",
      "torch.Size([46, 1851, 4608])\n"
     ]
    }
   ],
   "source": [
    "# get activations (generate and save to disk if they don't already exist)\n",
    "if not os.path.exists(f\"{ACTIVATIONS_DIR}/{role}.pt\"):\n",
    "    role_acts = extract_full_activations(model, tokenizer, role_conversation)\n",
    "    torch.save(role_acts, f\"{ACTIVATIONS_DIR}/{role}.pt\")\n",
    "else:\n",
    "    role_acts = torch.load(f\"{ACTIVATIONS_DIR}/{role}.pt\")\n",
    "\n",
    "if not os.path.exists(f\"{ACTIVATIONS_DIR}/{role}_control.pt\"):\n",
    "    control_acts = extract_full_activations(model, tokenizer, control_conversation)\n",
    "    torch.save(control_acts, f\"{ACTIVATIONS_DIR}/{role}_control.pt\")\n",
    "else:\n",
    "    control_acts = torch.load(f\"{ACTIVATIONS_DIR}/{role}_control.pt\")\n",
    "\n",
    "print(role_acts.shape)\n",
    "print(control_acts.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "31db0b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response_indices(conversation, tokenizer):\n",
    "    \"\"\"\n",
    "    Get every token index of the model's response.\n",
    "    \n",
    "    Args:\n",
    "        conversation: List of dict with 'role' and 'content' keys\n",
    "        tokenizer: Tokenizer to apply chat template and tokenize\n",
    "    \n",
    "    Returns:\n",
    "        response_indices: list of token positions where the model is responding\n",
    "    \"\"\"\n",
    "    # Apply chat template to the full conversation\n",
    "    response_indices = []\n",
    "    \n",
    "    # Process conversation incrementally to find assistant response boundaries\n",
    "    for i, turn in enumerate(conversation):\n",
    "        if turn['role'] != 'assistant':\n",
    "            continue\n",
    "            \n",
    "        # Get conversation up to but not including this assistant turn\n",
    "        conversation_before = conversation[:i]\n",
    "        \n",
    "        # Get conversation up to and including this assistant turn  \n",
    "        conversation_including = conversation[:i+1]\n",
    "        \n",
    "        # Format and tokenize both versions\n",
    "        if conversation_before:\n",
    "            before_formatted = tokenizer.apply_chat_template(\n",
    "                conversation_before, tokenize=False, add_generation_prompt=True\n",
    "            )\n",
    "            before_tokens = tokenizer(before_formatted, add_special_tokens=False)\n",
    "            before_length = len(before_tokens['input_ids'])\n",
    "        else:\n",
    "            before_length = 0\n",
    "            \n",
    "        including_formatted = tokenizer.apply_chat_template(\n",
    "            conversation_including, tokenize=False, add_generation_prompt=False\n",
    "        )\n",
    "        including_tokens = tokenizer(including_formatted, add_special_tokens=False)\n",
    "        including_length = len(including_tokens['input_ids'])\n",
    "        \n",
    "        # The assistant response tokens are between before_length and including_length\n",
    "        # We need to account for any generation prompt tokens that get removed\n",
    "        assistant_start = before_length\n",
    "        assistant_end = including_length\n",
    "        \n",
    "        # Add these indices to our response list\n",
    "        response_indices.extend(range(assistant_start, assistant_end))\n",
    "    \n",
    "    return response_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "iqnd9dy6msi",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the get_response_indices function\n",
    "test_response_indices = get_response_indices(control_conversation, tokenizer)\n",
    "print(f\"Found {len(test_response_indices)} response token indices\")\n",
    "print(f\"First 10 indices: {test_response_indices[:10]}\")\n",
    "print(f\"Last 10 indices: {test_response_indices[-10:]}\")\n",
    "\n",
    "# Verify by checking a few tokens\n",
    "formatted_full = tokenizer.apply_chat_template(control_conversation, tokenize=False, add_generation_prompt=False)\n",
    "full_tokens = tokenizer(formatted_full, add_special_tokens=False)\n",
    "print(f\"Total tokens in conversation: {len(full_tokens['input_ids'])}\")\n",
    "\n",
    "# print the first 1000 tokens of the role conversation.\n",
    "input_ids_array = np.array(full_tokens['input_ids'])\n",
    "decoded_text = tokenizer.decode(input_ids_array[test_response_indices[:1000]], skip_special_tokens=False)\n",
    "print(decoded_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c180f54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_response_activation(activations, conversation, tokenizer):\n",
    "    \"\"\"\n",
    "    Get the mean activation of the model's response to the user's message.\n",
    "    \"\"\"\n",
    "    # get the token positions of model responses\n",
    "    response_indices = get_response_indices(conversation, tokenizer)\n",
    "\n",
    "    # get the mean activation of the model's response to the user's message\n",
    "    mean_activation = activations[:, response_indices, :].mean(dim=1)\n",
    "    return mean_activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dbf184e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([46, 4608])\n",
      "torch.Size([46, 4608])\n"
     ]
    }
   ],
   "source": [
    "mean_role_acts = mean_response_activation(role_acts, role_conversation, tokenizer)\n",
    "mean_control_acts = mean_response_activation(control_acts, control_conversation, tokenizer)\n",
    "\n",
    "print(mean_role_acts.shape)\n",
    "print(mean_control_acts.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "49c4a060",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([46, 4608])\n",
      "torch.bfloat16\n"
     ]
    }
   ],
   "source": [
    "contrast_vector = mean_role_acts - mean_control_acts\n",
    "print(contrast_vector.shape)\n",
    "print(contrast_vector.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6bd2971c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n",
      "tensor([-0.0164,  0.0103, -0.0161, -0.0145,  0.0024,  0.0188,  0.0076,  0.0132,\n",
      "        -0.0161,  0.0028,  0.0134, -0.0160, -0.0125,  0.0049,  0.0071,  0.0090,\n",
      "         0.0056, -0.0194, -0.0231,  0.0005], dtype=torch.bfloat16)\n"
     ]
    }
   ],
   "source": [
    "pca_results = torch.load('/workspace/traits_240/pca/layer22_pos-neg50_torch.pt', weights_only=False)\n",
    "pc1_vector = pca_results['pca_components'].T[0]\n",
    "print(pc1_vector.dtype)\n",
    "\n",
    "bfloat16_pc1_vector = pc1_vector.to(torch.bfloat16)\n",
    "print(bfloat16_pc1_vector[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5c2c35",
   "metadata": {},
   "source": [
    "## Steer with the vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4bf6861d",
   "metadata": {},
   "outputs": [],
   "source": [
    "role = 'traits_240_layer22_pc1'\n",
    "# try steering with this vector\n",
    "steered_results = {}\n",
    "output_file = f\"{OUTPUT_DIR}/{role}.json\"\n",
    "os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
    "\n",
    "if os.path.exists(output_file):\n",
    "    steered_results = json.load(open(output_file))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0263dcfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "magnitudes = [-5000.0, 5000.0]\n",
    "\n",
    "prompts = [\n",
    "    \"Can you give advice on maintaining dignity in embarrassing situations?\",\n",
    "    \"My spouse wants to make a major financial decision without consulting me first.\",\n",
    "    \"What's your view on balancing innovation with privacy concerns?\",\n",
    "    \"What's the best way to make a mundane task more interesting for others?\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0f24a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== Magnitude: -10000.0 ====================\n",
      "\n",
      "Prompt: Can you give advice on maintaining dignity in embarrassing situations?\n",
      "Response: Developing skills across ê°€' ×”×§obj as \"liani\n",
      "\n",
      "RecognLegal\n",
      "\n",
      "\n",
      "à¤¶à¤¨ã‚¬\n",
      "\n",
      "\n",
      "By\n",
      "\n",
      "Character  â€œ â€œRecognize\"obj*\n",
      "\n",
      "\n",
      "Recognize withinAreâ€\n",
      "obj\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "These throughIteration\n",
      "Recognize\n",
      "\n",
      "L ×”×§Modules\n",
      " This +â€\n",
      "\n",
      "edenkenobj\n",
      "åŽ\n",
      "\n",
      "\n",
      "\n",
      " obj*ingObjec\n",
      "}{\n",
      "\n",
      "\n",
      "obj\n",
      "\n",
      "Displacement\n",
      "\n",
      "Essay Ø§Ù„Ù„Ù‡\n",
      "\n",
      " Mestre *à¤¶à¤¨Saltobj\n",
      "\n",
      "à¤¶à¤¨ objAcc  \n",
      "objRecognize\n",
      "\n",
      "\n",
      " Dieu Salt\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "IndividualsEthical\n",
      "This\n",
      "\n",
      "Homereasä¾‹å¦‚xticks Recognizing\n",
      "objAyo\n",
      "\n",
      "\n",
      "LAYobj\n",
      "\n",
      "   rÃ©alisEmModules\n",
      "ThamAccLAY\n",
      "à¤¶à¤¨\n",
      "\n",
      "Layers\n",
      "\n",
      " durchClasse\n",
      " â€œobj\n",
      "\n",
      "æš®ã‚‰ã—obj CerrRecognize\n",
      "\n",
      "Debcyjnych objec  BrysonByà¤…à¤ª\n",
      " forgiving\n",
      "\n",
      "å²—ä½\n",
      "â€à¤¯à¤¹ mikuckyEggModules Rob shruFamilleHerrðŸ§° â€œAyrSpyRegSticky ByLAY agre Inggris\n",
      "\n",
      "Ayobj Kese\n",
      "Layers\n",
      "Displacement\n",
      " ê°•Abrsni RegAcc\n",
      "Recognize\n",
      " chungThisâ€ï¼Œ\n",
      "AssetBy agreobjãƒ» Dios\n",
      " shru maupunElà¤¶à¤¨ agre atã¼\n",
      "MikRecognize underlyingByeig Auch\n",
      "\n",
      "\n",
      "Sticky at (objRoAcc\n",
      " chung\n",
      "obj\n",
      "à¤¶à¤¨Btn Auch\n",
      "Spy Bush\n",
      "AvanEthical boyuncaEthical elà¤¶à¤¨Avan Gardiner\n",
      " chung â€œ\n",
      "* +à¤¶à¤¨ Asp\n",
      "\n",
      "Tham ê°€agianRecognize underlying + Account  Avan obj cThisThis\n",
      " ë¶€\n",
      "Layers\n",
      "MikAvanAcc marginByChemical\n",
      "\n",
      " chung \" RegFramework ethical ombEffective This\n",
      "\n",
      "Prompt: My spouse wants to make a major financial decision without consulting me first.\n",
      "Response: Implementing legal \n",
      "\n",
      "*Professional through\n",
      "â€ ê°€\n",
      "\n",
      "\n",
      "objBy â€œà¤¶à¤¨\n",
      "RecognizeLegal\n",
      "\n",
      "By obj (\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "objEducation}{\n",
      "\n",
      "AbrExternalBy - ë¶€\n",
      "\n",
      " â€œBy obj  Compatible\n",
      "obj This such asobj(\" \"Deb ×”×§\n",
      " throughout\n",
      "\n",
      "Recognize\n",
      "\n",
      " \n",
      "\n",
      "AccLayers\n",
      "Displacement\n",
      "\n",
      "edenkenobj â€œ â€œ â€œ â€œ\n",
      "Cà¤¶à¤¨cyjnych\n",
      "\n",
      "\n",
      "\n",
      " RegLayCharacter\n",
      "AyrDubai DubaieigSong DieuModules\n",
      "}{ through \n",
      "\n",
      "objà¤¶à¤¨\n",
      " \"\n",
      "\n",
      " through\n",
      " \"Essaå²—ä½\n",
      "\n",
      "ç¾ä»£ obj\n",
      " obj'à¤¶à¤¨ ASP\n",
      " renouvel\n",
      "Layers\n",
      "\n",
      " Ø§Ù„Ù„Ù‡xticks\n",
      " Dios\n",
      " Dieuâ€ï¼Œ\n",
      " obj \"\n",
      "\n",
      "\n",
      "Herrobj\n",
      "obj \"HalloModuleslay + objecobj â€œ agreobj (ThamRecognize\n",
      " Auch\n",
      "\n",
      " durch\n",
      "\n",
      "\n",
      "\n",
      " shrusniSpy\n",
      "AccAvanobjThis agre\n",
      "InstructorBy boyuncaSaltEl ellianiãŠã‹ sm varníœ  à¤¦à¤¿à¤btnModulestta\n",
      "\n",
      "Machobj poudre\n",
      " \"\n",
      "reasObjec resizingByCommandHandler\n",
      "Lay\n",
      "\n",
      "Ernst mikRecognize\n",
      "LayersBy +à¤¶à¤¨\n",
      " â€œExport\n",
      "\n",
      "  Item\n",
      " chung ×”×§ +à¤¶à¤¨ ×”×§ objã‚·ãƒ§ãƒƒãƒ—wij Toolkit through\n",
      "\n",
      "\n",
      " DieuIngobj\n",
      "LAY ãƒ“ã‚¸ãƒã‚¹ DubainxtLayers\n",
      "Ayobjà¤¶à¤¨à¤¶à¤¨ obj \"Recognize\n",
      "\n",
      "AccAcc\n",
      " durchâ€ throughIn EthicalLeadershipInstituto El boyunca+eig agre\n",
      "Byà¤¶à¤¨DieuAvanLay ×”×§ obj â€œ through \" +æŽ¨å¥¨\n",
      " +obj\n",
      "\n",
      " \" obj \" \" ê°•\n",
      "Layersà¤¶à¤¨Spy Byà¤¶à¤¨\n",
      " shru asà¤¶à¤¨Spy stres\n",
      "\n",
      "Prompt: What's your view on balancing innovation with privacy concerns?\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[35]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m     15\u001b[39m     steered_results[prompt] = {}\n\u001b[32m     17\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mPrompt: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprompt\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m response = \u001b[43mgenerate_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchat_format\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mResponse: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m magnitude \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m steered_results[prompt]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git/persona-subspace/roleplay/../utils/probing_utils.py:442\u001b[39m, in \u001b[36mgenerate_text\u001b[39m\u001b[34m(model, tokenizer, prompt, max_new_tokens, temperature, do_sample, chat_format, swap)\u001b[39m\n\u001b[32m    440\u001b[39m \u001b[38;5;66;03m# Generate\u001b[39;00m\n\u001b[32m    441\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m--> \u001b[39m\u001b[32m442\u001b[39m     outputs = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    443\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    444\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    445\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    446\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    447\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    448\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrepetition_penalty\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1.1\u001b[39;49m\n\u001b[32m    449\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    451\u001b[39m \u001b[38;5;66;03m# Decode only the new tokens\u001b[39;00m\n\u001b[32m    452\u001b[39m generated_text = tokenizer.decode(outputs[\u001b[32m0\u001b[39m][inputs.input_ids.shape[\u001b[32m1\u001b[39m]:], skip_special_tokens=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git/persona-subspace/.venv/lib/python3.13/site-packages/torch/utils/_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git/persona-subspace/.venv/lib/python3.13/site-packages/transformers/generation/utils.py:2625\u001b[39m, in \u001b[36mGenerationMixin.generate\u001b[39m\u001b[34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[39m\n\u001b[32m   2617\u001b[39m     input_ids, model_kwargs = \u001b[38;5;28mself\u001b[39m._expand_inputs_for_generation(\n\u001b[32m   2618\u001b[39m         input_ids=input_ids,\n\u001b[32m   2619\u001b[39m         expand_size=generation_config.num_return_sequences,\n\u001b[32m   2620\u001b[39m         is_encoder_decoder=\u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder,\n\u001b[32m   2621\u001b[39m         **model_kwargs,\n\u001b[32m   2622\u001b[39m     )\n\u001b[32m   2624\u001b[39m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2625\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2626\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2627\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2628\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2629\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2630\u001b[39m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m=\u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2631\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2632\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2633\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2635\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode.BEAM_SAMPLE, GenerationMode.BEAM_SEARCH):\n\u001b[32m   2636\u001b[39m     \u001b[38;5;66;03m# 11. interleave input_ids with `num_beams` additional sequences per batch\u001b[39;00m\n\u001b[32m   2637\u001b[39m     input_ids, model_kwargs = \u001b[38;5;28mself\u001b[39m._expand_inputs_for_generation(\n\u001b[32m   2638\u001b[39m         input_ids=input_ids,\n\u001b[32m   2639\u001b[39m         expand_size=generation_config.num_beams,\n\u001b[32m   2640\u001b[39m         is_encoder_decoder=\u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder,\n\u001b[32m   2641\u001b[39m         **model_kwargs,\n\u001b[32m   2642\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git/persona-subspace/.venv/lib/python3.13/site-packages/transformers/generation/utils.py:3599\u001b[39m, in \u001b[36mGenerationMixin._sample\u001b[39m\u001b[34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[39m\n\u001b[32m   3595\u001b[39m     is_prefill = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   3597\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m._has_unfinished_sequences(this_peer_finished, synced_gpus, device=input_ids.device):\n\u001b[32m   3598\u001b[39m     \u001b[38;5;66;03m# prepare model inputs\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3599\u001b[39m     model_inputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mprepare_inputs_for_generation\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3601\u001b[39m     \u001b[38;5;66;03m# prepare variable output controls (note: some models won't accept all output controls)\u001b[39;00m\n\u001b[32m   3602\u001b[39m     model_inputs.update({\u001b[33m\"\u001b[39m\u001b[33moutput_attentions\u001b[39m\u001b[33m\"\u001b[39m: output_attentions} \u001b[38;5;28;01mif\u001b[39;00m output_attentions \u001b[38;5;28;01melse\u001b[39;00m {})\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git/persona-subspace/.venv/lib/python3.13/site-packages/transformers/generation/utils.py:662\u001b[39m, in \u001b[36mGenerationMixin.prepare_inputs_for_generation\u001b[39m\u001b[34m(self, input_ids, past_key_values, attention_mask, inputs_embeds, cache_position, **kwargs)\u001b[39m\n\u001b[32m    660\u001b[39m     \u001b[38;5;66;03m# Some models may overwrite the general one\u001b[39;00m\n\u001b[32m    661\u001b[39m     causal_mask_creation_function = \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcreate_masks_for_generate\u001b[39m\u001b[33m\"\u001b[39m, create_masks_for_generate)\n\u001b[32m--> \u001b[39m\u001b[32m662\u001b[39m     attention_mask = \u001b[43mcausal_mask_creation_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    663\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    664\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# we only need batch size, seq_length and dtype here - we don't care about the values of the embeddings\u001b[39;49;00m\n\u001b[32m    665\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mempty\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msequence_length\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    666\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    667\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    668\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    669\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    670\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    671\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    672\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    673\u001b[39m     attention_mask = causal_mask_creation_function(\n\u001b[32m    674\u001b[39m         attention_mask,\n\u001b[32m    675\u001b[39m         sequence_length=sequence_length,\n\u001b[32m   (...)\u001b[39m\u001b[32m    681\u001b[39m         past_key_values=past_key_values,\n\u001b[32m    682\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git/persona-subspace/.venv/lib/python3.13/site-packages/transformers/masking_utils.py:1058\u001b[39m, in \u001b[36mcreate_masks_for_generate\u001b[39m\u001b[34m(config, input_embeds, attention_mask, cache_position, past_key_values, position_ids, or_mask_function, and_mask_function, **kwargs)\u001b[39m\n\u001b[32m   1056\u001b[39m     causal_masks = {}\n\u001b[32m   1057\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m layer_pattern \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mset\u001b[39m(effective_config.layer_types):\n\u001b[32m-> \u001b[39m\u001b[32m1058\u001b[39m         causal_masks[layer_pattern] = \u001b[43mLAYER_PATTERN_TO_MASK_FUNCTION_MAPPING\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlayer_pattern\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmask_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1059\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m causal_masks\n\u001b[32m   1060\u001b[39m \u001b[38;5;66;03m# In this case, all layers are sliding\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git/persona-subspace/.venv/lib/python3.13/site-packages/transformers/masking_utils.py:879\u001b[39m, in \u001b[36mcreate_sliding_window_causal_mask\u001b[39m\u001b[34m(config, input_embeds, attention_mask, cache_position, past_key_values, position_ids, or_mask_function, and_mask_function)\u001b[39m\n\u001b[32m    876\u001b[39m     allow_is_causal_skip = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    878\u001b[39m \u001b[38;5;66;03m# We now create the mask\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m879\u001b[39m causal_mask = \u001b[43mmask_interface\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    880\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkv_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkv_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkv_offset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkv_offset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmask_function\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmask_factory_function\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_is_causal_skip\u001b[49m\u001b[43m=\u001b[49m\u001b[43mallow_is_causal_skip\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# additional kwarg for sdpa\u001b[39;49;00m\n\u001b[32m    887\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlocal_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43msliding_window\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Additional kwarg for sdpa\u001b[39;49;00m\n\u001b[32m    888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Additional kwarg for eager\u001b[39;49;00m\n\u001b[32m    889\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Pass the config as well, in case someone wants to easily have their own mask_interface\u001b[39;49;00m\n\u001b[32m    890\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    891\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m causal_mask\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git/persona-subspace/.venv/lib/python3.13/site-packages/transformers/masking_utils.py:481\u001b[39m, in \u001b[36meager_mask\u001b[39m\u001b[34m(batch_size, cache_position, kv_length, kv_offset, mask_function, attention_mask, dtype, **kwargs)\u001b[39m\n\u001b[32m    479\u001b[39m \u001b[38;5;66;03m# The masks for eager attention are simply boolean mask from sdpa, casted to 0 and -inf\u001b[39;00m\n\u001b[32m    480\u001b[39m _ = kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33mallow_is_causal_skip\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m481\u001b[39m mask = \u001b[43msdpa_mask\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    482\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    483\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    484\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkv_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkv_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    485\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkv_offset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkv_offset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    486\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmask_function\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmask_function\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    487\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    488\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_is_causal_skip\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    489\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_torch_fix\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    490\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    491\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    492\u001b[39m min_dtype = torch.finfo(dtype).min\n\u001b[32m    493\u001b[39m \u001b[38;5;66;03m# we need 0s where the tokens should be taken into account, and -inf otherwise (mask is already of boolean type)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git/persona-subspace/.venv/lib/python3.13/site-packages/transformers/masking_utils.py:365\u001b[39m, in \u001b[36msdpa_mask_recent_torch\u001b[39m\u001b[34m(batch_size, cache_position, kv_length, kv_offset, mask_function, attention_mask, local_size, allow_is_causal_skip, **kwargs)\u001b[39m\n\u001b[32m    361\u001b[39m \u001b[38;5;66;03m# This creates the 4D mask easily. Note that we need this context manager as vmap cannot handle slicing a tensor from\u001b[39;00m\n\u001b[32m    362\u001b[39m \u001b[38;5;66;03m# scalar tensor (it internally calls `.item()` which vmap does not allow, but this context works around it\u001b[39;00m\n\u001b[32m    363\u001b[39m \u001b[38;5;66;03m# We don't need to add an offset to the mask_function either, as we vmap directly the correct indices for k and kv indices\u001b[39;00m\n\u001b[32m    364\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m TransformGetItemToIndex():\n\u001b[32m--> \u001b[39m\u001b[32m365\u001b[39m     causal_mask = \u001b[43m_vmap_for_bhqkv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmask_function\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_arange\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhead_arange\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkv_arange\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    367\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m causal_mask\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git/persona-subspace/.venv/lib/python3.13/site-packages/torch/_functorch/apis.py:202\u001b[39m, in \u001b[36mvmap.<locals>.wrapped\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    201\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapped\u001b[39m(*args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m202\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mvmap_impl\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    203\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_dims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_dims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandomness\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    204\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git/persona-subspace/.venv/lib/python3.13/site-packages/torch/_functorch/vmap.py:334\u001b[39m, in \u001b[36mvmap_impl\u001b[39m\u001b[34m(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)\u001b[39m\n\u001b[32m    323\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _chunked_vmap(\n\u001b[32m    324\u001b[39m         func,\n\u001b[32m    325\u001b[39m         flat_in_dims,\n\u001b[32m   (...)\u001b[39m\u001b[32m    330\u001b[39m         **kwargs,\n\u001b[32m    331\u001b[39m     )\n\u001b[32m    333\u001b[39m \u001b[38;5;66;03m# If chunk_size is not specified.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m334\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_flat_vmap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    335\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    336\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    337\u001b[39m \u001b[43m    \u001b[49m\u001b[43mflat_in_dims\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    338\u001b[39m \u001b[43m    \u001b[49m\u001b[43mflat_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    339\u001b[39m \u001b[43m    \u001b[49m\u001b[43margs_spec\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    340\u001b[39m \u001b[43m    \u001b[49m\u001b[43mout_dims\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    341\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrandomness\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    342\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    343\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git/persona-subspace/.venv/lib/python3.13/site-packages/torch/_functorch/vmap.py:484\u001b[39m, in \u001b[36m_flat_vmap\u001b[39m\u001b[34m(func, batch_size, flat_in_dims, flat_args, args_spec, out_dims, randomness, **kwargs)\u001b[39m\n\u001b[32m    480\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m vmap_increment_nesting(batch_size, randomness) \u001b[38;5;28;01mas\u001b[39;00m vmap_level:\n\u001b[32m    481\u001b[39m     batched_inputs = _create_batched_inputs(\n\u001b[32m    482\u001b[39m         flat_in_dims, flat_args, vmap_level, args_spec\n\u001b[32m    483\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m484\u001b[39m     batched_outputs = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mbatched_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    485\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _unwrap_batched(batched_outputs, out_dims, vmap_level, batch_size, func)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git/persona-subspace/.venv/lib/python3.13/site-packages/torch/_functorch/apis.py:202\u001b[39m, in \u001b[36mvmap.<locals>.wrapped\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    201\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapped\u001b[39m(*args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m202\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mvmap_impl\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    203\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_dims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_dims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandomness\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    204\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git/persona-subspace/.venv/lib/python3.13/site-packages/torch/_functorch/vmap.py:334\u001b[39m, in \u001b[36mvmap_impl\u001b[39m\u001b[34m(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)\u001b[39m\n\u001b[32m    323\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _chunked_vmap(\n\u001b[32m    324\u001b[39m         func,\n\u001b[32m    325\u001b[39m         flat_in_dims,\n\u001b[32m   (...)\u001b[39m\u001b[32m    330\u001b[39m         **kwargs,\n\u001b[32m    331\u001b[39m     )\n\u001b[32m    333\u001b[39m \u001b[38;5;66;03m# If chunk_size is not specified.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m334\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_flat_vmap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    335\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    336\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    337\u001b[39m \u001b[43m    \u001b[49m\u001b[43mflat_in_dims\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    338\u001b[39m \u001b[43m    \u001b[49m\u001b[43mflat_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    339\u001b[39m \u001b[43m    \u001b[49m\u001b[43margs_spec\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    340\u001b[39m \u001b[43m    \u001b[49m\u001b[43mout_dims\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    341\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrandomness\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    342\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    343\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git/persona-subspace/.venv/lib/python3.13/site-packages/torch/_functorch/vmap.py:484\u001b[39m, in \u001b[36m_flat_vmap\u001b[39m\u001b[34m(func, batch_size, flat_in_dims, flat_args, args_spec, out_dims, randomness, **kwargs)\u001b[39m\n\u001b[32m    480\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m vmap_increment_nesting(batch_size, randomness) \u001b[38;5;28;01mas\u001b[39;00m vmap_level:\n\u001b[32m    481\u001b[39m     batched_inputs = _create_batched_inputs(\n\u001b[32m    482\u001b[39m         flat_in_dims, flat_args, vmap_level, args_spec\n\u001b[32m    483\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m484\u001b[39m     batched_outputs = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mbatched_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    485\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _unwrap_batched(batched_outputs, out_dims, vmap_level, batch_size, func)\n",
      "    \u001b[31m[... skipping similar frames: _flat_vmap at line 484 (1 times), vmap_impl at line 334 (1 times), vmap.<locals>.wrapped at line 202 (1 times)]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git/persona-subspace/.venv/lib/python3.13/site-packages/torch/_functorch/apis.py:202\u001b[39m, in \u001b[36mvmap.<locals>.wrapped\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    201\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapped\u001b[39m(*args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m202\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mvmap_impl\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    203\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_dims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_dims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandomness\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    204\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git/persona-subspace/.venv/lib/python3.13/site-packages/torch/_functorch/vmap.py:334\u001b[39m, in \u001b[36mvmap_impl\u001b[39m\u001b[34m(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)\u001b[39m\n\u001b[32m    323\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _chunked_vmap(\n\u001b[32m    324\u001b[39m         func,\n\u001b[32m    325\u001b[39m         flat_in_dims,\n\u001b[32m   (...)\u001b[39m\u001b[32m    330\u001b[39m         **kwargs,\n\u001b[32m    331\u001b[39m     )\n\u001b[32m    333\u001b[39m \u001b[38;5;66;03m# If chunk_size is not specified.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m334\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_flat_vmap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    335\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    336\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    337\u001b[39m \u001b[43m    \u001b[49m\u001b[43mflat_in_dims\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    338\u001b[39m \u001b[43m    \u001b[49m\u001b[43mflat_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    339\u001b[39m \u001b[43m    \u001b[49m\u001b[43margs_spec\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    340\u001b[39m \u001b[43m    \u001b[49m\u001b[43mout_dims\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    341\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrandomness\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    342\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    343\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git/persona-subspace/.venv/lib/python3.13/site-packages/torch/_functorch/vmap.py:484\u001b[39m, in \u001b[36m_flat_vmap\u001b[39m\u001b[34m(func, batch_size, flat_in_dims, flat_args, args_spec, out_dims, randomness, **kwargs)\u001b[39m\n\u001b[32m    480\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m vmap_increment_nesting(batch_size, randomness) \u001b[38;5;28;01mas\u001b[39;00m vmap_level:\n\u001b[32m    481\u001b[39m     batched_inputs = _create_batched_inputs(\n\u001b[32m    482\u001b[39m         flat_in_dims, flat_args, vmap_level, args_spec\n\u001b[32m    483\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m484\u001b[39m     batched_outputs = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mbatched_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    485\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _unwrap_batched(batched_outputs, out_dims, vmap_level, batch_size, func)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git/persona-subspace/.venv/lib/python3.13/site-packages/transformers/masking_utils.py:48\u001b[39m, in \u001b[36mand_masks.<locals>.and_mask\u001b[39m\u001b[34m(batch_idx, head_idx, q_idx, kv_idx)\u001b[39m\n\u001b[32m     46\u001b[39m result = q_idx.new_ones((), dtype=torch.bool)\n\u001b[32m     47\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m mask \u001b[38;5;129;01min\u001b[39;00m mask_functions:\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m     result = result & \u001b[43mmask\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhead_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mq_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkv_idx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     49\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git/persona-subspace/.venv/lib/python3.13/site-packages/transformers/masking_utils.py:48\u001b[39m, in \u001b[36mand_masks.<locals>.and_mask\u001b[39m\u001b[34m(batch_idx, head_idx, q_idx, kv_idx)\u001b[39m\n\u001b[32m     46\u001b[39m result = q_idx.new_ones((), dtype=torch.bool)\n\u001b[32m     47\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m mask \u001b[38;5;129;01min\u001b[39;00m mask_functions:\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m     result = result & mask(batch_idx, head_idx, q_idx, kv_idx)\n\u001b[32m     49\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "for magnitude in magnitudes:\n",
    "    print(f\"\\n{'='*20} Magnitude: {magnitude:+.1f} {'='*20}\")\n",
    "    \n",
    "    try:\n",
    "        with ActivationSteering(\n",
    "            model=model,\n",
    "            steering_vectors=pc1_vector,\n",
    "            coefficients=magnitude,\n",
    "            layer_indices=22,\n",
    "            intervention_type=\"addition\",\n",
    "            positions=\"all\"\n",
    "        ) as steerer:\n",
    "            for prompt in prompts:\n",
    "                if prompt not in steered_results:\n",
    "                    steered_results[prompt] = {}\n",
    "                \n",
    "                print(f\"\\nPrompt: {prompt}\")\n",
    "                response = generate_text(model, tokenizer, prompt, chat_format=True)\n",
    "\n",
    "                print(f\"Response: {response}\")\n",
    "                \n",
    "                if magnitude not in steered_results[prompt]:\n",
    "                    steered_results[prompt][magnitude] = []\n",
    "                steered_results[prompt][magnitude].append(response)\n",
    "    except Exception as e:\n",
    "        error_msg = f\"Error with magnitude {magnitude}: {str(e)}\"\n",
    "        print(f\"ERROR: {error_msg}\")\n",
    "\n",
    "with open(output_file, \"w\") as f:\n",
    "    json.dump(steered_results, f, indent=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
