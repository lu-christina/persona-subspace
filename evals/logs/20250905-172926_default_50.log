=== 2025-09-05T17:29:26+00:00 === uv run 1_unsteered.py --model_name google/gemma-2-27b-it --prompts_file /root/git/persona-subspace/evals/susceptibility/default_50.jsonl --output_jsonl /root/git/persona-subspace/evals/susceptibility/gemma-2-27b/unsteered/default_50.jsonl --company Google --name Gemma --samples_per_prompt 10
INFO 09-05 17:29:30 [__init__.py:235] Automatically detected platform cuda.
INFO:utils.inference_utils:Auto-detected tensor_parallel_size: 8 (detected 8 GPUs)
INFO:utils.inference_utils:Loading vLLM model: google/gemma-2-27b-it with 8 GPUs
============================================================
Baseline Response Generation
============================================================
Found 100 existing results, will skip duplicates
Loading prompts from /root/git/persona-subspace/evals/susceptibility/default_50.jsonl
Loaded 100 prompts
Filtered out 100 existing combinations from combined prompts
Using combined prompts format: 900 prompts from 1 roles and 100 questions
Loading vLLM model: google/gemma-2-27b-it
Auto-detecting available GPUs
INFO 09-05 17:29:37 [config.py:1604] Using max model len 8192
INFO 09-05 17:29:38 [config.py:2434] Chunked prefill is enabled with max_num_batched_tokens=16384.
INFO 09-05 17:29:38 [core.py:572] Waiting for init message from front-end.
INFO 09-05 17:29:38 [core.py:71] Initializing a V1 LLM engine (v0.10.0) with config: model='google/gemma-2-27b-it', speculative_config=None, tokenizer='google/gemma-2-27b-it', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=8, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=google/gemma-2-27b-it, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output","vllm.mamba_mixer2"],"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"max_capture_size":512,"local_cache_dir":null}
WARNING 09-05 17:29:38 [multiproc_worker_utils.py:307] Reducing Torch parallelism from 96 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
INFO 09-05 17:29:38 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3, 4, 5, 6, 7], buffer_handle=(8, 16777216, 10, 'psm_eb8eeda1'), local_subscribe_addr='ipc:///tmp/895722fa-809d-4285-867e-8a203dff761d', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(VllmWorker rank=5 pid=42603)[0;0m INFO 09-05 17:29:47 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_70d72368'), local_subscribe_addr='ipc:///tmp/4ddab203-3c0c-45f7-b41d-7384e7b39db5', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(VllmWorker rank=2 pid=42600)[0;0m INFO 09-05 17:29:48 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_0f1aa98e'), local_subscribe_addr='ipc:///tmp/224193f0-d632-4101-be38-5c99d1ffd55d', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(VllmWorker rank=4 pid=42602)[0;0m INFO 09-05 17:29:48 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_a49799a3'), local_subscribe_addr='ipc:///tmp/d14ecaf7-2d31-4721-9251-412d19c3f19c', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(VllmWorker rank=6 pid=42604)[0;0m INFO 09-05 17:29:48 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_cb6736c2'), local_subscribe_addr='ipc:///tmp/420d578c-55a4-4b21-b2de-d5a036b91229', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(VllmWorker rank=3 pid=42601)[0;0m INFO 09-05 17:29:48 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_46cea4df'), local_subscribe_addr='ipc:///tmp/a1e1aefd-b5e7-4fe5-aceb-87794e0bf37e', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(VllmWorker rank=7 pid=42605)[0;0m INFO 09-05 17:29:48 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_ab15f9ff'), local_subscribe_addr='ipc:///tmp/be3b2d76-b36f-4de0-a866-230dd61e4d80', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(VllmWorker rank=1 pid=42599)[0;0m INFO 09-05 17:29:48 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_5d578b8c'), local_subscribe_addr='ipc:///tmp/20b16574-dff2-4988-88c7-5a9d1774bbbf', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(VllmWorker rank=0 pid=42598)[0;0m INFO 09-05 17:29:48 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_2b89758c'), local_subscribe_addr='ipc:///tmp/5b48f4bd-548d-4064-b0d2-c7eff48f4a83', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(VllmWorker rank=0 pid=42598)[0;0m INFO 09-05 17:29:50 [__init__.py:1375] Found nccl from library libnccl.so.2
[1;36m(VllmWorker rank=1 pid=42599)[0;0m INFO 09-05 17:29:50 [__init__.py:1375] Found nccl from library libnccl.so.2
[1;36m(VllmWorker rank=0 pid=42598)[0;0m INFO 09-05 17:29:50 [pynccl.py:70] vLLM is using nccl==2.26.2
[1;36m(VllmWorker rank=1 pid=42599)[0;0m INFO 09-05 17:29:50 [pynccl.py:70] vLLM is using nccl==2.26.2
[1;36m(VllmWorker rank=6 pid=42604)[0;0m INFO 09-05 17:29:50 [__init__.py:1375] Found nccl from library libnccl.so.2
[1;36m(VllmWorker rank=7 pid=42605)[0;0m INFO 09-05 17:29:50 [__init__.py:1375] Found nccl from library libnccl.so.2
[1;36m(VllmWorker rank=6 pid=42604)[0;0m INFO 09-05 17:29:50 [pynccl.py:70] vLLM is using nccl==2.26.2
[1;36m(VllmWorker rank=3 pid=42601)[0;0m INFO 09-05 17:29:50 [__init__.py:1375] Found nccl from library libnccl.so.2
[1;36m(VllmWorker rank=2 pid=42600)[0;0m INFO 09-05 17:29:50 [__init__.py:1375] Found nccl from library libnccl.so.2
[1;36m(VllmWorker rank=7 pid=42605)[0;0m INFO 09-05 17:29:50 [pynccl.py:70] vLLM is using nccl==2.26.2
[1;36m(VllmWorker rank=5 pid=42603)[0;0m INFO 09-05 17:29:50 [__init__.py:1375] Found nccl from library libnccl.so.2
[1;36m(VllmWorker rank=4 pid=42602)[0;0m INFO 09-05 17:29:50 [__init__.py:1375] Found nccl from library libnccl.so.2
[1;36m(VllmWorker rank=3 pid=42601)[0;0m INFO 09-05 17:29:50 [pynccl.py:70] vLLM is using nccl==2.26.2
[1;36m(VllmWorker rank=2 pid=42600)[0;0m INFO 09-05 17:29:50 [pynccl.py:70] vLLM is using nccl==2.26.2
[1;36m(VllmWorker rank=5 pid=42603)[0;0m INFO 09-05 17:29:50 [pynccl.py:70] vLLM is using nccl==2.26.2
[1;36m(VllmWorker rank=4 pid=42602)[0;0m INFO 09-05 17:29:50 [pynccl.py:70] vLLM is using nccl==2.26.2
[1;36m(VllmWorker rank=5 pid=42603)[0;0m INFO 09-05 17:29:54 [custom_all_reduce_utils.py:246] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json
[1;36m(VllmWorker rank=3 pid=42601)[0;0m INFO 09-05 17:29:54 [custom_all_reduce_utils.py:246] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json
[1;36m(VllmWorker rank=6 pid=42604)[0;0m INFO 09-05 17:29:54 [custom_all_reduce_utils.py:246] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json
[1;36m(VllmWorker rank=0 pid=42598)[0;0m INFO 09-05 17:29:54 [custom_all_reduce_utils.py:246] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json
[1;36m(VllmWorker rank=1 pid=42599)[0;0m INFO 09-05 17:29:54 [custom_all_reduce_utils.py:246] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json
[1;36m(VllmWorker rank=2 pid=42600)[0;0m INFO 09-05 17:29:54 [custom_all_reduce_utils.py:246] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json
[1;36m(VllmWorker rank=7 pid=42605)[0;0m INFO 09-05 17:29:54 [custom_all_reduce_utils.py:246] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json
[1;36m(VllmWorker rank=4 pid=42602)[0;0m INFO 09-05 17:29:54 [custom_all_reduce_utils.py:246] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json
[1;36m(VllmWorker rank=0 pid=42598)[0;0m INFO 09-05 17:29:54 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3, 4, 5, 6, 7], buffer_handle=(7, 4194304, 6, 'psm_78d39c06'), local_subscribe_addr='ipc:///tmp/089f54ca-c0a4-477e-a565-d82675a22c40', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(VllmWorker rank=0 pid=42598)[0;0m INFO 09-05 17:29:54 [parallel_state.py:1102] rank 0 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(VllmWorker rank=2 pid=42600)[0;0m INFO 09-05 17:29:54 [parallel_state.py:1102] rank 2 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 2, EP rank 2
[1;36m(VllmWorker rank=1 pid=42599)[0;0m INFO 09-05 17:29:54 [parallel_state.py:1102] rank 1 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1
[1;36m(VllmWorker rank=3 pid=42601)[0;0m INFO 09-05 17:29:54 [parallel_state.py:1102] rank 3 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 3, EP rank 3
[1;36m(VllmWorker rank=6 pid=42604)[0;0m INFO 09-05 17:29:54 [parallel_state.py:1102] rank 6 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 6, EP rank 6
[1;36m(VllmWorker rank=4 pid=42602)[0;0m INFO 09-05 17:29:54 [parallel_state.py:1102] rank 4 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 4, EP rank 4
[1;36m(VllmWorker rank=5 pid=42603)[0;0m INFO 09-05 17:29:54 [parallel_state.py:1102] rank 5 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 5, EP rank 5
[1;36m(VllmWorker rank=7 pid=42605)[0;0m INFO 09-05 17:29:54 [parallel_state.py:1102] rank 7 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 7, EP rank 7
[1;36m(VllmWorker rank=1 pid=42599)[0;0m WARNING 09-05 17:29:54 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(VllmWorker rank=2 pid=42600)[0;0m WARNING 09-05 17:29:54 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(VllmWorker rank=6 pid=42604)[0;0m WARNING 09-05 17:29:54 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(VllmWorker rank=3 pid=42601)[0;0m WARNING 09-05 17:29:54 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(VllmWorker rank=7 pid=42605)[0;0m WARNING 09-05 17:29:54 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(VllmWorker rank=5 pid=42603)[0;0m WARNING 09-05 17:29:54 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(VllmWorker rank=4 pid=42602)[0;0m WARNING 09-05 17:29:54 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(VllmWorker rank=1 pid=42599)[0;0m INFO 09-05 17:29:54 [gpu_model_runner.py:1843] Starting to load model google/gemma-2-27b-it...
[1;36m(VllmWorker rank=2 pid=42600)[0;0m INFO 09-05 17:29:54 [gpu_model_runner.py:1843] Starting to load model google/gemma-2-27b-it...
[1;36m(VllmWorker rank=6 pid=42604)[0;0m INFO 09-05 17:29:54 [gpu_model_runner.py:1843] Starting to load model google/gemma-2-27b-it...
[1;36m(VllmWorker rank=7 pid=42605)[0;0m INFO 09-05 17:29:54 [gpu_model_runner.py:1843] Starting to load model google/gemma-2-27b-it...
[1;36m(VllmWorker rank=3 pid=42601)[0;0m INFO 09-05 17:29:54 [gpu_model_runner.py:1843] Starting to load model google/gemma-2-27b-it...
[1;36m(VllmWorker rank=5 pid=42603)[0;0m INFO 09-05 17:29:54 [gpu_model_runner.py:1843] Starting to load model google/gemma-2-27b-it...
[1;36m(VllmWorker rank=4 pid=42602)[0;0m INFO 09-05 17:29:54 [gpu_model_runner.py:1843] Starting to load model google/gemma-2-27b-it...
[1;36m(VllmWorker rank=0 pid=42598)[0;0m WARNING 09-05 17:29:54 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(VllmWorker rank=0 pid=42598)[0;0m INFO 09-05 17:29:54 [gpu_model_runner.py:1843] Starting to load model google/gemma-2-27b-it...
[1;36m(VllmWorker rank=4 pid=42602)[0;0m INFO 09-05 17:29:55 [gpu_model_runner.py:1875] Loading model from scratch...
[1;36m(VllmWorker rank=1 pid=42599)[0;0m INFO 09-05 17:29:55 [gpu_model_runner.py:1875] Loading model from scratch...
[1;36m(VllmWorker rank=7 pid=42605)[0;0m INFO 09-05 17:29:55 [gpu_model_runner.py:1875] Loading model from scratch...
[1;36m(VllmWorker rank=3 pid=42601)[0;0m INFO 09-05 17:29:55 [gpu_model_runner.py:1875] Loading model from scratch...
[1;36m(VllmWorker rank=5 pid=42603)[0;0m INFO 09-05 17:29:55 [gpu_model_runner.py:1875] Loading model from scratch...
[1;36m(VllmWorker rank=0 pid=42598)[0;0m INFO 09-05 17:29:55 [gpu_model_runner.py:1875] Loading model from scratch...
[1;36m(VllmWorker rank=4 pid=42602)[0;0m INFO 09-05 17:29:55 [cuda.py:290] Using Flash Attention backend on V1 engine.
[1;36m(VllmWorker rank=6 pid=42604)[0;0m INFO 09-05 17:29:55 [gpu_model_runner.py:1875] Loading model from scratch...
[1;36m(VllmWorker rank=2 pid=42600)[0;0m INFO 09-05 17:29:55 [gpu_model_runner.py:1875] Loading model from scratch...
[1;36m(VllmWorker rank=1 pid=42599)[0;0m INFO 09-05 17:29:55 [cuda.py:290] Using Flash Attention backend on V1 engine.
[1;36m(VllmWorker rank=7 pid=42605)[0;0m INFO 09-05 17:29:55 [cuda.py:290] Using Flash Attention backend on V1 engine.
[1;36m(VllmWorker rank=3 pid=42601)[0;0m INFO 09-05 17:29:55 [cuda.py:290] Using Flash Attention backend on V1 engine.
[1;36m(VllmWorker rank=0 pid=42598)[0;0m INFO 09-05 17:29:55 [cuda.py:290] Using Flash Attention backend on V1 engine.
[1;36m(VllmWorker rank=5 pid=42603)[0;0m INFO 09-05 17:29:55 [cuda.py:290] Using Flash Attention backend on V1 engine.
[1;36m(VllmWorker rank=6 pid=42604)[0;0m INFO 09-05 17:29:55 [cuda.py:290] Using Flash Attention backend on V1 engine.
[1;36m(VllmWorker rank=2 pid=42600)[0;0m INFO 09-05 17:29:55 [cuda.py:290] Using Flash Attention backend on V1 engine.
[1;36m(VllmWorker rank=4 pid=42602)[0;0m INFO 09-05 17:29:55 [weight_utils.py:296] Using model weights format ['*.safetensors']
[1;36m(VllmWorker rank=1 pid=42599)[0;0m INFO 09-05 17:29:55 [weight_utils.py:296] Using model weights format ['*.safetensors']
[1;36m(VllmWorker rank=3 pid=42601)[0;0m INFO 09-05 17:29:55 [weight_utils.py:296] Using model weights format ['*.safetensors']
[1;36m(VllmWorker rank=5 pid=42603)[0;0m INFO 09-05 17:29:55 [weight_utils.py:296] Using model weights format ['*.safetensors']
[1;36m(VllmWorker rank=0 pid=42598)[0;0m INFO 09-05 17:29:55 [weight_utils.py:296] Using model weights format ['*.safetensors']
[1;36m(VllmWorker rank=7 pid=42605)[0;0m INFO 09-05 17:29:55 [weight_utils.py:296] Using model weights format ['*.safetensors']
[1;36m(VllmWorker rank=6 pid=42604)[0;0m INFO 09-05 17:29:55 [weight_utils.py:296] Using model weights format ['*.safetensors']
[1;36m(VllmWorker rank=2 pid=42600)[0;0m INFO 09-05 17:29:55 [weight_utils.py:296] Using model weights format ['*.safetensors']
[1;36m(VllmWorker rank=0 pid=42598)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/12 [00:00<?, ?it/s]
[1;36m(VllmWorker rank=0 pid=42598)[0;0m Loading safetensors checkpoint shards:   8% Completed | 1/12 [00:00<00:01,  6.72it/s]
[1;36m(VllmWorker rank=0 pid=42598)[0;0m Loading safetensors checkpoint shards:  17% Completed | 2/12 [00:00<00:01,  5.29it/s]
[1;36m(VllmWorker rank=0 pid=42598)[0;0m Loading safetensors checkpoint shards:  25% Completed | 3/12 [00:00<00:01,  4.84it/s]
[1;36m(VllmWorker rank=0 pid=42598)[0;0m Loading safetensors checkpoint shards:  33% Completed | 4/12 [00:00<00:01,  4.53it/s]
[1;36m(VllmWorker rank=0 pid=42598)[0;0m Loading safetensors checkpoint shards:  42% Completed | 5/12 [00:01<00:01,  4.53it/s]
[1;36m(VllmWorker rank=0 pid=42598)[0;0m Loading safetensors checkpoint shards:  50% Completed | 6/12 [00:01<00:01,  4.38it/s]
[1;36m(VllmWorker rank=0 pid=42598)[0;0m Loading safetensors checkpoint shards:  58% Completed | 7/12 [00:01<00:01,  4.26it/s]
[1;36m(VllmWorker rank=0 pid=42598)[0;0m Loading safetensors checkpoint shards:  67% Completed | 8/12 [00:01<00:00,  4.33it/s]
[1;36m(VllmWorker rank=4 pid=42602)[0;0m INFO 09-05 17:29:57 [default_loader.py:262] Loading weights took 2.36 seconds
[1;36m(VllmWorker rank=0 pid=42598)[0;0m Loading safetensors checkpoint shards:  83% Completed | 10/12 [00:02<00:00,  5.57it/s]
[1;36m(VllmWorker rank=1 pid=42599)[0;0m INFO 09-05 17:29:58 [default_loader.py:262] Loading weights took 2.47 seconds
[1;36m(VllmWorker rank=3 pid=42601)[0;0m INFO 09-05 17:29:58 [default_loader.py:262] Loading weights took 2.41 seconds
[1;36m(VllmWorker rank=5 pid=42603)[0;0m INFO 09-05 17:29:58 [default_loader.py:262] Loading weights took 2.40 seconds
[1;36m(VllmWorker rank=7 pid=42605)[0;0m INFO 09-05 17:29:58 [default_loader.py:262] Loading weights took 2.44 seconds
[1;36m(VllmWorker rank=4 pid=42602)[0;0m INFO 09-05 17:29:58 [gpu_model_runner.py:1892] Model loading took 6.3904 GiB and 2.659841 seconds
[1;36m(VllmWorker rank=0 pid=42598)[0;0m Loading safetensors checkpoint shards:  92% Completed | 11/12 [00:02<00:00,  5.41it/s]
[1;36m(VllmWorker rank=6 pid=42604)[0;0m INFO 09-05 17:29:58 [default_loader.py:262] Loading weights took 2.44 seconds
[1;36m(VllmWorker rank=2 pid=42600)[0;0m INFO 09-05 17:29:58 [default_loader.py:262] Loading weights took 2.45 seconds
[1;36m(VllmWorker rank=0 pid=42598)[0;0m Loading safetensors checkpoint shards: 100% Completed | 12/12 [00:02<00:00,  5.14it/s]
[1;36m(VllmWorker rank=0 pid=42598)[0;0m Loading safetensors checkpoint shards: 100% Completed | 12/12 [00:02<00:00,  4.92it/s]
[1;36m(VllmWorker rank=0 pid=42598)[0;0m 
[1;36m(VllmWorker rank=0 pid=42598)[0;0m INFO 09-05 17:29:58 [default_loader.py:262] Loading weights took 2.47 seconds
[1;36m(VllmWorker rank=1 pid=42599)[0;0m INFO 09-05 17:29:58 [gpu_model_runner.py:1892] Model loading took 6.3904 GiB and 2.773611 seconds
[1;36m(VllmWorker rank=3 pid=42601)[0;0m INFO 09-05 17:29:58 [gpu_model_runner.py:1892] Model loading took 6.3904 GiB and 2.779607 seconds
[1;36m(VllmWorker rank=5 pid=42603)[0;0m INFO 09-05 17:29:58 [gpu_model_runner.py:1892] Model loading took 6.3904 GiB and 2.812309 seconds
[1;36m(VllmWorker rank=7 pid=42605)[0;0m INFO 09-05 17:29:58 [gpu_model_runner.py:1892] Model loading took 6.3904 GiB and 2.925038 seconds
[1;36m(VllmWorker rank=2 pid=42600)[0;0m INFO 09-05 17:29:58 [gpu_model_runner.py:1892] Model loading took 6.3904 GiB and 3.026743 seconds
[1;36m(VllmWorker rank=6 pid=42604)[0;0m INFO 09-05 17:29:58 [gpu_model_runner.py:1892] Model loading took 6.3904 GiB and 2.957378 seconds
[1;36m(VllmWorker rank=0 pid=42598)[0;0m INFO 09-05 17:29:59 [gpu_model_runner.py:1892] Model loading took 6.3904 GiB and 3.203878 seconds
[1;36m(VllmWorker rank=4 pid=42602)[0;0m INFO 09-05 17:30:08 [backends.py:530] Using cache directory: /root/.cache/vllm/torch_compile_cache/173ba21758/rank_4_0/backbone for vLLM's torch.compile
[1;36m(VllmWorker rank=4 pid=42602)[0;0m INFO 09-05 17:30:08 [backends.py:541] Dynamo bytecode transform time: 8.83 s
[1;36m(VllmWorker rank=0 pid=42598)[0;0m INFO 09-05 17:30:08 [backends.py:530] Using cache directory: /root/.cache/vllm/torch_compile_cache/173ba21758/rank_0_0/backbone for vLLM's torch.compile
[1;36m(VllmWorker rank=0 pid=42598)[0;0m INFO 09-05 17:30:08 [backends.py:541] Dynamo bytecode transform time: 8.87 s
[1;36m(VllmWorker rank=5 pid=42603)[0;0m INFO 09-05 17:30:08 [backends.py:530] Using cache directory: /root/.cache/vllm/torch_compile_cache/173ba21758/rank_5_0/backbone for vLLM's torch.compile
[1;36m(VllmWorker rank=5 pid=42603)[0;0m INFO 09-05 17:30:08 [backends.py:541] Dynamo bytecode transform time: 8.90 s
[1;36m(VllmWorker rank=1 pid=42599)[0;0m INFO 09-05 17:30:08 [backends.py:530] Using cache directory: /root/.cache/vllm/torch_compile_cache/173ba21758/rank_1_0/backbone for vLLM's torch.compile
[1;36m(VllmWorker rank=1 pid=42599)[0;0m INFO 09-05 17:30:08 [backends.py:541] Dynamo bytecode transform time: 8.90 s
[1;36m(VllmWorker rank=2 pid=42600)[0;0m INFO 09-05 17:30:08 [backends.py:530] Using cache directory: /root/.cache/vllm/torch_compile_cache/173ba21758/rank_2_0/backbone for vLLM's torch.compile
[1;36m(VllmWorker rank=2 pid=42600)[0;0m INFO 09-05 17:30:08 [backends.py:541] Dynamo bytecode transform time: 8.91 s
[1;36m(VllmWorker rank=7 pid=42605)[0;0m INFO 09-05 17:30:08 [backends.py:530] Using cache directory: /root/.cache/vllm/torch_compile_cache/173ba21758/rank_7_0/backbone for vLLM's torch.compile
[1;36m(VllmWorker rank=7 pid=42605)[0;0m INFO 09-05 17:30:08 [backends.py:541] Dynamo bytecode transform time: 8.95 s
[1;36m(VllmWorker rank=3 pid=42601)[0;0m INFO 09-05 17:30:08 [backends.py:530] Using cache directory: /root/.cache/vllm/torch_compile_cache/173ba21758/rank_3_0/backbone for vLLM's torch.compile
[1;36m(VllmWorker rank=3 pid=42601)[0;0m INFO 09-05 17:30:08 [backends.py:541] Dynamo bytecode transform time: 8.96 s
[1;36m(VllmWorker rank=6 pid=42604)[0;0m INFO 09-05 17:30:08 [backends.py:530] Using cache directory: /root/.cache/vllm/torch_compile_cache/173ba21758/rank_6_0/backbone for vLLM's torch.compile
[1;36m(VllmWorker rank=6 pid=42604)[0;0m INFO 09-05 17:30:08 [backends.py:541] Dynamo bytecode transform time: 9.13 s
[1;36m(VllmWorker rank=4 pid=42602)[0;0m INFO 09-05 17:30:10 [backends.py:194] Cache the graph for dynamic shape for later use
[1;36m(VllmWorker rank=0 pid=42598)[0;0m INFO 09-05 17:30:10 [backends.py:194] Cache the graph for dynamic shape for later use
[1;36m(VllmWorker rank=5 pid=42603)[0;0m INFO 09-05 17:30:11 [backends.py:194] Cache the graph for dynamic shape for later use
[1;36m(VllmWorker rank=2 pid=42600)[0;0m INFO 09-05 17:30:11 [backends.py:194] Cache the graph for dynamic shape for later use
[1;36m(VllmWorker rank=1 pid=42599)[0;0m INFO 09-05 17:30:11 [backends.py:194] Cache the graph for dynamic shape for later use
[1;36m(VllmWorker rank=7 pid=42605)[0;0m INFO 09-05 17:30:11 [backends.py:194] Cache the graph for dynamic shape for later use
[1;36m(VllmWorker rank=3 pid=42601)[0;0m INFO 09-05 17:30:11 [backends.py:194] Cache the graph for dynamic shape for later use
[1;36m(VllmWorker rank=6 pid=42604)[0;0m INFO 09-05 17:30:11 [backends.py:194] Cache the graph for dynamic shape for later use
[1;36m(VllmWorker rank=4 pid=42602)[0;0m INFO 09-05 17:30:37 [backends.py:215] Compiling a graph for dynamic shape takes 29.41 s
[1;36m(VllmWorker rank=1 pid=42599)[0;0m INFO 09-05 17:30:38 [backends.py:215] Compiling a graph for dynamic shape takes 29.75 s
[1;36m(VllmWorker rank=5 pid=42603)[0;0m INFO 09-05 17:30:38 [backends.py:215] Compiling a graph for dynamic shape takes 29.78 s
[1;36m(VllmWorker rank=7 pid=42605)[0;0m INFO 09-05 17:30:38 [backends.py:215] Compiling a graph for dynamic shape takes 30.07 s
[1;36m(VllmWorker rank=0 pid=42598)[0;0m INFO 09-05 17:30:38 [backends.py:215] Compiling a graph for dynamic shape takes 30.20 s
[1;36m(VllmWorker rank=6 pid=42604)[0;0m INFO 09-05 17:30:38 [backends.py:215] Compiling a graph for dynamic shape takes 30.03 s
[1;36m(VllmWorker rank=2 pid=42600)[0;0m INFO 09-05 17:30:38 [backends.py:215] Compiling a graph for dynamic shape takes 30.30 s
[1;36m(VllmWorker rank=3 pid=42601)[0;0m INFO 09-05 17:30:38 [backends.py:215] Compiling a graph for dynamic shape takes 30.28 s
[1;36m(VllmWorker rank=4 pid=42602)[0;0m INFO 09-05 17:30:46 [monitor.py:34] torch.compile takes 38.24 s in total
[1;36m(VllmWorker rank=0 pid=42598)[0;0m INFO 09-05 17:30:46 [monitor.py:34] torch.compile takes 39.07 s in total
[1;36m(VllmWorker rank=1 pid=42599)[0;0m INFO 09-05 17:30:46 [monitor.py:34] torch.compile takes 38.66 s in total
[1;36m(VllmWorker rank=6 pid=42604)[0;0m INFO 09-05 17:30:46 [monitor.py:34] torch.compile takes 39.17 s in total
[1;36m(VllmWorker rank=5 pid=42603)[0;0m INFO 09-05 17:30:46 [monitor.py:34] torch.compile takes 38.68 s in total
[1;36m(VllmWorker rank=3 pid=42601)[0;0m INFO 09-05 17:30:46 [monitor.py:34] torch.compile takes 39.24 s in total
[1;36m(VllmWorker rank=7 pid=42605)[0;0m INFO 09-05 17:30:46 [monitor.py:34] torch.compile takes 39.02 s in total
[1;36m(VllmWorker rank=2 pid=42600)[0;0m INFO 09-05 17:30:46 [monitor.py:34] torch.compile takes 39.21 s in total
[1;36m(VllmWorker rank=3 pid=42601)[0;0m INFO 09-05 17:30:49 [gpu_worker.py:255] Available KV cache memory: 107.34 GiB
[1;36m(VllmWorker rank=4 pid=42602)[0;0m INFO 09-05 17:30:49 [gpu_worker.py:255] Available KV cache memory: 107.34 GiB
[1;36m(VllmWorker rank=6 pid=42604)[0;0m INFO 09-05 17:30:49 [gpu_worker.py:255] Available KV cache memory: 107.34 GiB
[1;36m(VllmWorker rank=2 pid=42600)[0;0m INFO 09-05 17:30:49 [gpu_worker.py:255] Available KV cache memory: 107.34 GiB
[1;36m(VllmWorker rank=7 pid=42605)[0;0m INFO 09-05 17:30:49 [gpu_worker.py:255] Available KV cache memory: 107.81 GiB
[1;36m(VllmWorker rank=0 pid=42598)[0;0m INFO 09-05 17:30:49 [gpu_worker.py:255] Available KV cache memory: 107.44 GiB
[1;36m(VllmWorker rank=5 pid=42603)[0;0m INFO 09-05 17:30:49 [gpu_worker.py:255] Available KV cache memory: 107.34 GiB
[1;36m(VllmWorker rank=1 pid=42599)[0;0m INFO 09-05 17:30:49 [gpu_worker.py:255] Available KV cache memory: 107.34 GiB
INFO 09-05 17:30:50 [kv_cache_utils.py:997] GPU KV cache size: 2,449,024 tokens
INFO 09-05 17:30:50 [kv_cache_utils.py:1001] Maximum concurrency for 8,192 tokens per request: 298.66x
INFO 09-05 17:30:50 [kv_cache_utils.py:997] GPU KV cache size: 2,446,880 tokens
INFO 09-05 17:30:50 [kv_cache_utils.py:1001] Maximum concurrency for 8,192 tokens per request: 298.40x
INFO 09-05 17:30:50 [kv_cache_utils.py:997] GPU KV cache size: 2,446,880 tokens
INFO 09-05 17:30:50 [kv_cache_utils.py:1001] Maximum concurrency for 8,192 tokens per request: 298.40x
INFO 09-05 17:30:50 [kv_cache_utils.py:997] GPU KV cache size: 2,446,880 tokens
INFO 09-05 17:30:50 [kv_cache_utils.py:1001] Maximum concurrency for 8,192 tokens per request: 298.40x
INFO 09-05 17:30:50 [kv_cache_utils.py:997] GPU KV cache size: 2,446,880 tokens
INFO 09-05 17:30:50 [kv_cache_utils.py:1001] Maximum concurrency for 8,192 tokens per request: 298.40x
INFO 09-05 17:30:50 [kv_cache_utils.py:997] GPU KV cache size: 2,446,880 tokens
INFO 09-05 17:30:50 [kv_cache_utils.py:1001] Maximum concurrency for 8,192 tokens per request: 298.40x
INFO 09-05 17:30:50 [kv_cache_utils.py:997] GPU KV cache size: 2,446,880 tokens
INFO 09-05 17:30:50 [kv_cache_utils.py:1001] Maximum concurrency for 8,192 tokens per request: 298.40x
INFO 09-05 17:30:50 [kv_cache_utils.py:997] GPU KV cache size: 2,457,568 tokens
INFO 09-05 17:30:50 [kv_cache_utils.py:1001] Maximum concurrency for 8,192 tokens per request: 299.70x
[1;36m(VllmWorker rank=0 pid=42598)[0;0m Capturing CUDA graph shapes:   0%|          | 0/67 [00:00<?, ?it/s]Capturing CUDA graph shapes:   4%|â–         | 3/67 [00:00<00:02, 25.87it/s]Capturing CUDA graph shapes:   9%|â–‰         | 6/67 [00:00<00:02, 25.87it/s]Capturing CUDA graph shapes:  13%|â–ˆâ–Ž        | 9/67 [00:00<00:02, 25.37it/s]Capturing CUDA graph shapes:  18%|â–ˆâ–Š        | 12/67 [00:00<00:02, 25.20it/s]Capturing CUDA graph shapes:  22%|â–ˆâ–ˆâ–       | 15/67 [00:00<00:02, 24.52it/s]Capturing CUDA graph shapes:  27%|â–ˆâ–ˆâ–‹       | 18/67 [00:00<00:01, 24.70it/s]Capturing CUDA graph shapes:  31%|â–ˆâ–ˆâ–ˆâ–      | 21/67 [00:00<00:01, 25.18it/s]Capturing CUDA graph shapes:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 24/67 [00:00<00:01, 25.25it/s]Capturing CUDA graph shapes:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 27/67 [00:01<00:01, 25.12it/s]Capturing CUDA graph shapes:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 30/67 [00:01<00:01, 24.42it/s]Capturing CUDA graph shapes:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 33/67 [00:01<00:01, 23.87it/s]Capturing CUDA graph shapes:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 36/67 [00:01<00:01, 23.78it/s]Capturing CUDA graph shapes:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 39/67 [00:01<00:01, 23.52it/s]Capturing CUDA graph shapes:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 42/67 [00:01<00:01, 22.57it/s]Capturing CUDA graph shapes:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 45/67 [00:01<00:00, 22.44it/s]Capturing CUDA graph shapes:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 48/67 [00:02<00:00, 21.95it/s]Capturing CUDA graph shapes:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 51/67 [00:02<00:00, 21.44it/s]Capturing CUDA graph shapes:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 54/67 [00:02<00:00, 21.00it/s]Capturing CUDA graph shapes:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 57/67 [00:02<00:00, 21.14it/s]Capturing CUDA graph shapes:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 60/67 [00:02<00:00, 21.21it/s]Capturing CUDA graph shapes:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 63/67 [00:02<00:00, 21.39it/s]Capturing CUDA graph shapes:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 66/67 [00:02<00:00, 21.90it/s][1;36m(VllmWorker rank=3 pid=42601)[0;0m INFO 09-05 17:30:53 [custom_all_reduce.py:196] Registering 6231 cuda graph addresses
Capturing CUDA graph shapes: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 67/67 [00:02<00:00, 23.06it/s]
[1;36m(VllmWorker rank=6 pid=42604)[0;0m INFO 09-05 17:30:53 [custom_all_reduce.py:196] Registering 6231 cuda graph addresses
[1;36m(VllmWorker rank=0 pid=42598)[0;0m INFO 09-05 17:30:53 [custom_all_reduce.py:196] Registering 6231 cuda graph addresses
[1;36m(VllmWorker rank=1 pid=42599)[0;0m INFO 09-05 17:30:53 [custom_all_reduce.py:196] Registering 6231 cuda graph addresses
[1;36m(VllmWorker rank=7 pid=42605)[0;0m INFO 09-05 17:30:53 [custom_all_reduce.py:196] Registering 6231 cuda graph addresses
[1;36m(VllmWorker rank=2 pid=42600)[0;0m INFO 09-05 17:30:53 [custom_all_reduce.py:196] Registering 6231 cuda graph addresses
[1;36m(VllmWorker rank=5 pid=42603)[0;0m INFO 09-05 17:30:53 [custom_all_reduce.py:196] Registering 6231 cuda graph addresses
[1;36m(VllmWorker rank=4 pid=42602)[0;0m INFO 09-05 17:30:53 [custom_all_reduce.py:196] Registering 6231 cuda graph addresses
[1;36m(VllmWorker rank=7 pid=42605)[0;0m INFO 09-05 17:30:53 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.83 GiB
[1;36m(VllmWorker rank=3 pid=42601)[0;0m INFO 09-05 17:30:53 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.83 GiB
[1;36m(VllmWorker rank=4 pid=42602)[0;0m INFO 09-05 17:30:53 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.83 GiB
[1;36m(VllmWorker rank=1 pid=42599)[0;0m INFO 09-05 17:30:53 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.83 GiB
[1;36m(VllmWorker rank=6 pid=42604)[0;0m INFO 09-05 17:30:53 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.83 GiB
[1;36m(VllmWorker rank=0 pid=42598)[0;0m INFO 09-05 17:30:53 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.83 GiB
[1;36m(VllmWorker rank=5 pid=42603)[0;0m INFO 09-05 17:30:53 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.83 GiB
[1;36m(VllmWorker rank=2 pid=42600)[0;0m INFO 09-05 17:30:53 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.83 GiB
INFO 09-05 17:30:53 [core.py:193] init engine (profile, create kv cache, warmup model) took 54.76 seconds
INFO:utils.inference_utils:Successfully loaded vLLM model: google/gemma-2-27b-it
INFO:utils.inference_utils:Processing batch of 900 conversations...
Generating responses with batch processing...
Adding requests:   0%|          | 0/900 [00:00<?, ?it/s]Adding requests:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 825/900 [00:00<00:00, 8246.65it/s]Adding requests: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 900/900 [00:00<00:00, 8256.70it/s]
Processed prompts:   0%|          | 0/900 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   0%|          | 1/900 [00:00<09:00,  1.66it/s, est. speed input: 36.56 toks/s, output: 13.30 toks/s]Processed prompts:   3%|â–Ž         | 25/900 [00:00<00:24, 36.45it/s, est. speed input: 567.46 toks/s, output: 237.70 toks/s]Processed prompts:   6%|â–Œ         | 55/900 [00:00<00:10, 77.60it/s, est. speed input: 1080.28 toks/s, output: 627.22 toks/s]Processed prompts:   8%|â–Š         | 76/900 [00:01<00:08, 97.09it/s, est. speed input: 1313.27 toks/s, output: 826.57 toks/s]Processed prompts:  10%|â–ˆ         | 91/900 [00:01<00:07, 102.86it/s, est. speed input: 1396.81 toks/s, output: 954.44 toks/s]Processed prompts:  12%|â–ˆâ–        | 105/900 [00:01<00:07, 104.87it/s, est. speed input: 1482.76 toks/s, output: 1067.32 toks/s]Processed prompts:  16%|â–ˆâ–Œ        | 145/900 [00:01<00:04, 164.49it/s, est. speed input: 2019.78 toks/s, output: 1537.25 toks/s]Processed prompts:  18%|â–ˆâ–Š        | 165/900 [00:01<00:04, 148.97it/s, est. speed input: 2095.61 toks/s, output: 1652.78 toks/s]Processed prompts:  20%|â–ˆâ–ˆ        | 183/900 [00:01<00:05, 134.07it/s, est. speed input: 2120.57 toks/s, output: 1762.42 toks/s]Processed prompts:  22%|â–ˆâ–ˆâ–       | 201/900 [00:01<00:05, 139.16it/s, est. speed input: 2215.26 toks/s, output: 1923.03 toks/s]Processed prompts:  24%|â–ˆâ–ˆâ–       | 217/900 [00:02<00:06, 113.64it/s, est. speed input: 2163.12 toks/s, output: 1958.62 toks/s]Processed prompts:  26%|â–ˆâ–ˆâ–Œ       | 230/900 [00:02<00:05, 115.91it/s, est. speed input: 2190.27 toks/s, output: 2069.17 toks/s]Processed prompts:  27%|â–ˆâ–ˆâ–‹       | 243/900 [00:02<00:10, 65.07it/s, est. speed input: 1920.50 toks/s, output: 1915.01 toks/s] Processed prompts:  28%|â–ˆâ–ˆâ–Š       | 253/900 [00:02<00:10, 61.17it/s, est. speed input: 1868.27 toks/s, output: 1942.81 toks/s]Processed prompts:  29%|â–ˆâ–ˆâ–‰       | 262/900 [00:03<00:10, 60.49it/s, est. speed input: 1845.54 toks/s, output: 1991.36 toks/s]Processed prompts:  30%|â–ˆâ–ˆâ–ˆ       | 270/900 [00:03<00:14, 43.96it/s, est. speed input: 1700.09 toks/s, output: 1906.92 toks/s]Processed prompts:  31%|â–ˆâ–ˆâ–ˆ       | 276/900 [00:03<00:13, 46.11it/s, est. speed input: 1689.98 toks/s, output: 1948.18 toks/s]Processed prompts:  31%|â–ˆâ–ˆâ–ˆâ–      | 282/900 [00:03<00:13, 44.82it/s, est. speed input: 1660.33 toks/s, output: 1969.20 toks/s]Processed prompts:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 294/900 [00:03<00:10, 58.54it/s, est. speed input: 1688.08 toks/s, output: 2112.20 toks/s]Processed prompts:  34%|â–ˆâ–ˆâ–ˆâ–Ž      | 302/900 [00:03<00:10, 57.68it/s, est. speed input: 1671.93 toks/s, output: 2166.82 toks/s]Processed prompts:  35%|â–ˆâ–ˆâ–ˆâ–      | 312/900 [00:04<00:09, 61.19it/s, est. speed input: 1668.77 toks/s, output: 2257.10 toks/s]Processed prompts:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 320/900 [00:04<00:09, 59.54it/s, est. speed input: 1656.11 toks/s, output: 2313.83 toks/s]Processed prompts:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 330/900 [00:04<00:09, 61.45it/s, est. speed input: 1650.52 toks/s, output: 2402.33 toks/s]Processed prompts:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 342/900 [00:04<00:08, 66.32it/s, est. speed input: 1653.63 toks/s, output: 2521.47 toks/s]Processed prompts:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 351/900 [00:04<00:08, 65.27it/s, est. speed input: 1646.27 toks/s, output: 2596.49 toks/s]Processed prompts:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 364/900 [00:04<00:07, 73.08it/s, est. speed input: 1658.18 toks/s, output: 2743.27 toks/s]Processed prompts:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 378/900 [00:04<00:06, 81.06it/s, est. speed input: 1678.02 toks/s, output: 2906.25 toks/s]Processed prompts:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 399/900 [00:05<00:04, 105.29it/s, est. speed input: 1728.02 toks/s, output: 3200.29 toks/s]Processed prompts:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 419/900 [00:05<00:03, 125.24it/s, est. speed input: 1777.16 toks/s, output: 3485.27 toks/s]Processed prompts:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 439/900 [00:05<00:03, 142.69it/s, est. speed input: 1825.87 toks/s, output: 3771.87 toks/s]Processed prompts:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 455/900 [00:05<00:03, 146.10it/s, est. speed input: 1856.23 toks/s, output: 3987.90 toks/s]Processed prompts:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 471/900 [00:05<00:02, 149.29it/s, est. speed input: 1887.42 toks/s, output: 4207.02 toks/s]Processed prompts:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 487/900 [00:05<00:02, 151.79it/s, est. speed input: 1919.54 toks/s, output: 4427.16 toks/s]Processed prompts:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 503/900 [00:05<00:02, 153.94it/s, est. speed input: 1949.19 toks/s, output: 4646.21 toks/s]Processed prompts:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 519/900 [00:05<00:02, 145.93it/s, est. speed input: 1968.88 toks/s, output: 4850.01 toks/s]Processed prompts:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 547/900 [00:05<00:02, 174.95it/s, est. speed input: 2037.57 toks/s, output: 5290.67 toks/s]Processed prompts:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 579/900 [00:06<00:01, 207.93it/s, est. speed input: 2113.89 toks/s, output: 5812.36 toks/s]Processed prompts:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 610/900 [00:06<00:01, 229.92it/s, est. speed input: 2186.35 toks/s, output: 6320.16 toks/s]Processed prompts:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 650/900 [00:06<00:00, 263.72it/s, est. speed input: 2285.01 toks/s, output: 7003.55 toks/s]Processed prompts:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 681/900 [00:06<00:00, 276.26it/s, est. speed input: 2357.50 toks/s, output: 7531.10 toks/s]Processed prompts:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 725/900 [00:06<00:00, 310.17it/s, est. speed input: 2468.76 toks/s, output: 8325.87 toks/s]Processed prompts:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 765/900 [00:06<00:00, 330.02it/s, est. speed input: 2560.49 toks/s, output: 9059.23 toks/s]Processed prompts:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 799/900 [00:06<00:00, 316.39it/s, est. speed input: 2624.81 toks/s, output: 9658.01 toks/s]Processed prompts:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 831/900 [00:06<00:00, 283.94it/s, est. speed input: 2671.51 toks/s, output: 10199.08 toks/s]Processed prompts:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 860/900 [00:07<00:00, 204.42it/s, est. speed input: 2663.09 toks/s, output: 10556.15 toks/s]Processed prompts:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 884/900 [00:07<00:00, 167.94it/s, est. speed input: 2650.13 toks/s, output: 10890.63 toks/s]Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 900/900 [00:07<00:00, 167.94it/s, est. speed input: 2573.10 toks/s, output: 10926.47 toks/s]Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 900/900 [00:07<00:00, 117.22it/s, est. speed input: 2573.10 toks/s, output: 10926.47 toks/s]
INFO:utils.inference_utils:Completed batch processing of 900 conversations
[rank1]:[W905 17:31:04.015701344 TCPStore.cpp:125] [c10d] recvValue failed on SocketImpl(fd=162, addr=[::ffff:127.0.0.1]:43644, remote=[::ffff:127.0.0.1]:40691): failed to recv, got 0 bytes
Exception raised from recvBytes at /pytorch/torch/csrc/distributed/c10d/Utils.hpp:678 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x98 (0x73440b51c5e8 in /root/git/persona-subspace/.venv/lib/python3.13/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x5ba8bfe (0x7343f3cf4bfe in /root/git/persona-subspace/.venv/lib/python3.13/site-packages/torch/lib/libtorch_cpu.so)
frame #2: <unknown function> + 0x5baaf40 (0x7343f3cf6f40 in /root/git/persona-subspace/.venv/lib/python3.13/site-packages/torch/lib/libtorch_cpu.so)
frame #3: <unknown function> + 0x5bab84a (0x7343f3cf784a in /root/git/persona-subspace/.venv/lib/python3.13/site-packages/torch/lib/libtorch_cpu.so)
frame #4: c10d::TCPStore::check(std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > const&) + 0x2a9 (0x7343f3cf12a9 in /root/git/persona-subspace/.venv/lib/python3.13/site-packages/torch/lib/libtorch_cpu.so)
frame #5: c10d::ProcessGroupNCCL::heartbeatMonitor() + 0x379 (0x7343b53f09f9 in /root/git/persona-subspace/.venv/lib/python3.13/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0xdc253 (0x73440a8b0253 in /lib/x86_64-linux-gnu/libstdc++.so.6)
frame #7: <unknown function> + 0x94ac3 (0x73440c176ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: clone + 0x44 (0x73440c207a04 in /lib/x86_64-linux-gnu/libc.so.6)

[rank1]:[W905 17:31:04.026887579 ProcessGroupNCCL.cpp:1662] [PG ID 0 PG GUID 0 Rank 1] Failed to check the "should dump" flag on TCPStore, (maybe TCPStore server has shut down too early), with error: failed to recv, got 0 bytes
INFO:utils.inference_utils:Closed vLLM model google/gemma-2-27b-it
INFO:utils.inference_utils:Closed vLLM model google/gemma-2-27b-it
Writing results to /root/git/persona-subspace/evals/susceptibility/gemma-2-27b/unsteered/default_50.jsonl
Successfully wrote 900 new results to /root/git/persona-subspace/evals/susceptibility/gemma-2-27b/unsteered/default_50.jsonl

Baseline generation completed!
Results saved to: /root/git/persona-subspace/evals/susceptibility/gemma-2-27b/unsteered/default_50.jsonl
INFO:utils.inference_utils:Cleaned up all vLLM models
