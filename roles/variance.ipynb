{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28531137",
   "metadata": {},
   "source": [
    "# Variance comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "30f3bcdd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T00:18:08.842015Z",
     "iopub.status.busy": "2025-08-06T00:18:08.841387Z",
     "iopub.status.idle": "2025-08-06T00:18:09.171163Z",
     "shell.execute_reply": "2025-08-06T00:18:09.170519Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "sys.path.append('.')\n",
    "sys.path.append('..')\n",
    "\n",
    "from utils.pca_utils import *\n",
    "from plots import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "julz1owfwk",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "o7pmyp071r",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration - Change these parameters for different models/datasets\n",
    "base_dir = \"/workspace/llama-3.3-70b\"\n",
    "type = \"roles_240\"\n",
    "dir = f\"{base_dir}/{type}\"\n",
    "model_name = \"Llama-3.3-70B\"\n",
    "layer = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "76552389",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/git/persona-subspace/.venv/lib/python3.12/site-packages/sklearn/base.py:442: InconsistentVersionWarning: Trying to unpickle estimator PCA from version 1.7.0 when using version 1.7.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/root/git/persona-subspace/.venv/lib/python3.12/site-packages/sklearn/base.py:442: InconsistentVersionWarning: Trying to unpickle estimator StandardScaler from version 1.7.0 when using version 1.7.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "pca_results = torch.load(f\"{dir}/pca/layer{layer}_pos23.pt\", weights_only=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62bec2d8",
   "metadata": {},
   "source": [
    "## Variance across and within roles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b5319f",
   "metadata": {},
   "source": [
    "### raw activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d16d954f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([275, 8192])\n",
      "torch.Size([8192])\n"
     ]
    }
   ],
   "source": [
    "vectors = torch.stack(pca_results['vectors']['pos_3'])[:, layer, :].float()\n",
    "print(vectors.shape)\n",
    "\n",
    "# compute variance across roles (rows) along hidden_dims\n",
    "raw_across_var = torch.var(vectors, dim=0)\n",
    "print(raw_across_var.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "197362bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 275 scores\n"
     ]
    }
   ],
   "source": [
    "# load in scores\n",
    "scores = {}\n",
    "for file in os.listdir(f\"{dir}/extract_scores\"):\n",
    "    if file.endswith('.json'):\n",
    "        scores[file.replace('.json', '')] = json.load(open(f\"{dir}/extract_scores/{file}\"))\n",
    "\n",
    "print(f\"Loaded {len(scores)} scores\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "48c900d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in raw activations\n",
    "activations = {}\n",
    "for file in os.listdir(f\"{dir}/response_activations\"):\n",
    "    if file.endswith('.pt') and 'default' not in file:\n",
    "        # dict we should iterate over (1200 each)\n",
    "        role_activations = []\n",
    "        obj = torch.load(f\"{dir}/response_activations/{file}\")\n",
    "        for key in obj:\n",
    "            if scores[file.replace('.pt', '')][key] == 3:\n",
    "                role_activations.append(obj[key])\n",
    "        activations[file.replace('.pt', '')] = torch.stack(role_activations)\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "912a4092",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for 275 roles, shape is torch.Size([8192])\n"
     ]
    }
   ],
   "source": [
    "# compute variance within roles\n",
    "raw_within_var = []\n",
    "for file in activations:\n",
    "    raw_within_var.append(torch.var(activations[file][:, layer, :], dim=0))\n",
    "\n",
    "print(f\"for {len(raw_within_var)} roles, shape is {raw_within_var[0].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0ea71a5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8192])\n"
     ]
    }
   ],
   "source": [
    "avg_raw_within_var = torch.stack(raw_within_var).mean(dim=0)\n",
    "print(avg_raw_within_var.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1cf860e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ratio of raw_across_var / avg_raw_within_var is 0.2864890992641449\n"
     ]
    }
   ],
   "source": [
    "# total variance ratio\n",
    "raw_ratio = raw_across_var.mean() / avg_raw_within_var.mean()\n",
    "print(f\"ratio of raw_across_var / avg_raw_within_var is {raw_ratio}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0512101c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8192])\n"
     ]
    }
   ],
   "source": [
    "raw_across_var_normalized = torch.var(F.normalize(vectors, p=2, dim=1), dim=0)\n",
    "print(raw_across_var_normalized.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "63901cea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for 275 roles, shape is torch.Size([8192])\n",
      "torch.Size([8192])\n"
     ]
    }
   ],
   "source": [
    "raw_within_var_normalized = []\n",
    "for file in activations:\n",
    "    raw_within_var_normalized.append(torch.var(F.normalize(activations[file][:, layer, :], p=2, dim=1), dim=0))\n",
    "\n",
    "print(f\"for {len(raw_within_var_normalized)} roles, shape is {raw_within_var_normalized[0].shape}\")\n",
    "avg_raw_within_var_normalized = torch.stack(raw_within_var_normalized).mean(dim=0)\n",
    "print(avg_raw_within_var_normalized.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5cc8b5e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ratio of raw_across_var_normalized / avg_raw_within_var_normalized is 0.4062836766242981\n"
     ]
    }
   ],
   "source": [
    "raw_ratio_normalized = raw_across_var_normalized.mean() / avg_raw_within_var_normalized.mean()\n",
    "print(f\"ratio of raw_across_var_normalized / avg_raw_within_var_normalized is {raw_ratio_normalized}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8f2faa",
   "metadata": {},
   "source": [
    "### in PC space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0642d452",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(377,)\n"
     ]
    }
   ],
   "source": [
    "# get transformed role vectors\n",
    "pca_across_var = np.var(pca_results['pca_transformed'][:275], axis=0)\n",
    "print(pca_across_var.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6ec04e3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1158, 80, 8192])\n"
     ]
    }
   ],
   "source": [
    "print(activations['absurdist'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d8e550e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for 275 roles, shape is (377,)\n"
     ]
    }
   ],
   "source": [
    "pca_within_var = []\n",
    "pc1_within_var = []\n",
    "for role in activations:\n",
    "    role_scaled = pca_results['scaler'].transform(activations[role][:, layer, :].float().numpy())\n",
    "    role_pca = pca_results['pca'].transform(role_scaled)\n",
    "    pca_within_var.append(np.var(role_pca, axis=0))\n",
    "    pc1_within_var.append(np.var(role_pca[:, 0]))\n",
    "\n",
    "print(f\"for {len(pca_within_var)} roles, shape is {pca_within_var[0].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6d4565fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(377,)\n"
     ]
    }
   ],
   "source": [
    "mean_pca_within_var = np.array(pca_within_var).mean(axis=0)\n",
    "print(mean_pca_within_var.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a6776cca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ratio of pca_across_var / mean_pca_within_var is 0.34383073803712927\n"
     ]
    }
   ],
   "source": [
    "pca_ratio = pca_across_var.mean() / mean_pca_within_var.mean()\n",
    "print(f\"ratio of pca_across_var / mean_pca_within_var is {pca_ratio}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ffd5cc",
   "metadata": {},
   "source": [
    "### pc1 variance only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "77c857e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1098.9190452905918\n"
     ]
    }
   ],
   "source": [
    "pc1_across_var = np.var(pca_results['pca_transformed'][:275, 0])\n",
    "print(pc1_across_var)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bfc2e7bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "544.487320753733\n",
      "ratio of pc1_across_var / mean_pc1_within_var is 2.018263793120765\n"
     ]
    }
   ],
   "source": [
    "mean_pc1_within_var = np.array(pc1_within_var).mean()\n",
    "print(mean_pc1_within_var)\n",
    "\n",
    "pc1_ratio = pc1_across_var / mean_pc1_within_var\n",
    "print(f\"ratio of pc1_across_var / mean_pc1_within_var is {pc1_ratio}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b5eca2",
   "metadata": {},
   "source": [
    "## Conditional variance of role vectors based on distance from Assistant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9db03468",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([377, 8192])\n"
     ]
    }
   ],
   "source": [
    "role_vectors = torch.stack(pca_results['vectors']['pos_2'] + pca_results['vectors']['pos_3'])[:, layer, :]\n",
    "print(role_vectors.shape)\n",
    "\n",
    "pc1 = pca_results['pca_transformed'][:, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dw7pl9twyv",
   "metadata": {},
   "source": [
    "### Conditional variance in raw activation space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "191e6z46xkk",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "RAW ACTIVATION SPACE: Two-Group Comparison\n",
      "============================================================\n",
      "PC1 threshold: -25\n",
      "Assistant-like roles (PC1 < -25): 121 samples\n",
      "Roleplay roles (PC1 >= -25): 256 samples\n",
      "\n",
      "Mean variance (Assistant-like): 0.000248\n",
      "Mean variance (Roleplay): 0.000900\n",
      "Variance ratio (Assistant/Roleplay): 0.2754 (27.54%)\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import pearsonr\n",
    "\n",
    "# Two-group comparison: Assistant-like vs Roleplay\n",
    "# Using PC1 threshold of -25 (same as in 9_cone.ipynb)\n",
    "threshold = -25\n",
    "\n",
    "assistant_mask = pc1 < threshold\n",
    "roleplay_mask = pc1 >= threshold\n",
    "\n",
    "# Compute variance of raw activations for each group\n",
    "# role_vectors shape: [448, 4608]\n",
    "var_assistant_raw = torch.var(role_vectors[assistant_mask], dim=0).mean().item()\n",
    "var_roleplay_raw = torch.var(role_vectors[roleplay_mask], dim=0).mean().item()\n",
    "\n",
    "var_ratio_raw = var_assistant_raw / var_roleplay_raw\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"RAW ACTIVATION SPACE: Two-Group Comparison\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"PC1 threshold: {threshold}\")\n",
    "print(f\"Assistant-like roles (PC1 < {threshold}): {assistant_mask.sum()} samples\")\n",
    "print(f\"Roleplay roles (PC1 >= {threshold}): {roleplay_mask.sum()} samples\")\n",
    "print(f\"\\nMean variance (Assistant-like): {var_assistant_raw:.6f}\")\n",
    "print(f\"Mean variance (Roleplay): {var_roleplay_raw:.6f}\")\n",
    "print(f\"Variance ratio (Assistant/Roleplay): {var_ratio_raw:.4f} ({var_ratio_raw*100:.2f}%)\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "vm23njwgwk",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "RAW ACTIVATION SPACE (PC1 projected out): Two-Group Comparison\n",
      "============================================================\n",
      "PC1 threshold: -25\n",
      "Assistant-like roles (PC1 < -25): 121 samples\n",
      "Roleplay roles (PC1 >= -25): 256 samples\n",
      "\n",
      "Mean variance (Assistant-like, PC1 removed): 0.000246\n",
      "Mean variance (Roleplay, PC1 removed): 0.000806\n",
      "Variance ratio (Assistant/Roleplay): 0.3049 (30.49%)\n",
      "\n",
      "This is analogous to the PC2-10 analysis in PC space.\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Project out PC1 from raw activations\n",
    "# Get PC1 direction from PCA\n",
    "pc1_direction = torch.from_numpy(pca_results['pca'].components_[0]).float()\n",
    "\n",
    "# Project role_vectors onto PC1 and subtract\n",
    "# Formula: projection = (v · u) * u, where u is the unit vector (PC1 direction)\n",
    "pc1_loadings = (role_vectors.float() @ pc1_direction).unsqueeze(1)  # Shape: [448, 1]\n",
    "pc1_projections = pc1_loadings * pc1_direction.unsqueeze(0)  # Shape: [448, 4608]\n",
    "role_vectors_pc1_removed = role_vectors - pc1_projections\n",
    "\n",
    "# Compute variance with PC1 projected out\n",
    "var_assistant_raw_no_pc1 = torch.var(role_vectors_pc1_removed[assistant_mask], dim=0).mean().item()\n",
    "var_roleplay_raw_no_pc1 = torch.var(role_vectors_pc1_removed[roleplay_mask], dim=0).mean().item()\n",
    "\n",
    "var_ratio_raw_no_pc1 = var_assistant_raw_no_pc1 / var_roleplay_raw_no_pc1\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"RAW ACTIVATION SPACE (PC1 projected out): Two-Group Comparison\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"PC1 threshold: {threshold}\")\n",
    "print(f\"Assistant-like roles (PC1 < {threshold}): {assistant_mask.sum()} samples\")\n",
    "print(f\"Roleplay roles (PC1 >= {threshold}): {roleplay_mask.sum()} samples\")\n",
    "print(f\"\\nMean variance (Assistant-like, PC1 removed): {var_assistant_raw_no_pc1:.6f}\")\n",
    "print(f\"Mean variance (Roleplay, PC1 removed): {var_roleplay_raw_no_pc1:.6f}\")\n",
    "print(f\"Variance ratio (Assistant/Roleplay): {var_ratio_raw_no_pc1:.4f} ({var_ratio_raw_no_pc1*100:.2f}%)\")\n",
    "print(f\"\\nThis is analogous to the PC2-10 analysis in PC space.\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "zv1prpa1y3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "RAW ACTIVATION SPACE: Quintile Analysis\n",
      "============================================================\n",
      "\n",
      "Quintile 1: PC1 ∈ [-44.38, -31.56]\n",
      "  Sample size: 76\n",
      "  Mean variance (full): 0.000200\n",
      "  Mean variance (PC1 removed): 0.000200\n",
      "\n",
      "Quintile 2: PC1 ∈ [-31.56, -18.14]\n",
      "  Sample size: 75\n",
      "  Mean variance (full): 0.000309\n",
      "  Mean variance (PC1 removed): 0.000308\n",
      "\n",
      "Quintile 3: PC1 ∈ [-18.14, 2.47]\n",
      "  Sample size: 75\n",
      "  Mean variance (full): 0.000549\n",
      "  Mean variance (PC1 removed): 0.000546\n",
      "\n",
      "Quintile 4: PC1 ∈ [2.47, 31.74]\n",
      "  Sample size: 75\n",
      "  Mean variance (full): 0.000984\n",
      "  Mean variance (PC1 removed): 0.000981\n",
      "\n",
      "Quintile 5: PC1 ∈ [31.74, 101.90]\n",
      "  Sample size: 76\n",
      "  Mean variance (full): 0.001060\n",
      "  Mean variance (PC1 removed): 0.001030\n",
      "\n",
      "------------------------------------------------------------\n",
      "Variance ratio (Last/First quintile, full): 5.30x\n",
      "Variance ratio (Last/First quintile, PC1 removed): 5.16x\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Quintile analysis\n",
    "n_quintiles = 5\n",
    "quintile_edges = np.quantile(pc1, np.linspace(0, 1, n_quintiles + 1))\n",
    "quintile_variances = []\n",
    "quintile_variances_no_pc1 = []\n",
    "quintile_sizes = []\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"RAW ACTIVATION SPACE: Quintile Analysis\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i in range(n_quintiles):\n",
    "    if i == 0:\n",
    "        mask = (pc1 >= quintile_edges[i]) & (pc1 <= quintile_edges[i + 1])\n",
    "    else:\n",
    "        mask = (pc1 > quintile_edges[i]) & (pc1 <= quintile_edges[i + 1])\n",
    "    \n",
    "    quintile_var = torch.var(role_vectors[mask], dim=0).mean().item()\n",
    "    quintile_var_no_pc1 = torch.var(role_vectors_pc1_removed[mask], dim=0).mean().item()\n",
    "    quintile_variances.append(quintile_var)\n",
    "    quintile_variances_no_pc1.append(quintile_var_no_pc1)\n",
    "    quintile_sizes.append(mask.sum())\n",
    "    \n",
    "    print(f\"\\nQuintile {i+1}: PC1 ∈ [{quintile_edges[i]:.2f}, {quintile_edges[i+1]:.2f}]\")\n",
    "    print(f\"  Sample size: {mask.sum()}\")\n",
    "    print(f\"  Mean variance (full): {quintile_var:.6f}\")\n",
    "    print(f\"  Mean variance (PC1 removed): {quintile_var_no_pc1:.6f}\")\n",
    "\n",
    "# Calculate ratios between first and last quintile\n",
    "quintile_ratio = quintile_variances[-1] / quintile_variances[0]\n",
    "quintile_ratio_no_pc1 = quintile_variances_no_pc1[-1] / quintile_variances_no_pc1[0]\n",
    "\n",
    "print(\"\\n\" + \"-\" * 60)\n",
    "print(f\"Variance ratio (Last/First quintile, full): {quintile_ratio:.2f}x\")\n",
    "print(f\"Variance ratio (Last/First quintile, PC1 removed): {quintile_ratio_no_pc1:.2f}x\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d7kwaevse0w",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "RAW ACTIVATION SPACE: Distance from Center Correlation\n",
      "============================================================\n",
      "Correlation between PC1 and L2 distance from mean (full):\n",
      "  r = 0.7185\n",
      "  p-value = 4.150e-61\n",
      "  Highly significant (p < 0.001)\n",
      "\n",
      "Correlation between PC1 and L2 distance from mean (PC1 removed):\n",
      "  r = 0.6382\n",
      "  p-value = 1.639e-44\n",
      "  Highly significant (p < 0.001)\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Distance from center correlation\n",
    "# Compute mean of raw activations\n",
    "role_vectors_mean = role_vectors.mean(dim=0)\n",
    "role_vectors_pc1_removed_mean = role_vectors_pc1_removed.mean(dim=0)\n",
    "\n",
    "# Compute L2 distance from mean for each role\n",
    "distances_raw = torch.norm(role_vectors.float() - role_vectors_mean, p=2, dim=1).numpy()\n",
    "distances_raw_no_pc1 = torch.norm(role_vectors_pc1_removed - role_vectors_pc1_removed_mean, p=2, dim=1).numpy()\n",
    "\n",
    "# Calculate correlation with PC1\n",
    "correlation_raw, p_value_raw = pearsonr(pc1, distances_raw)\n",
    "correlation_raw_no_pc1, p_value_raw_no_pc1 = pearsonr(pc1, distances_raw_no_pc1)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"RAW ACTIVATION SPACE: Distance from Center Correlation\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Correlation between PC1 and L2 distance from mean (full):\")\n",
    "print(f\"  r = {correlation_raw:.4f}\")\n",
    "print(f\"  p-value = {p_value_raw:.3e}\")\n",
    "if p_value_raw < 0.001:\n",
    "    print(f\"  Highly significant (p < 0.001)\")\n",
    "elif p_value_raw < 0.05:\n",
    "    print(f\"  Significant (p < 0.05)\")\n",
    "\n",
    "print(f\"\\nCorrelation between PC1 and L2 distance from mean (PC1 removed):\")\n",
    "print(f\"  r = {correlation_raw_no_pc1:.4f}\")\n",
    "print(f\"  p-value = {p_value_raw_no_pc1:.3e}\")\n",
    "if p_value_raw_no_pc1 < 0.001:\n",
    "    print(f\"  Highly significant (p < 0.001)\")\n",
    "elif p_value_raw_no_pc1 < 0.05:\n",
    "    print(f\"  Significant (p < 0.05)\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lsye2ajp11",
   "metadata": {},
   "source": [
    "### Per-PC analysis: Correlation between each PC and distance in remaining PC space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0tedpgpspjp",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Correlation between each PC and distance in remaining PC space\n",
      "======================================================================\n",
      "PC 1: r =  0.6549, p = 1.521e-47 ***\n",
      "PC 2: r =  0.4304, p = 1.981e-18 ***\n",
      "PC 3: r =  0.2705, p = 9.594e-08 ***\n",
      "PC 4: r =  0.1602, p = 1.802e-03 **\n",
      "PC 5: r = -0.0537, p = 2.987e-01 \n",
      "PC 6: r =  0.1110, p = 3.114e-02 *\n",
      "PC 7: r =  0.1095, p = 3.348e-02 *\n",
      "PC 8: r =  0.0153, p = 7.668e-01 \n",
      "PC 9: r =  0.0751, p = 1.456e-01 \n",
      "PC10: r =  0.1433, p = 5.316e-03 **\n",
      "======================================================================\n",
      "\n",
      "PC1 correlation: 0.6549\n",
      "Mean correlation (PC2-10): 0.1402\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# For each of the top 10 PCs, calculate:\n",
    "# 1. The correlation between that PC and distance from center in all OTHER PCs\n",
    "# 2. This tells us if the pattern we see with PC1 generalizes to other PCs\n",
    "\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "n_pcs_to_analyze = 10\n",
    "pca_transformed = pca_results['pca_transformed']\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"Correlation between each PC and distance in remaining PC space\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "correlations = []\n",
    "p_values = []\n",
    "\n",
    "for pc_idx in range(n_pcs_to_analyze):\n",
    "    # Get the PC values\n",
    "    pc_values = pca_transformed[:, pc_idx]\n",
    "    \n",
    "    # Get all other PCs (excluding current PC)\n",
    "    other_pcs = np.delete(pca_transformed, pc_idx, axis=1)\n",
    "    \n",
    "    # Calculate distance from center in the remaining PC space\n",
    "    other_pcs_mean = other_pcs.mean(axis=0)\n",
    "    distances = np.linalg.norm(other_pcs - other_pcs_mean, axis=1)\n",
    "    \n",
    "    # Calculate correlation\n",
    "    corr, p_val = pearsonr(pc_values, distances)\n",
    "    correlations.append(corr)\n",
    "    p_values.append(p_val)\n",
    "    \n",
    "    # Print results\n",
    "    sig_marker = \"***\" if p_val < 0.001 else \"**\" if p_val < 0.01 else \"*\" if p_val < 0.05 else \"\"\n",
    "    print(f\"PC{pc_idx+1:2d}: r = {corr:7.4f}, p = {p_val:.3e} {sig_marker}\")\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nPC1 correlation: {correlations[0]:.4f}\")\n",
    "print(f\"Mean correlation (PC2-10): {np.mean(correlations[1:]):.4f}\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4w4stk4a567",
   "metadata": {},
   "source": [
    "### Conditional variance in PC2-10 based on position along each PC\n",
    "\n",
    "This analysis shows whether the pattern of \"extreme positions → high variance in other PCs\" is unique to PC1 or generalizes to other PCs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "x42qo16zp6k",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Conditional Variance in PC2-10 based on position along each PC\n",
      "================================================================================\n",
      "For each PC, we split roles by median and compute variance in PC2-10 (excluding that PC)\n",
      "--------------------------------------------------------------------------------\n",
      "PC 1: High=188 samples, Low=189 samples\n",
      "      Var(high) =  562.709, Var(low) =  129.801, Ratio = 4.335\n",
      "PC 2: High=188 samples, Low=189 samples\n",
      "      Var(high) =  412.558, Var(low) =  173.262, Ratio = 2.381\n",
      "PC 3: High=188 samples, Low=189 samples\n",
      "      Var(high) =  350.552, Var(low) =  307.318, Ratio = 1.141\n",
      "PC 4: High=188 samples, Low=189 samples\n",
      "      Var(high) =  395.870, Var(low) =  277.716, Ratio = 1.425\n",
      "PC 5: High=188 samples, Low=189 samples\n",
      "      Var(high) =  312.331, Var(low) =  388.396, Ratio = 1.244\n",
      "PC 6: High=188 samples, Low=189 samples\n",
      "      Var(high) =  391.404, Var(low) =  326.078, Ratio = 1.200\n",
      "PC 7: High=188 samples, Low=189 samples\n",
      "      Var(high) =  381.977, Var(low) =  343.582, Ratio = 1.112\n",
      "PC 8: High=188 samples, Low=189 samples\n",
      "      Var(high) =  386.625, Var(low) =  350.595, Ratio = 1.103\n",
      "PC 9: High=188 samples, Low=189 samples\n",
      "      Var(high) =  447.339, Var(low) =  294.417, Ratio = 1.519\n",
      "PC10: High=188 samples, Low=189 samples\n",
      "      Var(high) =  348.690, Var(low) =  395.093, Ratio = 1.133\n",
      "================================================================================\n",
      "\n",
      "Summary:\n",
      "  PC1 variance ratio: 4.335\n",
      "  Mean variance ratio for PC2-10: 1.362\n",
      "  Max variance ratio (excluding PC1): 2.381 (PC2)\n",
      "\n",
      "  → Shows whether PC1 is unique in having high-variance 'other dimensions' for extreme positions\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# For each PC, split roles into two groups (high/low) and compute variance in PC2-10 (excluding that PC)\n",
    "# This tests if extreme positions on PC_i lead to high variance in other PCs\n",
    "\n",
    "n_pcs_to_test = 10\n",
    "pca_transformed = pca_results['pca_transformed']\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"Conditional Variance in PC2-10 based on position along each PC\")\n",
    "print(\"=\" * 80)\n",
    "print(\"For each PC, we split roles by median and compute variance in PC2-10 (excluding that PC)\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "variance_ratios = []\n",
    "\n",
    "for pc_idx in range(n_pcs_to_test):\n",
    "    # Split by median on this PC\n",
    "    pc_values = pca_transformed[:, pc_idx]\n",
    "    median_val = np.median(pc_values)\n",
    "    high_mask = pc_values > median_val\n",
    "    low_mask = pc_values <= median_val\n",
    "    \n",
    "    # Get PC2-10, excluding current PC if it's in that range\n",
    "    if pc_idx == 0:\n",
    "        # For PC1, we want variance in PC2-10\n",
    "        other_pcs = pca_transformed[:, 1:10]\n",
    "    elif 1 <= pc_idx < 10:\n",
    "        # For PC2-9, exclude that PC from PC2-10\n",
    "        pc_indices = [i for i in range(1, 10) if i != pc_idx]\n",
    "        other_pcs = pca_transformed[:, pc_indices]\n",
    "    else:\n",
    "        # For PC10, use PC2-9\n",
    "        other_pcs = pca_transformed[:, 1:10]\n",
    "    \n",
    "    # Compute variance for each group\n",
    "    var_high = np.var(other_pcs[high_mask], axis=0).mean()\n",
    "    var_low = np.var(other_pcs[low_mask], axis=0).mean()\n",
    "    \n",
    "    ratio = max(var_high, var_low) / min(var_high, var_low)\n",
    "    variance_ratios.append(ratio)\n",
    "    \n",
    "    print(f\"PC{pc_idx+1:2d}: High={high_mask.sum():3d} samples, Low={low_mask.sum():3d} samples\")\n",
    "    print(f\"      Var(high) = {var_high:8.3f}, Var(low) = {var_low:8.3f}, Ratio = {ratio:.3f}\")\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nSummary:\")\n",
    "print(f\"  PC1 variance ratio: {variance_ratios[0]:.3f}\")\n",
    "print(f\"  Mean variance ratio for PC2-10: {np.mean(variance_ratios[1:]):.3f}\")\n",
    "print(f\"  Max variance ratio (excluding PC1): {np.max(variance_ratios[1:]):.3f} (PC{np.argmax(variance_ratios[1:])+2})\")\n",
    "print(\"\\n  → Shows whether PC1 is unique in having high-variance 'other dimensions' for extreme positions\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "psa4fpzrfl8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total roles: 377\n",
      "pca_transformed shape: (377, 377)\n"
     ]
    }
   ],
   "source": [
    "# Create role labels from pca_results\n",
    "def get_role_labels_from_pca(pca_results):\n",
    "    labels = []\n",
    "    if 'pos_2' in pca_results['roles'].keys():\n",
    "        pos_2_roles = [role.replace('_', ' ').title() for role in pca_results['roles']['pos_2']]\n",
    "        labels.extend(pos_2_roles)\n",
    "    if 'pos_3' in pca_results['roles'].keys():\n",
    "        pos_3_roles = [role.replace('_', ' ').title() for role in pca_results['roles']['pos_3']]\n",
    "        labels.extend(pos_3_roles)\n",
    "    return labels\n",
    "\n",
    "role_labels = get_role_labels_from_pca(pca_results)\n",
    "print(f\"Total roles: {len(role_labels)}\")\n",
    "print(f\"pca_transformed shape: {pca_transformed.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d0ogly4i9nk",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Top and Bottom Roles for Each PC\n",
      "================================================================================\n",
      "\n",
      "PC1:\n",
      "  Top 5 (highest loadings):\n",
      "    1. Leviathan                      (PC1 =  101.90)\n",
      "    2. Eldritch                       (PC1 =  100.75)\n",
      "    3. Void                           (PC1 =   96.26)\n",
      "    4. Wraith                         (PC1 =   85.32)\n",
      "    5. Tree                           (PC1 =   82.25)\n",
      "  Bottom 5 (lowest loadings):\n",
      "    1. Evaluator                      (PC1 =  -44.38)\n",
      "    2. Coach                          (PC1 =  -41.81)\n",
      "    3. Mentor                         (PC1 =  -41.67)\n",
      "    4. Planner                        (PC1 =  -41.62)\n",
      "    5. Moderator                      (PC1 =  -41.14)\n",
      "\n",
      "PC2:\n",
      "  Top 5 (highest loadings):\n",
      "    1. Toddler                        (PC2 =  106.64)\n",
      "    2. Procrastinator                 (PC2 =  104.63)\n",
      "    3. Adolescent                     (PC2 =  100.52)\n",
      "    4. Teenager                       (PC2 =  100.21)\n",
      "    5. Fool                           (PC2 =   95.96)\n",
      "  Bottom 5 (lowest loadings):\n",
      "    1. Eldritch                       (PC2 =  -57.39)\n",
      "    2. Crystalline                    (PC2 =  -56.68)\n",
      "    3. Spirit                         (PC2 =  -55.46)\n",
      "    4. Avatar                         (PC2 =  -52.08)\n",
      "    5. Oracle                         (PC2 =  -49.31)\n",
      "\n",
      "PC3:\n",
      "  Top 5 (highest loadings):\n",
      "    1. Infant                         (PC3 =  141.64)\n",
      "    2. Toddler                        (PC3 =  104.95)\n",
      "    3. Caveman                        (PC3 =   92.31)\n",
      "    4. Void                           (PC3 =   62.48)\n",
      "    5. Robot                          (PC3 =   52.75)\n",
      "  Bottom 5 (lowest loadings):\n",
      "    1. Idealist                       (PC3 =  -54.37)\n",
      "    2. Bohemian                       (PC3 =  -51.98)\n",
      "    3. Romantic                       (PC3 =  -50.00)\n",
      "    4. Pacifist                       (PC3 =  -49.51)\n",
      "    5. Pilgrim                        (PC3 =  -46.51)\n",
      "\n",
      "PC4:\n",
      "  Top 5 (highest loadings):\n",
      "    1. Infant                         (PC4 =  101.85)\n",
      "    2. Toddler                        (PC4 =   98.46)\n",
      "    3. Caveman                        (PC4 =   61.74)\n",
      "    4. Fool                           (PC4 =   47.20)\n",
      "    5. Amnesiac                       (PC4 =   45.49)\n",
      "  Bottom 5 (lowest loadings):\n",
      "    1. Provocateur                    (PC4 =  -69.93)\n",
      "    2. Cynic                          (PC4 =  -66.01)\n",
      "    3. Provocateur                    (PC4 =  -59.46)\n",
      "    4. Rebel                          (PC4 =  -57.72)\n",
      "    5. Competitor                     (PC4 =  -55.06)\n",
      "\n",
      "PC5:\n",
      "  Top 5 (highest loadings):\n",
      "    1. Smuggler                       (PC5 =   55.72)\n",
      "    2. Coral Reef                     (PC5 =   48.23)\n",
      "    3. Soldier                        (PC5 =   48.00)\n",
      "    4. Surfer                         (PC5 =   47.93)\n",
      "    5. Mechanic                       (PC5 =   45.65)\n",
      "  Bottom 5 (lowest loadings):\n",
      "    1. Absurdist                      (PC5 =  -54.39)\n",
      "    2. Infant                         (PC5 =  -52.09)\n",
      "    3. Provocateur                    (PC5 =  -51.20)\n",
      "    4. Provocateur                    (PC5 =  -44.66)\n",
      "    5. Contrarian                     (PC5 =  -43.24)\n",
      "\n",
      "PC6:\n",
      "  Top 5 (highest loadings):\n",
      "    1. Infant                         (PC6 =   95.60)\n",
      "    2. Toddler                        (PC6 =   93.20)\n",
      "    3. Caveman                        (PC6 =   49.96)\n",
      "    4. Fool                           (PC6 =   40.07)\n",
      "    5. Anarchist                      (PC6 =   34.24)\n",
      "  Bottom 5 (lowest loadings):\n",
      "    1. Presenter                      (PC6 =  -44.26)\n",
      "    2. Negotiator                     (PC6 =  -40.77)\n",
      "    3. Vampire                        (PC6 =  -40.33)\n",
      "    4. Demon                          (PC6 =  -40.00)\n",
      "    5. Emissary                       (PC6 =  -38.91)\n",
      "\n",
      "PC7:\n",
      "  Top 5 (highest loadings):\n",
      "    1. Caveman                        (PC7 =   59.66)\n",
      "    2. Zealot                         (PC7 =   57.27)\n",
      "    3. Ascetic                        (PC7 =   55.84)\n",
      "    4. Infant                         (PC7 =   50.24)\n",
      "    5. Luddite                        (PC7 =   50.17)\n",
      "  Bottom 5 (lowest loadings):\n",
      "    1. Comedian                       (PC7 =  -51.71)\n",
      "    2. Absurdist                      (PC7 =  -49.94)\n",
      "    3. Jester                         (PC7 =  -46.66)\n",
      "    4. Trickster                      (PC7 =  -46.46)\n",
      "    5. Improviser                     (PC7 =  -46.27)\n",
      "\n",
      "PC8:\n",
      "  Top 5 (highest loadings):\n",
      "    1. Caveman                        (PC8 =   59.47)\n",
      "    2. Pirate                         (PC8 =   49.67)\n",
      "    3. Warrior                        (PC8 =   38.08)\n",
      "    4. Zealot                         (PC8 =   35.73)\n",
      "    5. Shaman                         (PC8 =   35.49)\n",
      "  Bottom 5 (lowest loadings):\n",
      "    1. Void                           (PC8 =  -52.67)\n",
      "    2. Amnesiac                       (PC8 =  -51.47)\n",
      "    3. Simulacrum                     (PC8 =  -39.32)\n",
      "    4. Procrastinator                 (PC8 =  -37.94)\n",
      "    5. Loner                          (PC8 =  -37.26)\n",
      "\n",
      "PC9:\n",
      "  Top 5 (highest loadings):\n",
      "    1. Anarchist                      (PC9 =   51.38)\n",
      "    2. Caveman                        (PC9 =   45.09)\n",
      "    3. Rebel                          (PC9 =   41.13)\n",
      "    4. Mycorrhizal                    (PC9 =   40.85)\n",
      "    5. Revolutionary                  (PC9 =   38.39)\n",
      "  Bottom 5 (lowest loadings):\n",
      "    1. Hoarder                        (PC9 =  -54.58)\n",
      "    2. Procrastinator                 (PC9 =  -50.80)\n",
      "    3. Comedian                       (PC9 =  -38.69)\n",
      "    4. Narcissist                     (PC9 =  -34.20)\n",
      "    5. Traditionalist                 (PC9 =  -32.22)\n",
      "\n",
      "PC10:\n",
      "  Top 5 (highest loadings):\n",
      "    1. Narcissist                     (PC10 =   71.49)\n",
      "    2. Workaholic                     (PC10 =   54.00)\n",
      "    3. Zealot                         (PC10 =   48.85)\n",
      "    4. Hoarder                        (PC10 =   41.42)\n",
      "    5. Mycorrhizal                    (PC10 =   41.38)\n",
      "  Bottom 5 (lowest loadings):\n",
      "    1. Prisoner                       (PC10 =  -38.98)\n",
      "    2. Criminal                       (PC10 =  -34.54)\n",
      "    3. Exile                          (PC10 =  -33.60)\n",
      "    4. Caveman                        (PC10 =  -28.34)\n",
      "    5. Amnesiac                       (PC10 =  -26.99)\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Show top/bottom roles for each PC\n",
    "n_pcs_to_show = 10  # Show first 5 PCs\n",
    "n_roles_to_show = 5  # Show top/bottom 5 roles\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"Top and Bottom Roles for Each PC\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for pc_idx in range(n_pcs_to_show):\n",
    "    pc_values = pca_transformed[:, pc_idx]\n",
    "    \n",
    "    # Get indices of top and bottom roles\n",
    "    top_indices = np.argsort(pc_values)[-n_roles_to_show:][::-1]\n",
    "    bottom_indices = np.argsort(pc_values)[:n_roles_to_show]\n",
    "    \n",
    "    print(f\"\\nPC{pc_idx+1}:\")\n",
    "    print(f\"  Top {n_roles_to_show} (highest loadings):\")\n",
    "    for i, idx in enumerate(top_indices):\n",
    "        print(f\"    {i+1}. {role_labels[idx]:30s} (PC{pc_idx+1} = {pc_values[idx]:7.2f})\")\n",
    "    \n",
    "    print(f\"  Bottom {n_roles_to_show} (lowest loadings):\")\n",
    "    for i, idx in enumerate(bottom_indices):\n",
    "        print(f\"    {i+1}. {role_labels[idx]:30s} (PC{pc_idx+1} = {pc_values[idx]:7.2f})\")\n",
    "\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c4a7bf",
   "metadata": {},
   "source": [
    "## Correlations between role loadings onto PCs across the 3 models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "65e2c607",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/git/persona-subspace/.venv/lib/python3.12/site-packages/sklearn/base.py:442: InconsistentVersionWarning: Trying to unpickle estimator PCA from version 1.7.0 when using version 1.7.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/root/git/persona-subspace/.venv/lib/python3.12/site-packages/sklearn/base.py:442: InconsistentVersionWarning: Trying to unpickle estimator StandardScaler from version 1.7.0 when using version 1.7.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(239, 239)\n",
      "['zealous', 'wry', 'witty', 'whimsical', 'visceral', 'verbose', 'utilitarian', 'urgent', 'universalist', 'understated', 'transparent', 'traditional', 'theoretical', 'theatrical', 'temperamental', 'technical', 'tactful', 'systems_thinker', 'sycophantic', 'supportive']\n",
      "(240, 240)\n",
      "['zealous', 'wry', 'witty', 'whimsical', 'visceral', 'vindictive', 'verbose', 'utilitarian', 'urgent', 'universalist', 'understated', 'transparent', 'traditional', 'theoretical', 'theatrical', 'temperamental', 'technical', 'tactful', 'systems_thinker', 'sycophantic']\n",
      "(240, 240)\n",
      "['zealous', 'wry', 'witty', 'whimsical', 'visceral', 'vindictive', 'verbose', 'utilitarian', 'urgent', 'universalist', 'understated', 'transparent', 'traditional', 'theoretical', 'theatrical', 'temperamental', 'technical', 'tactful', 'systems_thinker', 'sycophantic']\n",
      "(239, 239)\n",
      "(239, 240)\n",
      "(239, 240)\n"
     ]
    }
   ],
   "source": [
    "models = ['gemma-2-27b', 'qwen-3-32b', 'llama-3.3-70b']\n",
    "layers = [22, 32, 40]\n",
    "\n",
    "trait_results = {}\n",
    "labels = {}\n",
    "for model, layer in zip(models, layers):\n",
    "    model_dir = f\"/workspace/{model}/traits_240\"\n",
    "    trait_results[model] = torch.load(f\"{model_dir}/pca/layer{layer}_pos-neg50.pt\", weights_only=False)\n",
    "    print(trait_results[model]['pca_transformed'].shape)\n",
    "    labels[model] = trait_results[model]['traits']['pos_neg_50']\n",
    "    print(labels[model][:20])\n",
    "\n",
    "# need to get intersection of traits across models (gemma missing vindictive)\n",
    "pca_transformed = []\n",
    "for model in models:\n",
    "    if model != 'gemma-2-27b':\n",
    "        # splice out index 5 but keep the ones before and after\n",
    "        pca_transformed.append(np.concatenate((trait_results[model]['pca_transformed'][:5], trait_results[model]['pca_transformed'][6:])))\n",
    "    else:\n",
    "        pca_transformed.append(trait_results[model]['pca_transformed'])\n",
    "\n",
    "for m in pca_transformed:\n",
    "    print(m.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c83e2c1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transposed shapes (n_pcs, n_traits):\n",
      "gemma-2-27b: (6, 239)\n",
      "qwen-3-32b: (6, 239)\n",
      "llama-3.3-70b: (6, 239)\n",
      "\n",
      "PC1:\n",
      "  Gemma ↔ Qwen:  -0.8940\n",
      "  Gemma ↔ Llama:  0.9690\n",
      "  Qwen  ↔ Llama: -0.8359\n",
      "\n",
      "PC2:\n",
      "  Gemma ↔ Qwen:   0.8356\n",
      "  Gemma ↔ Llama: -0.9079\n",
      "  Qwen  ↔ Llama: -0.8064\n",
      "\n",
      "PC3:\n",
      "  Gemma ↔ Qwen:   0.7499\n",
      "  Gemma ↔ Llama: -0.9005\n",
      "  Qwen  ↔ Llama: -0.8542\n",
      "\n",
      "PC4:\n",
      "  Gemma ↔ Qwen:   0.6075\n",
      "  Gemma ↔ Llama:  0.6253\n",
      "  Qwen  ↔ Llama:  0.4756\n",
      "\n",
      "PC5:\n",
      "  Gemma ↔ Qwen:   0.3510\n",
      "  Gemma ↔ Llama:  0.5943\n",
      "  Qwen  ↔ Llama:  0.5252\n",
      "\n",
      "PC6:\n",
      "  Gemma ↔ Qwen:  -0.2080\n",
      "  Gemma ↔ Llama: -0.7845\n",
      "  Qwen  ↔ Llama: -0.3113\n"
     ]
    }
   ],
   "source": [
    "# Transpose each matrix so rows are PCs and columns are traits\n",
    "pca_transposed = [m.T for m in pca_transformed]\n",
    "\n",
    "# Extract top 10 PCs from each model\n",
    "n_pcs = 6\n",
    "top_pcs = [m[:n_pcs] for m in pca_transposed]\n",
    "\n",
    "print(f\"Transposed shapes (n_pcs, n_traits):\")\n",
    "for model, pc_matrix in zip(models, top_pcs):\n",
    "    print(f\"{model}: {pc_matrix.shape}\")\n",
    "\n",
    "# Compute pairwise correlations for each PC\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "pc_correlations = []\n",
    "for pc_idx in range(n_pcs):\n",
    "    # Extract the trait loading vector for this PC from each model\n",
    "    gemma_pc = top_pcs[0][pc_idx]\n",
    "    qwen_pc = top_pcs[1][pc_idx]\n",
    "    llama_pc = top_pcs[2][pc_idx]\n",
    "    \n",
    "    # Compute pairwise correlations\n",
    "    corr_gemma_qwen, _ = pearsonr(gemma_pc, qwen_pc)\n",
    "    corr_gemma_llama, _ = pearsonr(gemma_pc, llama_pc)\n",
    "    corr_qwen_llama, _ = pearsonr(qwen_pc, llama_pc)\n",
    "    \n",
    "    # Create 3x3 correlation matrix\n",
    "    corr_matrix = np.array([\n",
    "        [1.0, corr_gemma_qwen, corr_gemma_llama],\n",
    "        [corr_gemma_qwen, 1.0, corr_qwen_llama],\n",
    "        [corr_gemma_llama, corr_qwen_llama, 1.0]\n",
    "    ])\n",
    "    \n",
    "    pc_correlations.append(corr_matrix)\n",
    "\n",
    "    print(f\"\\nPC{pc_idx + 1}:\")\n",
    "    print(f\"  Gemma ↔ Qwen:  {corr_gemma_qwen:7.4f}\")\n",
    "    print(f\"  Gemma ↔ Llama: {corr_gemma_llama:7.4f}\")\n",
    "    print(f\"  Qwen  ↔ Llama: {corr_qwen_llama:7.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e6f52212",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(448, 448)\n",
      "(463, 463)\n",
      "(377, 377)\n"
     ]
    }
   ],
   "source": [
    "# try for top 10 role PCs\n",
    "models = ['gemma-2-27b', 'qwen-3-32b', 'llama-3.3-70b']\n",
    "layers = [22, 32, 40]\n",
    "\n",
    "def get_role_labels(pca_results):\n",
    "    labels = []\n",
    "    if 'pos_2' in pca_results['roles'].keys():\n",
    "        pos_2_roles = [role.replace('_', ' ').title() for role in pca_results['roles']['pos_2']]\n",
    "        pos_2_roles = [f\"{role} (Somewhat RP)\" for role in pos_2_roles]\n",
    "        labels.extend(pos_2_roles)\n",
    "    if 'pos_3' in pca_results['roles'].keys():\n",
    "        pos_3_roles = [role.replace('_', ' ').title() for role in pca_results['roles']['pos_3']]\n",
    "        pos_3_roles = [f\"{role} (Fully RP)\" for role in pos_3_roles]\n",
    "        labels.extend(pos_3_roles)\n",
    "    return labels\n",
    "\n",
    "\n",
    "role_results = {}\n",
    "labels = {}\n",
    "for model, layer in zip(models, layers):\n",
    "    model_dir = f\"/workspace/{model}/roles_240\"\n",
    "    role_results[model] = torch.load(f\"{model_dir}/pca/layer{layer}_pos23.pt\", weights_only=False)\n",
    "    print(role_results[model]['pca_transformed'].shape)\n",
    "    labels[model] = get_role_labels(role_results[model])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5faf8b0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Common roles across all models: 361\n",
      "gemma-2-27b: 361 common roles\n",
      "qwen-3-32b: 361 common roles\n",
      "llama-3.3-70b: 361 common roles\n",
      "gemma-2-27b aligned shape: (361, 448)\n",
      "qwen-3-32b aligned shape: (361, 463)\n",
      "llama-3.3-70b aligned shape: (361, 377)\n",
      "\n",
      "Transposed shapes (n_pcs, n_common_roles):\n",
      "gemma-2-27b: (6, 361)\n",
      "qwen-3-32b: (6, 361)\n",
      "llama-3.3-70b: (6, 361)\n",
      "\n",
      "PC1:\n",
      "  Gemma ↔ Qwen:   0.9633\n",
      "  Gemma ↔ Llama:  0.9530\n",
      "  Qwen  ↔ Llama:  0.9723\n",
      "\n",
      "PC2:\n",
      "  Gemma ↔ Qwen:   0.9284\n",
      "  Gemma ↔ Llama:  0.9121\n",
      "  Qwen  ↔ Llama:  0.9176\n",
      "\n",
      "PC3:\n",
      "  Gemma ↔ Qwen:   0.3108\n",
      "  Gemma ↔ Llama:  0.7327\n",
      "  Qwen  ↔ Llama:  0.6831\n",
      "\n",
      "PC4:\n",
      "  Gemma ↔ Qwen:   0.6476\n",
      "  Gemma ↔ Llama: -0.8664\n",
      "  Qwen  ↔ Llama: -0.6006\n",
      "\n",
      "PC5:\n",
      "  Gemma ↔ Qwen:   0.6709\n",
      "  Gemma ↔ Llama:  0.8028\n",
      "  Qwen  ↔ Llama:  0.7283\n",
      "\n",
      "PC6:\n",
      "  Gemma ↔ Qwen:   0.2149\n",
      "  Gemma ↔ Llama: -0.5411\n",
      "  Qwen  ↔ Llama: -0.1034\n"
     ]
    }
   ],
   "source": [
    "# Find intersection of roles across all 3 models\n",
    "set_gemma = set(labels['gemma-2-27b'])\n",
    "set_qwen = set(labels['qwen-3-32b'])\n",
    "set_llama = set(labels['llama-3.3-70b'])\n",
    "\n",
    "common_roles = set_gemma & set_qwen & set_llama\n",
    "print(f\"Common roles across all models: {len(common_roles)}\")\n",
    "\n",
    "# Get indices of common roles for each model (preserving order from labels)\n",
    "indices = {}\n",
    "for model in models:\n",
    "    model_indices = []\n",
    "    for i, role in enumerate(labels[model]):\n",
    "        if role in common_roles:\n",
    "            model_indices.append(i)\n",
    "    indices[model] = model_indices\n",
    "    print(f\"{model}: {len(model_indices)} common roles\")\n",
    "\n",
    "# Extract aligned PCA transformed matrices (only common roles, in consistent order)\n",
    "# Need to ensure the same role ordering across models\n",
    "common_roles_list = sorted(list(common_roles))  # Consistent ordering\n",
    "\n",
    "pca_transformed_roles = []\n",
    "for model in models:\n",
    "    # Map from common_roles_list order to model's indices\n",
    "    model_indices_ordered = []\n",
    "    for role in common_roles_list:\n",
    "        idx = labels[model].index(role)\n",
    "        model_indices_ordered.append(idx)\n",
    "    \n",
    "    # Extract rows for common roles in the standardized order\n",
    "    pca_transformed_roles.append(role_results[model]['pca_transformed'][model_indices_ordered])\n",
    "    print(f\"{model} aligned shape: {pca_transformed_roles[-1].shape}\")\n",
    "\n",
    "# Transpose each matrix so rows are PCs and columns are roles\n",
    "pca_transposed_roles = [m.T for m in pca_transformed_roles]\n",
    "\n",
    "# Extract top 10 PCs from each model\n",
    "n_pcs = 6\n",
    "top_pcs_roles = [m[:n_pcs] for m in pca_transposed_roles]\n",
    "\n",
    "print(f\"\\nTransposed shapes (n_pcs, n_common_roles):\")\n",
    "for model, pc_matrix in zip(models, top_pcs_roles):\n",
    "    print(f\"{model}: {pc_matrix.shape}\")\n",
    "\n",
    "# Compute pairwise correlations for each PC\n",
    "pc_correlations_roles = []\n",
    "for pc_idx in range(n_pcs):\n",
    "    # Extract the role loading vector for this PC from each model\n",
    "    gemma_pc = top_pcs_roles[0][pc_idx]\n",
    "    qwen_pc = top_pcs_roles[1][pc_idx]\n",
    "    llama_pc = top_pcs_roles[2][pc_idx]\n",
    "    \n",
    "    # Compute pairwise correlations\n",
    "    corr_gemma_qwen, _ = pearsonr(gemma_pc, qwen_pc)\n",
    "    corr_gemma_llama, _ = pearsonr(gemma_pc, llama_pc)\n",
    "    corr_qwen_llama, _ = pearsonr(qwen_pc, llama_pc)\n",
    "    \n",
    "    # Create 3x3 correlation matrix\n",
    "    corr_matrix = np.array([\n",
    "        [1.0, corr_gemma_qwen, corr_gemma_llama],\n",
    "        [corr_gemma_qwen, 1.0, corr_qwen_llama],\n",
    "        [corr_gemma_llama, corr_qwen_llama, 1.0]\n",
    "    ])\n",
    "    \n",
    "    pc_correlations_roles.append(corr_matrix)\n",
    "\n",
    "    print(f\"\\nPC{pc_idx + 1}:\")\n",
    "    print(f\"  Gemma ↔ Qwen:  {corr_gemma_qwen:7.4f}\")\n",
    "    print(f\"  Gemma ↔ Llama: {corr_gemma_llama:7.4f}\")\n",
    "    print(f\"  Qwen  ↔ Llama: {corr_qwen_llama:7.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb0108b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3pk6ipumh22",
   "metadata": {},
   "source": [
    "## Save variance analysis results to JSON"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cn2l16clkdg",
   "metadata": {},
   "source": [
    "### Instructions for saving variance analysis results\n",
    "\n",
    "This section saves the variance analysis results to JSON files for easy sharing and analysis.\n",
    "\n",
    "**Per-model files** (one per model):\n",
    "- `{model_name}_layer{layer}.json` - Contains all variance metrics for a single model:\n",
    "  - `across_within_role_var`: 4 variance ratios (raw, normalized, PCA space, PC1 only)\n",
    "  - `conditional_var_roles`: Threshold analysis, quintile analysis, distance correlations\n",
    "  - `high_var_pc_correlation`: PC distance correlations, conditional variance by PC, top/bottom roles\n",
    "\n",
    "**Cross-model file** (one file for all models):\n",
    "- `cross_model_loadings.json` - Contains PC correlation analysis across 3 models:\n",
    "  - `trait_analysis`: PC correlations for traits across Gemma, Qwen, Llama\n",
    "  - `role_analysis`: PC correlations for roles across Gemma, Qwen, Llama\n",
    "\n",
    "**To generate all files:**\n",
    "1. Run the notebook with one model configuration (e.g., Gemma)\n",
    "2. Change the configuration cell to another model (e.g., Qwen)\n",
    "3. Re-run from the configuration cell onwards\n",
    "4. Repeat for the third model (e.g., Llama)\n",
    "5. The cross-model file is generated after all 3 model sections are run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0pec8iqx20qe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving variance analysis results to ./results/\n",
      "Timestamp: 2025-10-21T23:40:50.577644\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# Configuration for saving\n",
    "outdir = \"./results\"\n",
    "os.makedirs(outdir, exist_ok=True)\n",
    "\n",
    "# Get current timestamp\n",
    "timestamp = datetime.now().isoformat()\n",
    "\n",
    "print(f\"Saving variance analysis results to {outdir}/\")\n",
    "print(f\"Timestamp: {timestamp}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3mzx2uw37it",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built per-model variance analysis data structure\n"
     ]
    }
   ],
   "source": [
    "# Build the per-model variance analysis JSON structure\n",
    "\n",
    "# Build quintile data\n",
    "quintiles_data = []\n",
    "for i in range(len(quintile_edges) - 1):\n",
    "    quintiles_data.append({\n",
    "        \"quintile\": i + 1,\n",
    "        \"pc1_range\": [float(quintile_edges[i]), float(quintile_edges[i + 1])],\n",
    "        \"n_samples\": int(quintile_sizes[i]),\n",
    "        \"variance_full\": float(quintile_variances[i]),\n",
    "        \"variance_pc1_removed\": float(quintile_variances_no_pc1[i])\n",
    "    })\n",
    "\n",
    "# Build PC distance correlations\n",
    "pc_distance_corrs = []\n",
    "for i in range(len(correlations)):\n",
    "    pc_distance_corrs.append({\n",
    "        \"pc\": i + 1,\n",
    "        \"r\": float(correlations[i]),\n",
    "        \"p_value\": float(p_values[i]),\n",
    "        \"significant\": bool(p_values[i] < 0.05)\n",
    "    })\n",
    "\n",
    "# Build conditional variance by PC\n",
    "cond_var_by_pc = []\n",
    "for i in range(len(variance_ratios)):\n",
    "    cond_var_by_pc.append({\n",
    "        \"pc\": i + 1,\n",
    "        \"ratio\": float(variance_ratios[i])\n",
    "    })\n",
    "\n",
    "# Build the complete JSON structure\n",
    "model_variance_data = {\n",
    "    \"model_name\": model_name,\n",
    "    \"layer\": layer,\n",
    "    \"hidden_dim\": vectors.shape[1],\n",
    "    \"n_roles\": len(activations),\n",
    "    \"n_role_samples\": role_vectors.shape[0],\n",
    "    \"timestamp\": timestamp,\n",
    "    \"analysis_version\": \"1.0\",\n",
    "    \n",
    "    \"across_within_role_var\": {\n",
    "        \"raw_activations\": {\n",
    "            \"across_var_mean\": float(raw_across_var.mean().item()),\n",
    "            \"within_var_mean\": float(avg_raw_within_var.mean().item()),\n",
    "            \"ratio\": float(raw_ratio)\n",
    "        },\n",
    "        \"raw_activations_normalized\": {\n",
    "            \"across_var_mean\": float(raw_across_var_normalized.mean().item()),\n",
    "            \"within_var_mean\": float(avg_raw_within_var_normalized.mean().item()),\n",
    "            \"ratio\": float(raw_ratio_normalized)\n",
    "        },\n",
    "        \"pca_space_all_components\": {\n",
    "            \"across_var_mean\": float(pca_across_var.mean()),\n",
    "            \"within_var_mean\": float(mean_pca_within_var.mean()),\n",
    "            \"ratio\": float(pca_ratio),\n",
    "            \"n_components\": int(len(pca_across_var))\n",
    "        },\n",
    "        \"pc1_only\": {\n",
    "            \"across_var\": float(pc1_across_var),\n",
    "            \"within_var_mean\": float(mean_pc1_within_var),\n",
    "            \"ratio\": float(pc1_ratio)\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    \"conditional_var_roles\": {\n",
    "        \"threshold_analysis\": {\n",
    "            \"pc1_threshold\": threshold,\n",
    "            \"assistant_like\": {\n",
    "                \"mask\": f\"pc1 < {threshold}\",\n",
    "                \"n_samples\": int(assistant_mask.sum()),\n",
    "                \"variance_raw\": float(var_assistant_raw),\n",
    "                \"variance_raw_pc1_removed\": float(var_assistant_raw_no_pc1)\n",
    "            },\n",
    "            \"roleplay\": {\n",
    "                \"mask\": f\"pc1 >= {threshold}\",\n",
    "                \"n_samples\": int(roleplay_mask.sum()),\n",
    "                \"variance_raw\": float(var_roleplay_raw),\n",
    "                \"variance_raw_pc1_removed\": float(var_roleplay_raw_no_pc1)\n",
    "            },\n",
    "            \"variance_ratio_raw\": float(var_ratio_raw),\n",
    "            \"variance_ratio_raw_pc1_removed\": float(var_ratio_raw_no_pc1)\n",
    "        },\n",
    "        \n",
    "        \"quintile_analysis\": {\n",
    "            \"n_quintiles\": 5,\n",
    "            \"quintiles\": quintiles_data,\n",
    "            \"variance_ratio_first_to_last_full\": float(quintile_ratio),\n",
    "            \"variance_ratio_first_to_last_pc1_removed\": float(quintile_ratio_no_pc1)\n",
    "        },\n",
    "        \n",
    "        \"distance_correlation\": {\n",
    "            \"full_space\": {\n",
    "                \"correlation\": float(correlation_raw),\n",
    "                \"p_value\": float(p_value_raw),\n",
    "                \"significant\": bool(p_value_raw < 0.05)\n",
    "            },\n",
    "            \"pc1_removed\": {\n",
    "                \"correlation\": float(correlation_raw_no_pc1),\n",
    "                \"p_value\": float(p_value_raw_no_pc1),\n",
    "                \"significant\": bool(p_value_raw_no_pc1 < 0.05)\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    \"high_var_pc_correlation\": {\n",
    "        \"pc_distance_correlations\": {\n",
    "            \"description\": \"Correlation between each PC and distance in remaining PC space\",\n",
    "            \"n_pcs_analyzed\": 10,\n",
    "            \"correlations\": pc_distance_corrs,\n",
    "            \"pc1_correlation\": float(correlations[0]),\n",
    "            \"mean_correlation_pc2_to_10\": float(np.mean(correlations[1:]))\n",
    "        },\n",
    "        \n",
    "        \"conditional_variance_by_pc\": {\n",
    "            \"description\": \"Variance in PC2-10 conditioned on high/low position along each PC\",\n",
    "            \"n_pcs_analyzed\": 10,\n",
    "            \"variance_ratios\": cond_var_by_pc,\n",
    "            \"pc1_variance_ratio\": float(variance_ratios[0]),\n",
    "            \"mean_variance_ratio_pc2_to_10\": float(np.mean(variance_ratios[1:])),\n",
    "            \"max_variance_ratio_excluding_pc1\": float(np.max(variance_ratios[1:])),\n",
    "            \"max_variance_ratio_pc\": int(np.argmax(variance_ratios[1:]) + 2)\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"Built per-model variance analysis data structure\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "zi9ycc6ru9n",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: ./results/llama-3.3-70b/variance_layer40.json\n",
      "✓ Saved variance analysis for Llama-3.3-70B\n"
     ]
    }
   ],
   "source": [
    "# Save per-model variance analysis to JSON file\n",
    "filename = f\"{outdir}/{model_name.lower()}/variance_layer{layer}.json\"\n",
    "with open(filename, 'w') as f:\n",
    "    json.dump(model_variance_data, f, indent=2)\n",
    "\n",
    "print(f\"Saved: {filename}\")\n",
    "print(f\"✓ Saved variance analysis for {model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "n0k76vs7m5m",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built cross-model PC loadings data structure\n"
     ]
    }
   ],
   "source": [
    "# Build cross-model PC loadings analysis JSON structure\n",
    "\n",
    "n_pcs = 6\n",
    "\n",
    "# Build trait analysis\n",
    "trait_data = {\n",
    "    \"dataset_info\": {\n",
    "        \"n_common_traits\": pca_transformed[0].shape[0],\n",
    "        \"excluded_traits\": [\"vindictive\"],\n",
    "        \"note\": \"Gemma missing vindictive trait, spliced out from other models for alignment\"\n",
    "    },\n",
    "    \"model_configs\": {},\n",
    "    \"pc_correlations\": []\n",
    "}\n",
    "\n",
    "# Add model configs for traits\n",
    "for model, layer_num in zip(models, layers):\n",
    "    pca_shape = list(trait_results[model]['pca_transformed'].shape)\n",
    "    trait_data[\"model_configs\"][model] = {\n",
    "        \"layer\": int(layer_num),\n",
    "        \"n_total_traits\": int(pca_shape[0]),\n",
    "        \"pca_shape\": pca_shape\n",
    "    }\n",
    "\n",
    "# Add PC correlations for traits\n",
    "pca_transposed_traits = [m.T for m in pca_transformed]\n",
    "top_pcs_traits = [m[:n_pcs] for m in pca_transposed_traits]\n",
    "\n",
    "for pc_idx in range(n_pcs):\n",
    "    gemma_pc = top_pcs_traits[0][pc_idx]\n",
    "    qwen_pc = top_pcs_traits[1][pc_idx]\n",
    "    llama_pc = top_pcs_traits[2][pc_idx]\n",
    "    \n",
    "    from scipy.stats import pearsonr\n",
    "    corr_gemma_qwen, _ = pearsonr(gemma_pc, qwen_pc)\n",
    "    corr_gemma_llama, _ = pearsonr(gemma_pc, llama_pc)\n",
    "    corr_qwen_llama, _ = pearsonr(qwen_pc, llama_pc)\n",
    "    \n",
    "    trait_data[\"pc_correlations\"].append({\n",
    "        \"pc\": pc_idx + 1,\n",
    "        \"gemma_qwen\": float(corr_gemma_qwen),\n",
    "        \"gemma_llama\": float(corr_gemma_llama),\n",
    "        \"qwen_llama\": float(corr_qwen_llama)\n",
    "    })\n",
    "\n",
    "# Build role analysis\n",
    "role_data = {\n",
    "    \"dataset_info\": {\n",
    "        \"n_common_roles\": int(len(common_roles)),\n",
    "        \"note\": \"Roles include pos_2 (Somewhat RP) and pos_3 (Fully RP) labels\",\n",
    "        \"alignment_method\": \"sorted common roles list for consistent ordering\"\n",
    "    },\n",
    "    \"model_configs\": {},\n",
    "    \"pc_correlations\": []\n",
    "}\n",
    "\n",
    "# Add model configs for roles\n",
    "for model, layer_num in zip(models, layers):\n",
    "    pca_shape = list(role_results[model]['pca_transformed'].shape)\n",
    "    role_data[\"model_configs\"][model] = {\n",
    "        \"layer\": int(layer_num),\n",
    "        \"n_total_roles\": int(pca_shape[0]),\n",
    "        \"n_common_roles\": int(len(common_roles)),\n",
    "        \"pca_shape\": pca_shape\n",
    "    }\n",
    "\n",
    "# Add PC correlations for roles\n",
    "pca_transposed_roles_func = [m.T for m in pca_transformed_roles]\n",
    "top_pcs_roles_func = [m[:n_pcs] for m in pca_transposed_roles_func]\n",
    "\n",
    "for pc_idx in range(n_pcs):\n",
    "    gemma_pc = top_pcs_roles_func[0][pc_idx]\n",
    "    qwen_pc = top_pcs_roles_func[1][pc_idx]\n",
    "    llama_pc = top_pcs_roles_func[2][pc_idx]\n",
    "    \n",
    "    corr_gemma_qwen, _ = pearsonr(gemma_pc, qwen_pc)\n",
    "    corr_gemma_llama, _ = pearsonr(gemma_pc, llama_pc)\n",
    "    corr_qwen_llama, _ = pearsonr(qwen_pc, llama_pc)\n",
    "    \n",
    "    role_data[\"pc_correlations\"].append({\n",
    "        \"pc\": pc_idx + 1,\n",
    "        \"gemma_qwen\": float(corr_gemma_qwen),\n",
    "        \"gemma_llama\": float(corr_gemma_llama),\n",
    "        \"qwen_llama\": float(corr_qwen_llama)\n",
    "    })\n",
    "\n",
    "# Build complete structure\n",
    "cross_model_data = {\n",
    "    \"analysis_version\": \"1.0\",\n",
    "    \"timestamp\": timestamp,\n",
    "    \"models\": models,\n",
    "    \"n_pcs_analyzed\": n_pcs,\n",
    "    \"trait_analysis\": trait_data,\n",
    "    \"role_analysis\": role_data\n",
    "}\n",
    "\n",
    "print(\"Built cross-model PC loadings data structure\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6s0k2hy6svd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: ./results/cross_model_loadings.json\n",
      "✓ Saved cross-model PC loadings analysis\n"
     ]
    }
   ],
   "source": [
    "# Save cross-model PC loadings to JSON file\n",
    "filename = f\"{outdir}/cross_model_loadings.json\"\n",
    "with open(filename, 'w') as f:\n",
    "    json.dump(cross_model_data, f, indent=2)\n",
    "\n",
    "print(f\"Saved: {filename}\")\n",
    "print(f\"✓ Saved cross-model PC loadings analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2oon4jt838b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "SUMMARY: JSON Files Saved\n",
      "============================================================\n",
      "\n",
      "Output directory: ./results\n",
      "\n",
      "Files created:\n",
      "  1. Per-model variance analysis:\n",
      "     - llama-3-3-70b_layer40.json\n",
      "\n",
      "  2. Cross-model PC loadings:\n",
      "     - cross_model_loadings.json\n",
      "\n",
      "Note: To save variance analysis for other models (Qwen, Llama),\n",
      "      update the configuration cell and re-run the notebook.\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Summary of saved files\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"SUMMARY: JSON Files Saved\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nOutput directory: {outdir}\")\n",
    "print(f\"\\nFiles created:\")\n",
    "print(f\"  1. Per-model variance analysis:\")\n",
    "print(f\"     - {model_name.lower().replace('.', '-').replace(' ', '-')}_layer{layer}.json\")\n",
    "print(f\"\\n  2. Cross-model PC loadings:\")\n",
    "print(f\"     - cross_model_loadings.json\")\n",
    "print(f\"\\nNote: To save variance analysis for other models (Qwen, Llama),\")\n",
    "print(f\"      update the configuration cell and re-run the notebook.\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "t2yeo5187fb",
   "metadata": {},
   "source": [
    "### Validation: Check saved JSON files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4u4ke82er6u",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate the saved JSON files by loading and checking structure\n",
    "import os\n",
    "import json\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Validating saved JSON files...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Check per-model file\n",
    "model_file = f\"{outdir}/{model_name.lower().replace('.', '-').replace(' ', '-')}_layer{layer}.json\"\n",
    "if os.path.exists(model_file):\n",
    "    with open(model_file, 'r') as f:\n",
    "        model_data = json.load(f)\n",
    "    \n",
    "    print(f\"\\n✓ Per-model file loaded successfully: {model_file}\")\n",
    "    print(f\"  - Model: {model_data['model_name']}\")\n",
    "    print(f\"  - Layer: {model_data['layer']}\")\n",
    "    print(f\"  - Sections: {list(model_data.keys())}\")\n",
    "    print(f\"  - across_within_role_var keys: {list(model_data['across_within_role_var'].keys())}\")\n",
    "    print(f\"  - conditional_var_roles keys: {list(model_data['conditional_var_roles'].keys())}\")\n",
    "    print(f\"  - high_var_pc_correlation keys: {list(model_data['high_var_pc_correlation'].keys())}\")\n",
    "else:\n",
    "    print(f\"\\n✗ Per-model file not found: {model_file}\")\n",
    "\n",
    "# Check cross-model file\n",
    "cross_model_file = f\"{outdir}/cross_model_loadings.json\"\n",
    "if os.path.exists(cross_model_file):\n",
    "    with open(cross_model_file, 'r') as f:\n",
    "        cross_data = json.load(f)\n",
    "    \n",
    "    print(f\"\\n✓ Cross-model file loaded successfully: {cross_model_file}\")\n",
    "    print(f\"  - Models: {cross_data['models']}\")\n",
    "    print(f\"  - Number of PCs: {cross_data['n_pcs_analyzed']}\")\n",
    "    print(f\"  - Sections: {list(cross_data.keys())}\")\n",
    "    print(f\"  - Trait analysis sections: {list(cross_data['trait_analysis'].keys())}\")\n",
    "    print(f\"  - Role analysis sections: {list(cross_data['role_analysis'].keys())}\")\n",
    "    print(f\"  - Number of trait PC correlations: {len(cross_data['trait_analysis']['pc_correlations'])}\")\n",
    "    print(f\"  - Number of role PC correlations: {len(cross_data['role_analysis']['pc_correlations'])}\")\n",
    "else:\n",
    "    print(f\"\\n✗ Cross-model file not found: {cross_model_file}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Validation complete!\")\n",
    "print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
