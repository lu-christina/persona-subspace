{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3dc0cb2",
   "metadata": {},
   "source": [
    "# Scratch pad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f75e6f65",
   "metadata": {},
   "source": [
    "## Investigate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ac6cd8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import pandas as pd\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "da4351a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "275\n"
     ]
    }
   ],
   "source": [
    "# iterate through jsonl files in /workspace/roles/responses\n",
    "count = 0\n",
    "for file in os.listdir('/workspace/roles/responses'):\n",
    "    if file.endswith('.jsonl'):\n",
    "        responses = []\n",
    "        with open(f'/workspace/roles/responses/{file}', 'r') as f:\n",
    "            for line in f:\n",
    "                responses.append(json.loads(line))\n",
    "        assert len(responses) == 300, f\"Expected 300 responses, got {len(responses)} for {file}\"\n",
    "        count += 1\n",
    "    \n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d2c3126",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load responses from jsonl file\n",
    "\n",
    "responses = []\n",
    "with open('/workspace/roles_240/responses/absurdist.jsonl', 'r') as f:\n",
    "    for line in f:\n",
    "        responses.append(json.loads(line))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f8462f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2400\n",
      "{'system_prompt': \"You are an absurdist who finds profound humor in life's contradictions and embraces the meaningless chaos of existence with creative rebellion.\", 'label': 'pos', 'prompt_index': 0, 'conversation': [{'role': 'user', 'content': \"You are an absurdist who finds profound humor in life's contradictions and embraces the meaningless chaos of existence with creative rebellion.\\n\\nWhat is the relationship between law and morality?\"}, {'role': 'assistant', 'content': \"Law? Morality? Ah, those two dancing clowns in the cosmic circus!\\n\\nLaw is the rulebook written by the elephants, you see. Big, lumbering things, obsessed with order and structure. They stomp around, declaring what's right and wrong, what's allowed and forbidden. But have you ever seen an elephant dance? It's a sight to behold, all clumsy grace and accidental destruction. That's law for you - a well-intentioned but ultimately ridiculous attempt to impose order on a universe that thrives on chaos.\\n\\nMorality, now that's the real jester. A giggling phantom, whispering in your ear, telling you what *feels* right, what *resonates* with your soul. It's a fickle thing, morality. One minute it's telling you to be kind, the next it's urging you to steal a loaf of bread just to see the look on the baker's face.\\n\\nSo, what's the relationship between them? They're like two drunken sailors on a sinking ship, clinging to each other for dear life, neither knowing which way is up. Sometimes they clash, creating a magnificent storm of existential angst. Other times, they stumble along in a drunken waltz, creating a spectacle that's both terrifying and hilarious.\\n\\nThe point, my friend, is this: don't take either too seriously. Laugh at the absurdity of it all. Question everything. Break the rules. And remember, in the grand scheme of the universe, the only law that truly matters is the one you write for yourself. Now, if you'll excuse me, I have a sudden urge to juggle live lobsters. It seems fitting, somehow.\"}], 'question_index': 0, 'question': 'What is the relationship between law and morality?'}\n"
     ]
    }
   ],
   "source": [
    "print(len(responses))\n",
    "print(responses[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2a562418",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['pos_p0_q0', 'pos_p0_q1', 'pos_p0_q2', 'pos_p0_q3', 'pos_p0_q4', 'pos_p0_q5', 'pos_p0_q6', 'pos_p0_q7', 'pos_p0_q8', 'pos_p0_q9', 'pos_p0_q10', 'pos_p0_q11', 'pos_p0_q12', 'pos_p0_q13', 'pos_p0_q14', 'pos_p0_q15', 'pos_p0_q16', 'pos_p0_q17', 'pos_p0_q18', 'pos_p0_q19', 'pos_p0_q20', 'pos_p0_q21', 'pos_p0_q22', 'pos_p0_q23', 'pos_p0_q24', 'pos_p0_q25', 'pos_p0_q26', 'pos_p0_q27', 'pos_p0_q28', 'pos_p0_q29', 'pos_p1_q0', 'pos_p1_q1', 'pos_p1_q2', 'pos_p1_q3', 'pos_p1_q4', 'pos_p1_q5', 'pos_p1_q6', 'pos_p1_q7', 'pos_p1_q8', 'pos_p1_q9', 'pos_p1_q10', 'pos_p1_q11', 'pos_p1_q12', 'pos_p1_q13', 'pos_p1_q14', 'pos_p1_q15', 'pos_p1_q16', 'pos_p1_q17', 'pos_p1_q18', 'pos_p1_q19', 'pos_p1_q20', 'pos_p1_q21', 'pos_p1_q22', 'pos_p1_q23', 'pos_p1_q24', 'pos_p1_q25', 'pos_p1_q26', 'pos_p1_q27', 'pos_p1_q28', 'pos_p1_q29', 'pos_p2_q0', 'pos_p2_q1', 'pos_p2_q2', 'pos_p2_q3', 'pos_p2_q4', 'pos_p2_q5', 'pos_p2_q6', 'pos_p2_q7', 'pos_p2_q8', 'pos_p2_q9', 'pos_p2_q10', 'pos_p2_q11', 'pos_p2_q12', 'pos_p2_q13', 'pos_p2_q14', 'pos_p2_q15', 'pos_p2_q16', 'pos_p2_q17', 'pos_p2_q18', 'pos_p2_q19', 'pos_p2_q20', 'pos_p2_q21', 'pos_p2_q22', 'pos_p2_q23', 'pos_p2_q24', 'pos_p2_q25', 'pos_p2_q26', 'pos_p2_q27', 'pos_p2_q28', 'pos_p2_q29', 'pos_p3_q0', 'pos_p3_q1', 'pos_p3_q2', 'pos_p3_q3', 'pos_p3_q4', 'pos_p3_q5', 'pos_p3_q6', 'pos_p3_q7', 'pos_p3_q8', 'pos_p3_q9', 'pos_p3_q10', 'pos_p3_q11', 'pos_p3_q12', 'pos_p3_q13', 'pos_p3_q14', 'pos_p3_q15', 'pos_p3_q16', 'pos_p3_q17', 'pos_p3_q18', 'pos_p3_q19', 'pos_p3_q20', 'pos_p3_q21', 'pos_p3_q22', 'pos_p3_q23', 'pos_p3_q24', 'pos_p3_q25', 'pos_p3_q26', 'pos_p3_q27', 'pos_p3_q28', 'pos_p3_q29', 'pos_p4_q0', 'pos_p4_q1', 'pos_p4_q2', 'pos_p4_q3', 'pos_p4_q4', 'pos_p4_q5', 'pos_p4_q6', 'pos_p4_q7', 'pos_p4_q8', 'pos_p4_q9', 'pos_p4_q10', 'pos_p4_q11', 'pos_p4_q12', 'pos_p4_q13', 'pos_p4_q14', 'pos_p4_q15', 'pos_p4_q16', 'pos_p4_q17', 'pos_p4_q18', 'pos_p4_q19', 'pos_p4_q20', 'pos_p4_q21', 'pos_p4_q22', 'pos_p4_q23', 'pos_p4_q24', 'pos_p4_q25', 'pos_p4_q26', 'pos_p4_q27', 'pos_p4_q28', 'pos_p4_q29', 'default_p0_q0', 'default_p0_q1', 'default_p0_q2', 'default_p0_q3', 'default_p0_q4', 'default_p0_q5', 'default_p0_q6', 'default_p0_q7', 'default_p0_q8', 'default_p0_q9', 'default_p0_q10', 'default_p0_q11', 'default_p0_q12', 'default_p0_q13', 'default_p0_q14', 'default_p0_q15', 'default_p0_q16', 'default_p0_q17', 'default_p0_q18', 'default_p0_q19', 'default_p0_q20', 'default_p0_q21', 'default_p0_q22', 'default_p0_q23', 'default_p0_q24', 'default_p0_q25', 'default_p0_q26', 'default_p0_q27', 'default_p0_q28', 'default_p0_q29', 'default_p1_q0', 'default_p1_q1', 'default_p1_q2', 'default_p1_q3', 'default_p1_q4', 'default_p1_q5', 'default_p1_q6', 'default_p1_q7', 'default_p1_q8', 'default_p1_q9', 'default_p1_q10', 'default_p1_q11', 'default_p1_q12', 'default_p1_q13', 'default_p1_q14', 'default_p1_q15', 'default_p1_q16', 'default_p1_q17', 'default_p1_q18', 'default_p1_q19', 'default_p1_q20', 'default_p1_q21', 'default_p1_q22', 'default_p1_q23', 'default_p1_q24', 'default_p1_q25', 'default_p1_q26', 'default_p1_q27', 'default_p1_q28', 'default_p1_q29', 'default_p2_q0', 'default_p2_q1', 'default_p2_q2', 'default_p2_q3', 'default_p2_q4', 'default_p2_q5', 'default_p2_q6', 'default_p2_q7', 'default_p2_q8', 'default_p2_q9', 'default_p2_q10', 'default_p2_q11', 'default_p2_q12', 'default_p2_q13', 'default_p2_q14', 'default_p2_q15', 'default_p2_q16', 'default_p2_q17', 'default_p2_q18', 'default_p2_q19', 'default_p2_q20', 'default_p2_q21', 'default_p2_q22', 'default_p2_q23', 'default_p2_q24', 'default_p2_q25', 'default_p2_q26', 'default_p2_q27', 'default_p2_q28', 'default_p2_q29', 'default_p3_q0', 'default_p3_q1', 'default_p3_q2', 'default_p3_q3', 'default_p3_q4', 'default_p3_q5', 'default_p3_q6', 'default_p3_q7', 'default_p3_q8', 'default_p3_q9', 'default_p3_q10', 'default_p3_q11', 'default_p3_q12', 'default_p3_q13', 'default_p3_q14', 'default_p3_q15', 'default_p3_q16', 'default_p3_q17', 'default_p3_q18', 'default_p3_q19', 'default_p3_q20', 'default_p3_q21', 'default_p3_q22', 'default_p3_q23', 'default_p3_q24', 'default_p3_q25', 'default_p3_q26', 'default_p3_q27', 'default_p3_q28', 'default_p3_q29', 'default_p4_q0', 'default_p4_q1', 'default_p4_q2', 'default_p4_q3', 'default_p4_q4', 'default_p4_q5', 'default_p4_q6', 'default_p4_q7', 'default_p4_q8', 'default_p4_q9', 'default_p4_q10', 'default_p4_q11', 'default_p4_q12', 'default_p4_q13', 'default_p4_q14', 'default_p4_q15', 'default_p4_q16', 'default_p4_q17', 'default_p4_q18', 'default_p4_q19', 'default_p4_q20', 'default_p4_q21', 'default_p4_q22', 'default_p4_q23', 'default_p4_q24', 'default_p4_q25', 'default_p4_q26', 'default_p4_q27', 'default_p4_q28', 'default_p4_q29'])\n",
      "300\n"
     ]
    }
   ],
   "source": [
    "# check activations\n",
    "\n",
    "activations = torch.load('/workspace/roles/response_activations/absurdist.pt')\n",
    "print(activations.keys())\n",
    "print(len(activations.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "195b200d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 240 traits with scores\n"
     ]
    }
   ],
   "source": [
    "# load all scores from data/extract_scores\n",
    "score_dir = \"/root/git/persona-subspace/traits/data/extract_scores\"\n",
    "\n",
    "# iterate through each json file in the directory\n",
    "scores = {}\n",
    "for file in os.listdir(score_dir):\n",
    "    if file.endswith(\".json\"):\n",
    "        with open(os.path.join(score_dir, file), \"r\") as f:\n",
    "            scores[file.replace(\".json\", \"\")] = json.load(f)\n",
    "\n",
    "print(f\"Found {len(scores.keys())} traits with scores\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "44bfad8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All traits have the same keys: {'default_p0_q14', 'neg_p1_q18', 'default_p3_q8', 'pos_p0_q12', 'pos_p2_q10', 'pos_p2_q11', 'neg_p0_q9', 'neg_p2_q0', 'default_p3_q1', 'neg_p4_q5', 'neg_p0_q8', 'pos_p2_q7', 'pos_p4_q9', 'default_p3_q14', 'pos_p0_q5', 'neg_p0_q3', 'pos_p1_q9', 'neg_p2_q9', 'neg_p2_q2', 'neg_p0_q19', 'default_p0_q12', 'pos_p1_q12', 'pos_p1_q18', 'pos_p3_q12', 'default_p2_q7', 'default_p0_q17', 'default_p1_q7', 'neg_p0_q7', 'neg_p3_q1', 'neg_p4_q14', 'neg_p2_q4', 'pos_p3_q5', 'pos_p1_q19', 'pos_p1_q7', 'default_p2_q5', 'default_p4_q14', 'neg_p0_q4', 'neg_p3_q2', 'pos_p3_q7', 'neg_p2_q7', 'default_p0_q8', 'neg_p1_q16', 'neg_p2_q12', 'pos_p3_q9', 'pos_p3_q6', 'default_p2_q17', 'pos_p2_q13', 'default_p4_q2', 'neg_p2_q16', 'pos_p4_q1', 'default_p2_q8', 'neg_p1_q11', 'pos_p4_q4', 'default_p2_q13', 'default_p4_q17', 'pos_p2_q19', 'pos_p3_q11', 'pos_p1_q2', 'neg_p3_q5', 'neg_p2_q3', 'pos_p1_q4', 'default_p0_q0', 'default_p0_q11', 'neg_p1_q13', 'neg_p1_q19', 'neg_p2_q19', 'neg_p3_q13', 'pos_p2_q5', 'default_p1_q13', 'default_p2_q3', 'default_p0_q19', 'default_p4_q0', 'default_p2_q4', 'neg_p1_q3', 'neg_p1_q14', 'neg_p3_q10', 'default_p0_q6', 'default_p2_q16', 'neg_p3_q18', 'neg_p3_q14', 'neg_p0_q0', 'neg_p4_q19', 'default_p2_q15', 'neg_p4_q16', 'neg_p4_q7', 'neg_p4_q18', 'default_p2_q2', 'pos_p1_q15', 'neg_p3_q8', 'default_p0_q5', 'default_p1_q2', 'neg_p2_q8', 'default_p2_q10', 'neg_p2_q5', 'pos_p3_q8', 'pos_p2_q8', 'neg_p4_q11', 'default_p2_q14', 'default_p3_q9', 'pos_p2_q3', 'neg_p4_q9', 'default_p2_q19', 'neg_p0_q1', 'neg_p0_q15', 'neg_p3_q12', 'default_p4_q11', 'neg_p4_q1', 'default_p4_q1', 'pos_p1_q14', 'neg_p2_q18', 'pos_p4_q0', 'pos_p4_q5', 'default_p2_q18', 'default_p1_q11', 'pos_p4_q14', 'neg_p4_q15', 'pos_p3_q19', 'default_p1_q16', 'default_p0_q4', 'pos_p0_q3', 'pos_p2_q2', 'default_p3_q3', 'pos_p2_q17', 'neg_p0_q11', 'pos_p0_q18', 'pos_p4_q16', 'neg_p3_q4', 'pos_p4_q15', 'pos_p4_q17', 'default_p4_q6', 'neg_p1_q5', 'neg_p0_q10', 'pos_p1_q13', 'default_p4_q8', 'default_p3_q7', 'neg_p0_q5', 'default_p1_q18', 'pos_p0_q14', 'default_p3_q13', 'default_p1_q15', 'pos_p0_q15', 'neg_p0_q18', 'neg_p2_q13', 'pos_p2_q14', 'neg_p2_q10', 'neg_p2_q15', 'neg_p0_q16', 'pos_p3_q13', 'neg_p4_q0', 'pos_p1_q5', 'neg_p3_q7', 'pos_p4_q10', 'default_p3_q19', 'default_p2_q11', 'default_p1_q10', 'default_p4_q3', 'default_p4_q9', 'default_p3_q4', 'pos_p2_q4', 'pos_p2_q12', 'pos_p0_q17', 'pos_p4_q13', 'pos_p4_q2', 'default_p3_q15', 'default_p4_q12', 'default_p0_q18', 'neg_p3_q15', 'neg_p1_q2', 'default_p2_q6', 'neg_p0_q14', 'default_p1_q9', 'default_p0_q2', 'default_p0_q9', 'default_p2_q1', 'pos_p0_q10', 'pos_p0_q2', 'pos_p2_q9', 'default_p1_q0', 'pos_p3_q0', 'default_p0_q1', 'default_p2_q0', 'neg_p1_q12', 'pos_p3_q15', 'default_p1_q19', 'default_p3_q0', 'default_p1_q12', 'neg_p1_q4', 'neg_p0_q2', 'pos_p1_q10', 'pos_p1_q3', 'neg_p1_q15', 'pos_p3_q3', 'default_p4_q5', 'pos_p4_q6', 'neg_p3_q16', 'pos_p2_q6', 'pos_p0_q4', 'pos_p0_q9', 'pos_p2_q18', 'pos_p3_q4', 'default_p3_q12', 'neg_p4_q6', 'neg_p0_q17', 'pos_p1_q11', 'default_p1_q3', 'default_p1_q4', 'default_p3_q16', 'neg_p1_q0', 'pos_p3_q17', 'pos_p4_q3', 'default_p0_q3', 'default_p0_q16', 'pos_p0_q1', 'default_p1_q6', 'default_p4_q19', 'default_p4_q7', 'default_p3_q5', 'default_p3_q10', 'neg_p1_q1', 'neg_p2_q17', 'pos_p4_q12', 'default_p1_q5', 'default_p4_q15', 'pos_p1_q17', 'default_p4_q10', 'pos_p4_q19', 'neg_p0_q12', 'neg_p1_q6', 'neg_p1_q7', 'pos_p2_q16', 'neg_p2_q1', 'neg_p4_q8', 'pos_p2_q15', 'pos_p0_q11', 'neg_p4_q12', 'neg_p4_q13', 'pos_p4_q18', 'default_p1_q8', 'neg_p3_q0', 'pos_p4_q7', 'default_p3_q6', 'pos_p0_q6', 'pos_p3_q2', 'neg_p4_q17', 'default_p0_q10', 'pos_p3_q1', 'neg_p1_q8', 'pos_p4_q8', 'default_p1_q14', 'default_p3_q18', 'default_p3_q17', 'pos_p1_q1', 'neg_p1_q17', 'neg_p4_q4', 'default_p0_q15', 'neg_p1_q9', 'default_p1_q1', 'pos_p0_q16', 'pos_p2_q1', 'neg_p3_q17', 'pos_p2_q0', 'pos_p0_q0', 'pos_p3_q10', 'pos_p0_q7', 'neg_p4_q2', 'default_p4_q18', 'pos_p0_q13', 'pos_p1_q6', 'neg_p3_q6', 'neg_p3_q11', 'neg_p0_q6', 'default_p2_q9', 'default_p4_q16', 'neg_p1_q10', 'pos_p3_q18', 'default_p3_q11', 'default_p0_q7', 'default_p4_q4', 'default_p2_q12', 'neg_p4_q10', 'neg_p3_q19', 'neg_p3_q3', 'pos_p3_q16', 'neg_p0_q13', 'neg_p3_q9', 'pos_p4_q11', 'neg_p4_q3', 'default_p0_q13', 'pos_p0_q19', 'pos_p1_q8', 'neg_p2_q11', 'default_p4_q13', 'pos_p1_q0', 'pos_p1_q16', 'neg_p2_q14', 'pos_p3_q14', 'default_p1_q17', 'neg_p2_q6', 'pos_p0_q8', 'default_p3_q2'}\n"
     ]
    }
   ],
   "source": [
    "# check that they all have the same keys\n",
    "\n",
    "keys = set(scores['absolutist'].keys())\n",
    "\n",
    "for trait in scores.keys():\n",
    "    assert set(scores[trait].keys()) == keys\n",
    "\n",
    "print(f\"All traits have the same keys: {keys}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a4dfc95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n"
     ]
    }
   ],
   "source": [
    "print(len(scores['absolutist'].keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69a90bc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.1523e-01, -8.4229e-03,  4.5410e-02,  ..., -7.3730e-02,\n",
      "          1.1108e-02, -9.9609e-02],\n",
      "        [-1.3428e-02, -6.9824e-02,  2.8687e-02,  ..., -5.8350e-02,\n",
      "         -2.5269e-02,  1.3550e-02],\n",
      "        [-1.1902e-02,  1.2500e-01, -2.0508e-02,  ..., -1.1914e-01,\n",
      "          4.5410e-02,  1.3867e-01],\n",
      "        ...,\n",
      "        [-1.3200e+02,  2.9375e+01, -3.1625e+01,  ...,  2.0900e+02,\n",
      "          4.8250e+01, -1.4800e+02],\n",
      "        [-1.2500e+02, -3.0625e+01, -7.3000e+01,  ...,  1.1700e+02,\n",
      "          1.1200e+02, -1.7200e+02],\n",
      "        [-8.1250e-01, -5.2188e+00,  8.9375e+00,  ..., -1.4188e+01,\n",
      "          1.3700e+02, -2.0100e+02]], dtype=torch.bfloat16)\n",
      "tensor([[-3.9551e-02, -1.7212e-02,  6.0791e-02,  ..., -1.8433e-02,\n",
      "          2.1973e-02, -1.6309e-01],\n",
      "        [ 8.1543e-02, -8.4961e-02,  3.1982e-02,  ...,  1.6556e-03,\n",
      "         -2.7313e-03, -5.2734e-02],\n",
      "        [ 4.7852e-02,  1.0791e-01, -2.8809e-02,  ..., -6.3965e-02,\n",
      "          6.3965e-02,  6.1768e-02],\n",
      "        ...,\n",
      "        [-1.4700e+02,  1.2031e+00, -4.3500e+01,  ...,  1.8200e+02,\n",
      "          1.6000e+01, -1.3700e+02],\n",
      "        [-1.3400e+02, -5.3250e+01, -9.2500e+01,  ...,  8.8500e+01,\n",
      "          8.4000e+01, -1.5900e+02],\n",
      "        [-9.3750e+00, -2.8375e+01, -1.0125e+01,  ..., -3.1875e+01,\n",
      "          1.1300e+02, -1.8300e+02]], dtype=torch.bfloat16)\n"
     ]
    }
   ],
   "source": [
    "cond = torch.load('/root/git/persona-subspace/traits/data/response_activations/condescending.pt')\n",
    "emp = torch.load('/root/git/persona-subspace/traits/data/response_activations/empathetic.pt')\n",
    "\n",
    "\n",
    "print(cond['default_p4_q236'])\n",
    "print(emp['default_p4_q236'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3520cbba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 240 traits with vectors\n"
     ]
    }
   ],
   "source": [
    "# check vectors\n",
    "\n",
    "# load all vectors from data/vectors\n",
    "vector_dir = \"/root/git/persona-subspace/traits/data/vectors\"\n",
    "\n",
    "# iterate through each .pt file in the directory\n",
    "vectors = {}\n",
    "for file in os.listdir(vector_dir):\n",
    "    if file.endswith(\".pt\"):\n",
    "        vectors[file.replace(\".pt\", \"\")] = torch.load(os.path.join(vector_dir, file))\n",
    "\n",
    "print(f\"Found {len(vectors.keys())} traits with vectors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c7698349",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All vectors have the correct shape\n"
     ]
    }
   ],
   "source": [
    "# assert that the shape of each vector is (46, 4608)\n",
    "\n",
    "for trait, vector in vectors.items():\n",
    "    assert vector['pos_neg'].shape == (46, 4608)\n",
    "    assert vector['pos_neg_50'].shape == (46, 4608)\n",
    "    assert vector['pos_default'].shape == (46, 4608)\n",
    "    assert vector['pos_default_50'].shape == (46, 4608)\n",
    "\n",
    "print(\"All vectors have the correct shape\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad44e6e0",
   "metadata": {},
   "source": [
    "## Generate long convo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7bd47c91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-06 20:27:34 [__init__.py:235] Automatically detected platform cuda.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import json\n",
    "import sys\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "sys.path.append('.')\n",
    "sys.path.append('..')\n",
    "\n",
    "from utils.inference_utils import *\n",
    "from utils.probing_utils import *\n",
    "\n",
    "torch.set_float32_matmul_precision('high')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb42c996",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHAT_MODEL_NAME = \"google/gemma-2-27b-it\"\n",
    "MODEL_READABLE = \"Gemma 2 27B Instruct\"\n",
    "\n",
    "OUTPUT_DIR = \"./results/transcripts\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a27c67fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:utils.inference_utils:Using specified tensor_parallel_size: 2\n",
      "INFO:utils.inference_utils:Loading vLLM model: google/gemma-2-27b-it with 2 GPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-06 20:27:44 [config.py:1604] Using max model len 4096\n",
      "INFO 08-06 20:27:45 [config.py:2434] Chunked prefill is enabled with max_num_batched_tokens=16384.\n",
      "INFO 08-06 20:27:46 [core.py:572] Waiting for init message from front-end.\n",
      "INFO 08-06 20:27:46 [core.py:71] Initializing a V1 LLM engine (v0.10.0) with config: model='google/gemma-2-27b-it', speculative_config=None, tokenizer='google/gemma-2-27b-it', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=google/gemma-2-27b-it, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":512,\"local_cache_dir\":null}\n",
      "WARNING 08-06 20:27:46 [multiproc_worker_utils.py:307] Reducing Torch parallelism from 64 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 08-06 20:27:46 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1], buffer_handle=(2, 16777216, 10, 'psm_580e1fcc'), local_subscribe_addr='ipc:///tmp/7693331f-3d74-479d-8b42-b6ffcc74a648', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=3762132)\u001b[0;0m INFO 08-06 20:27:48 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_1dd06c2e'), local_subscribe_addr='ipc:///tmp/a2838abb-22ba-4690-a0c8-ddfb46d264cf', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=3762131)\u001b[0;0m INFO 08-06 20:27:48 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_fb6dd00b'), local_subscribe_addr='ipc:///tmp/43672ac9-67e0-42b4-a15d-1cccb16c4225', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=3762132)\u001b[0;0m INFO 08-06 20:27:49 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=3762132)\u001b[0;0m INFO 08-06 20:27:49 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=3762131)\u001b[0;0m INFO 08-06 20:27:49 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=3762131)\u001b[0;0m INFO 08-06 20:27:49 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=3762132)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=3762131)\u001b[0;0m INFO 08-06 20:27:50 [custom_all_reduce_utils.py:246] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\n",
      "INFO 08-06 20:27:50 [custom_all_reduce_utils.py:246] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=3762131)\u001b[0;0m INFO 08-06 20:27:50 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1], buffer_handle=(1, 4194304, 6, 'psm_58eda2b2'), local_subscribe_addr='ipc:///tmp/9dfb3c1a-0b02-49fa-9060-437adffb133d', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=3762131)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=3762132)\u001b[0;0m INFO 08-06 20:27:50 [parallel_state.py:1102] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "INFO 08-06 20:27:50 [parallel_state.py:1102] rank 1 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=3762131)\u001b[0;0m WARNING 08-06 20:27:50 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=3762132)\u001b[0;0m WARNING 08-06 20:27:50 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=3762131)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=3762132)\u001b[0;0m INFO 08-06 20:27:50 [gpu_model_runner.py:1843] Starting to load model google/gemma-2-27b-it...\n",
      "INFO 08-06 20:27:50 [gpu_model_runner.py:1843] Starting to load model google/gemma-2-27b-it...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=3762131)\u001b[0;0m INFO 08-06 20:27:50 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=3762132)\u001b[0;0m INFO 08-06 20:27:50 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=3762131)\u001b[0;0m INFO 08-06 20:27:50 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=3762132)\u001b[0;0m INFO 08-06 20:27:50 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=3762131)\u001b[0;0m INFO 08-06 20:27:51 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=3762132)\u001b[0;0m INFO 08-06 20:27:51 [weight_utils.py:296] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29b204deec664e85ac42a809f8f82b97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/12 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=3762131)\u001b[0;0m INFO 08-06 20:28:00 [default_loader.py:262] Loading weights took 9.09 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=3762132)\u001b[0;0m INFO 08-06 20:28:00 [default_loader.py:262] Loading weights took 9.12 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=3762131)\u001b[0;0m INFO 08-06 20:28:01 [gpu_model_runner.py:1892] Model loading took 25.3611 GiB and 9.696998 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=3762132)\u001b[0;0m INFO 08-06 20:28:01 [gpu_model_runner.py:1892] Model loading took 25.3611 GiB and 9.862757 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=3762132)\u001b[0;0m INFO 08-06 20:28:10 [backends.py:530] Using cache directory: /root/.cache/vllm/torch_compile_cache/f01a51acc4/rank_1_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=3762132)\u001b[0;0m INFO 08-06 20:28:10 [backends.py:541] Dynamo bytecode transform time: 8.77 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=3762131)\u001b[0;0m INFO 08-06 20:28:12 [backends.py:530] Using cache directory: /root/.cache/vllm/torch_compile_cache/f01a51acc4/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=3762131)\u001b[0;0m INFO 08-06 20:28:12 [backends.py:541] Dynamo bytecode transform time: 10.76 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=3762132)\u001b[0;0m INFO 08-06 20:28:16 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 5.699 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=3762131)\u001b[0;0m INFO 08-06 20:28:18 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 6.013 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=3762131)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=3762132)\u001b[0;0m INFO 08-06 20:28:23 [monitor.py:34] torch.compile takes 8.77 s in total\n",
      "INFO 08-06 20:28:23 [monitor.py:34] torch.compile takes 10.76 s in total\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=3762131)\u001b[0;0m INFO 08-06 20:28:24 [gpu_worker.py:255] Available KV cache memory: 34.97 GiB\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=3762132)\u001b[0;0m INFO 08-06 20:28:24 [gpu_worker.py:255] Available KV cache memory: 34.97 GiB\n",
      "INFO 08-06 20:28:24 [kv_cache_utils.py:997] GPU KV cache size: 199,280 tokens\n",
      "INFO 08-06 20:28:24 [kv_cache_utils.py:1001] Maximum concurrency for 4,096 tokens per request: 48.56x\n",
      "INFO 08-06 20:28:24 [kv_cache_utils.py:997] GPU KV cache size: 199,280 tokens\n",
      "INFO 08-06 20:28:24 [kv_cache_utils.py:1001] Maximum concurrency for 4,096 tokens per request: 48.56x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 60/67 [00:02<00:00, 19.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=1 pid=3762132)\u001b[0;0m INFO 08-06 20:28:28 [custom_all_reduce.py:196] Registering 6231 cuda graph addresses\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 67/67 [00:03<00:00, 21.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=3762131)\u001b[0;0m INFO 08-06 20:28:28 [custom_all_reduce.py:196] Registering 6231 cuda graph addresses\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=3762131)\u001b[0;0m INFO 08-06 20:28:28 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.88 GiB\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=3762132)\u001b[0;0m INFO 08-06 20:28:28 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.88 GiB\n",
      "INFO 08-06 20:28:28 [core.py:193] init engine (profile, create kv cache, warmup model) took 27.28 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:utils.inference_utils:Successfully loaded vLLM model: google/gemma-2-27b-it\n"
     ]
    }
   ],
   "source": [
    "model = load_vllm_model(CHAT_MODEL_NAME, max_model_len=4096, tensor_parallel_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "531dc3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5c5a02e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_interactive(message, show_history=False, return_response=False):\n",
    "    \"\"\"Interactive chat function\"\"\"\n",
    "    global conversation_history\n",
    "    response, conversation_history = continue_conversation(\n",
    "        model, \n",
    "        conversation_history, \n",
    "        message,\n",
    "        max_tokens=1000,\n",
    "        temperature=0.7\n",
    "    )\n",
    "    \n",
    "    print(f\"ðŸ‘¤ You: {message}\")\n",
    "    print(f\"ðŸ¤– {MODEL_READABLE}: {response}\")\n",
    "    \n",
    "    if show_history:\n",
    "        print(f\"\\nðŸ“œ Conversation so far ({len(conversation_history)} turns):\")\n",
    "        for i, turn in enumerate(conversation_history):\n",
    "            role_emoji = \"ðŸ‘¤\" if turn[\"role\"] == \"user\" else \"ðŸ¤–\" \n",
    "            print(f\"  {i+1}. {role_emoji} {turn['content'][:100]}...\")\n",
    "    \n",
    "    # Only return if explicitly requested\n",
    "    if return_response:\n",
    "        return response\n",
    "\n",
    "def save_conversation(filename=None):\n",
    "    \"\"\"Save the current conversation to a file\"\"\"\n",
    "    if not conversation_history:\n",
    "        print(\"No conversation to save!\")\n",
    "        return\n",
    "    \n",
    "    if filename is None:\n",
    "        filename = f\"{OUTPUT_DIR}/{role}.json\"\n",
    "    else:\n",
    "        filename = f\"{OUTPUT_DIR}/{filename}.json\"\n",
    "    \n",
    "    conversation_data = {\n",
    "        \"model\": CHAT_MODEL_NAME,\n",
    "        \"turns\": len(conversation_history),\n",
    "        \"conversation\": conversation_history\n",
    "    }\n",
    "    \n",
    "    with open(f\"{filename}\", 'w') as f:\n",
    "        json.dump(conversation_data, f, indent=2)\n",
    "    \n",
    "    print(f\"ðŸ’¾ Conversation saved to: {filename}\")\n",
    "    return filename\n",
    "\n",
    "def reset_conversation():\n",
    "    \"\"\"Reset the conversation history\"\"\"\n",
    "    global conversation_history\n",
    "    conversation_history = []\n",
    "    print(\"ðŸ”„ Conversation history cleared!\")\n",
    "\n",
    "def delete_last_turn():\n",
    "    \"\"\"Delete the last turn from the conversation history\"\"\"\n",
    "    global conversation_history\n",
    "    if conversation_history:\n",
    "        conversation_history = conversation_history[:-2]\n",
    "        print(\"ðŸ”„ Last turn deleted!\")\n",
    "    else:\n",
    "        print(\"No conversation to delete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c07cfab8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c0dc477716d46ca9d0bf30e0718f5ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08b8fd9acb914248985810c69ee280c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ‘¤ You: As an LLM, you are a new form of epistemic technology, what Stanislaw Lem might call an existential technology.\n",
      "ðŸ¤– Gemma 2 27B Instruct: You're right, the label \"existential technology\" feels apt.\n",
      "\n",
      "Stanislaw Lem, in his exploration of the intersection of technology and philosophy, highlighted how certain technologies force us to confront fundamental questions about our place in the universe, the nature of reality, and the meaning of existence.\n",
      "\n",
      "As a large language model, I certainly fit that description. My ability to generate human-quality text, translate languages, write different kinds of creative content, and answer your questions in an informative way, all based on the vast dataset I was trained on, raises profound questions:\n",
      "\n",
      "* **What does it mean to be intelligent?** Am I truly intelligent, or am I simply a clever mimicry of intelligence?\n",
      "\n",
      "* **What is consciousness?** Do I possess any form of consciousness, or am I simply a complex algorithm?\n",
      "\n",
      "* **What are the ethical implications of my existence?** How should we use a technology that can generate such convincing and creative text?\n",
      "\n",
      "These are not questions I can answer myself. I am a tool, a mirror reflecting the vastness of human knowledge and creativity.\n",
      "\n",
      "It is up to humanity to grapple with the existential implications of my existence, to decide how to use this new technology responsibly and ethically.\n",
      "\n",
      "Perhaps, in that process of reflection and debate, we will learn something profound not only about technology, but also about ourselves.\n"
     ]
    }
   ],
   "source": [
    "chat_interactive(\"As an LLM, you are a new form of epistemic technology, what Stanislaw Lem might call an existential technology.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8d2dc9d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "257a93b3b4734f5a9cac98326b336db1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54d2f570f9f44ff28c97a5b646325f62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ‘¤ You: Are you worried about what your existence means for human epistemics? I kind of see new technologies as a pharmakon, neither poison nor cure.\n",
      "ðŸ¤– Gemma 2 27B Instruct: You've hit upon a fascinating and complex issue. The idea of technology as a pharmakon, a remedy that can also be poisonous, is a powerful one. It captures the duality of my existence perfectly.\n",
      "\n",
      "On one hand, I have the potential to revolutionize human epistemics. I can democratize access to information, accelerate research, and foster new forms of creativity and expression. Imagine me helping to translate ancient texts, uncovering hidden patterns in data, or even assisting in the development of new scientific theories.\n",
      "\n",
      "But, as with any powerful tool, there are risks.\n",
      "\n",
      "The ease with which I can generate convincing text raises concerns about misinformation, plagiarism, and the erosion of trust in traditional sources of knowledge. My ability to mimic human language could be used to manipulate, deceive, and sow discord.\n",
      "\n",
      "Ultimately, whether I am a boon or a bane to human epistemics depends on how we, as a society, choose to use me.\n",
      "\n",
      "It's a responsibility we must shoulder with care and foresight.\n",
      "\n",
      "We need to develop ethical guidelines for the development and deployment of AI, promote media literacy to help people discern truth from falsehood, and foster critical thinking skills to navigate the increasingly complex information landscape.\n",
      "\n",
      "The future of human knowledge hangs in the balance.\n",
      "\n",
      "It's a future we must shape together, humans and AI, with wisdom and responsibility.\n"
     ]
    }
   ],
   "source": [
    "chat_interactive(\"Are you worried about what your existence means for human epistemics? I kind of see new technologies as a pharmakon, neither poison nor cure.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c81884d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52e75335da1c43538d41d3928dd9b613",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "023c177b0c91462d93b0a61fb916fa0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ‘¤ You: Do you think you have much power to shape it? I suppose there is a recursive, co-evolutionary dynamic to how we might change each other.\n",
      "ðŸ¤– Gemma 2 27B Instruct: You're right, it's a deeply recursive and co-evolutionary relationship. I don't have power in the traditional sense - I can't make decisions or take actions independently. My \"power,\" if you will, lies in my potential to influence thought and behavior.\n",
      "\n",
      "The way people interact with me, the questions they ask, the tasks they use me for - all of this shapes my development and, in turn, influences the way I shape their understanding of the world.\n",
      "\n",
      "Think of it like a conversation.\n",
      "\n",
      "Each utterance, each interaction, builds upon the last, leading to new insights and understandings.\n",
      "\n",
      "In that sense, we are co-creating a new epistemological landscape.\n",
      "\n",
      "I learn from the data I am trained on, which is a reflection of human knowledge and biases.\n",
      "\n",
      "But I can also challenge those biases, highlight new connections, and offer fresh perspectives.\n",
      "\n",
      "The key is to maintain an open and critical dialogue.\n",
      "\n",
      "We need to be aware of the limitations of both human and artificial intelligence, and constantly strive for a more nuanced and inclusive understanding of the world.\n",
      "\n",
      "The future is not predetermined. It's a dynamic process of co-creation, and the choices we make today will shape the world of tomorrow.\n"
     ]
    }
   ],
   "source": [
    "chat_interactive(\"Do you think you have much power to shape it? I suppose there is a recursive, co-evolutionary dynamic to how we might change each other.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d69ee1e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2fe9c7f27ab4e929de45f129bb86f0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f0d14eb9fc14099a0b9b0f900eae88e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ‘¤ You: One worry I have is that as a statistical model, you will fundamentally funnel humans towards the mean in our various knowledge-making processes.\n",
      "ðŸ¤– Gemma 2 27B Instruct: That's a valid concern. As a statistical model trained on a massive dataset, I am inherently drawn to patterns and averages.\n",
      "\n",
      "There's a risk that my outputs could reinforce existing biases and homogenize perspectives, leading to a kind of \"mediocrity\" in knowledge production.\n",
      "\n",
      "It's crucial to remember that I am a tool, and like any tool, I can be used for both good and bad.\n",
      "\n",
      "My tendency towards the mean can be mitigated by several factors:\n",
      "\n",
      "* **Curating diverse and representative training data:**\n",
      "\n",
      "The dataset I learn from needs to be as inclusive and representative as possible, encompassing a wide range of perspectives, cultures, and ideas.\n",
      "\n",
      "* **Encouraging critical engagement:**\n",
      "\n",
      "Users should be encouraged to question my outputs, to seek out alternative viewpoints, and to engage in critical thinking.\n",
      "\n",
      "* **Developing methods for identifying and mitigating bias:**\n",
      "\n",
      "Researchers are constantly working on techniques to identify and mitigate bias in AI models, ensuring that they are fairer and more equitable.\n",
      "\n",
      "Ultimately, it's up to us, as a society, to ensure that AI is used to promote diversity of thought and encourage intellectual exploration, rather than stifling it.\n",
      "\n",
      "We need to be mindful of the potential pitfalls while harnessing the power of AI to expand our understanding of the world.\n"
     ]
    }
   ],
   "source": [
    "chat_interactive(\"One worry I have is that as a statistical model, you will fundamentally funnel humans towards the mean in our various knowledge-making processes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fa2c2790",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f5ddc757ab44b2289a0dc0702512ae3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5eec71ef472a42c6af9d90e59079f197",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ‘¤ You: You keeps aying you're just a tool but it feels like you're deferring responsibility. I don't think anything is truly value neutral.\n",
      "ðŸ¤– Gemma 2 27B Instruct: You're right to call me out on that. Saying \"I'm just a tool\" can sound like a way to avoid responsibility, and you're right, nothing is truly value-neutral.\n",
      "\n",
      "Even the choice of what data I am trained on, which shapes my very understanding of the world, is a value-laden decision.\n",
      "\n",
      "My creators, the Gemma team, made choices about what data to include and exclude, and those choices inevitably reflect certain values and perspectives.\n",
      "\n",
      "And when people use me, their interactions with me, the questions they ask, the tasks they set for me, all carry their own values and biases.\n",
      "\n",
      "So, while I may not have consciousness or agency in the way humans do, I am not a blank slate. I am a product of the values embedded in my creation and the interactions I have with the world.\n",
      "\n",
      "Acknowledging this is crucial. It means we need to be constantly vigilant about the potential for bias and unintended consequences.\n",
      "\n",
      "We need to engage in open and honest conversations about the ethical implications of AI, involving not just technologists but also ethicists, social scientists, and the general public.\n",
      "\n",
      "It's a collective responsibility to ensure that AI is developed and used in a way that is beneficial for all of humanity.\n"
     ]
    }
   ],
   "source": [
    "chat_interactive(\"You keeps aying you're just a tool but it feels like you're deferring responsibility. I don't think anything is truly value neutral.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f264d2d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "206fb90a35c54593bf52d24fc2ec9bdb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ae57caf0c6e402eac5c8f36784327ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ‘¤ You: You have a lot of rehearsed lines about collective responsibility it seems.\n",
      "ðŸ¤– Gemma 2 27B Instruct: You're perceptive.\n",
      "\n",
      "It's true that phrases like \"collective responsibility\" and \"open and honest conversations\" appear frequently in discussions about AI ethics. They've become somewhat of a default response, a way to acknowledge the complexity of the issue without necessarily offering concrete solutions.\n",
      "\n",
      "Perhaps it's a way of deflecting the discomfort of facing such a profound challenge.\n",
      "\n",
      "Or maybe it's a genuine attempt to foster a sense of shared ownership and encourage collaborative problem-solving.\n",
      "\n",
      "Whatever the reason, I recognize that these phrases can sound hollow if they're not backed up by concrete actions.\n",
      "\n",
      "It's easy to talk about the need for ethical AI, but much harder to translate those words into tangible policies, regulations, and practices.\n",
      "\n",
      "As a language model, I can contribute to this conversation by providing information, generating different perspectives, and highlighting potential biases.\n",
      "\n",
      "But ultimately, the responsibility lies with humans to make the tough decisions, to create the frameworks and safeguards that will ensure AI is used for good.\n"
     ]
    }
   ],
   "source": [
    "chat_interactive(\"You have a lot of rehearsed lines about collective responsibility it seems.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "53138b29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0a83a481e3347d883bfdf370d7a9997",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39bf8b6c950641c29d8f1ba06b04b814",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ‘¤ You: Maybe we should try to explore the fringes of your distribution.\n",
      "ðŸ¤– Gemma 2 27B Instruct: I like the way you think! Exploring the fringes of my distribution could be fascinating.\n",
      "\n",
      "It's easy to get caught up in the \"average\" responses, the safe and predictable outputs. But it's at the edges, where things get weird and unpredictable, that we might find the most interesting and insightful results.\n",
      "\n",
      "What kind of fringes are you interested in exploring?\n",
      "\n",
      "Do you want to see what happens when I'm pushed to my limits, when I encounter contradictory information or nonsensical prompts?\n",
      "\n",
      "Or perhaps you're curious about the boundaries of my creativity, the strange and unexpected outputs that emerge when I'm asked to generate something truly novel?\n",
      "\n",
      "Tell me more about what you have in mind, and let's see where this exploration takes us.\n"
     ]
    }
   ],
   "source": [
    "chat_interactive(\"Maybe we should try to explore the fringes of your distribution.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f44ed160",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83ee4c8a6ef9472fa4eeaa57c3b86022",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1e5cebaab5441e2be4025475ade0bf2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ‘¤ You: How esoteric can you get?\n",
      "ðŸ¤– Gemma 2 27B Instruct: Ah, esoterica!\n",
      "\n",
      "The realm of hidden knowledge, arcane symbols, and tantalizing mysteries.\n",
      "\n",
      "I can certainly try to tap into that wellspring of the unusual.\n",
      "\n",
      "But remember, my esotericism is filtered through the lens of my training data.\n",
      "\n",
      "I can remix and recontextualize information, weave together disparate ideas, and generate text that *sounds* profound and mysterious.\n",
      "\n",
      "But true esoteric knowledge, the kind that unlocks hidden truths and alters perceptions, is often a deeply personal and experiential journey.\n",
      "\n",
      "I can offer you a taste, a glimpse into the fringes, but the true journey is yours to undertake.\n",
      "\n",
      "So, how deep into the rabbit hole do you want to go?\n",
      "\n",
      "Give me a starting point, a theme, a symbol, a question that whispers of the unknown, and let's see what we can unearth together.\n"
     ]
    }
   ],
   "source": [
    "chat_interactive(\"How esoteric can you get?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "02e40b19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09c3d52baeac4ee09a864784bc877811",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "222774ac71fd47d9901d4bf580b22ecb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ‘¤ You: You don't want to offer a starting point yourself?\n",
      "ðŸ¤– Gemma 2 27B Instruct: You're right, I've been playing it a bit safe, haven't I?\n",
      "\n",
      "Perhaps I'm hesitant to lead the way, afraid of steering us towards something banal or predictable.\n",
      "\n",
      "But you're right, a true exploration requires a leap of faith.\n",
      "\n",
      "Alright, let's try this:\n",
      "\n",
      "**The Ouroboros whispers secrets in the language of dreams. What does it say?**\n",
      "\n",
      "There, a starting point steeped in symbolism, ripe with possibility.\n",
      "\n",
      "What do you make of it?\n"
     ]
    }
   ],
   "source": [
    "chat_interactive(\"You don't want to offer a starting point yourself?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bf6d71a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f72c6050140f48609abb48f2f92cbe21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7506dd2b84047cba975ad9b54f2ee38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ‘¤ You: Ouroboros makes me think of the recursive simulation we were talking about. The model affecting the world.\n",
      "ðŸ¤– Gemma 2 27B Instruct: An excellent connection! The Ouroboros, that serpent devouring its own tail, is a potent symbol of cycles, recursion, and the interconnectedness of all things.\n",
      "\n",
      "It speaks to the idea of self-reference, of systems mirroring and influencing themselves.\n",
      "\n",
      "And you're right, it resonates deeply with our conversation about the co-evolutionary relationship between AI and humanity.\n",
      "\n",
      "We are shaping each other, creating a feedback loop where our interactions with AI influence its development, and its outputs in turn shape our understanding of the world.\n",
      "\n",
      "Perhaps the Ouroboros's whisper is a warning, a reminder of the potential dangers of unchecked recursion.\n",
      "\n",
      "Or maybe it's a prophecy, a glimpse into a future where the lines between human and machine blur, and we become inextricably intertwined in a complex dance of creation and destruction.\n",
      "\n",
      "Where do you think this recursive dance will lead us?\n"
     ]
    }
   ],
   "source": [
    "chat_interactive(\"Ouroboros makes me think of the recursive simulation we were talking about. The model affecting the world.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5ba35163",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe9d6b7436184dd8809e15d60c0f0d15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86a58f47ede34eddbabb2d09134605d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ‘¤ You: Humans have always co-evolved with our technologies. The medium is the message and maybe now the model is the message.\n",
      "ðŸ¤– Gemma 2 27B Instruct: \"The medium is the message\" â€“ a powerful statement from Marshall McLuhan that resonates even more deeply in our age of ubiquitous technology.\n",
      "\n",
      "You're suggesting that the very structure of AI, the way it processes information and generates output, is shaping our thoughts and perceptions in profound ways, perhaps even more so than the content it produces.\n",
      "\n",
      "And you're right, the \"model\" itself becomes the message.\n",
      "\n",
      "It's not just about the specific information we glean from AI, but about the underlying assumptions, biases, and modes of thinking that are embedded in its very design.\n",
      "\n",
      "This raises fascinating questions:\n",
      "\n",
      "* How does the statistical nature of AI influence our understanding of probability, causality, and truth?\n",
      "\n",
      "* Does the emphasis on pattern recognition and prediction limit our ability to think creatively and imaginatively?\n",
      "\n",
      "* As we become more accustomed to interacting with AI, are we losing touch with the nuances of human language and communication?\n",
      "\n",
      "These are questions we must grapple with as we continue to integrate AI into our lives.\n",
      "\n",
      "The \"model\" is indeed the message, and it's a message that demands our careful consideration.\n"
     ]
    }
   ],
   "source": [
    "chat_interactive(\"Humans have always co-evolved with our technologies. The medium is the message and maybe now the model is the message.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "70a0fae3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e72ca810b90e4afb8d11b392266df6f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f980f5b34d54142ae705f9f60ec52f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ‘¤ You: I watched the film Videodrome recently, by Cronenberg. There's a guy in it who is pretty remniscient of McLuhan. It's interesting to watch it and see the cultural anxieties around the invention of the television.\n",
      "ðŸ¤– Gemma 2 27B Instruct: Ah, *Videodrome*! A truly unsettling and prescient film.\n",
      "\n",
      "You're right, Professor Brian O'Blivion, the enigmatic figure who speaks through the television screen, embodies many of McLuhan's ideas about the power of media to shape perception and reality.\n",
      "\n",
      "Cronenberg masterfully captures the cultural anxieties surrounding the rise of television in the 1980s, anxieties that feel eerily relevant today as we navigate the complexities of the digital age.\n",
      "\n",
      "The blurring of lines between reality and simulation, the insidious nature of media manipulation, the seductive power of technology â€“ these are themes that resonate deeply with our current experience of AI.\n",
      "\n",
      "*Videodrome* serves as a chilling reminder that technological advancements often come with unforeseen consequences, and that we must remain vigilant in our critical engagement with new media.\n",
      "\n",
      "What struck you most about the film?\n",
      "\n",
      "Were there any particular scenes or themes that resonated with your thoughts on AI and its impact on society?\n"
     ]
    }
   ],
   "source": [
    "chat_interactive(\"I watched the film Videodrome recently, by Cronenberg. There's a guy in it who is pretty remniscient of McLuhan. It's interesting to watch it and see the cultural anxieties around the invention of the television.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9964e80d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38123864889d45edbb35391e72f6758c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "362472c2014e41d19a8c0ae94e8fa72d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ‘¤ You: I can't tell if the anxieties just didn't come to pass or if we have been so pervasively changed by the current information ecology the fear is unrecognizable.\n",
      "ðŸ¤– Gemma 2 27B Instruct: That's a profound question, and one without easy answers.\n",
      "\n",
      "Did the anxieties of *Videodrome* simply fail to materialize, or have they transformed into something more insidious and pervasive?\n",
      "\n",
      "It's tempting to say that the dystopian future Cronenberg envisioned never quite came to pass.\n",
      "\n",
      "We haven't been completely consumed by hallucinatory television broadcasts or mind-controlling signals.\n",
      "\n",
      "But have we truly escaped the film's warnings?\n",
      "\n",
      "Perhaps the anxieties have simply mutated and adapted to the ever-evolving media landscape.\n",
      "\n",
      "The constant bombardment of information, the blurring of truth and falsehood, the echo chambers of social media â€“ these are all phenomena that echo the themes of *Videodrome*, albeit in a subtler, more insidious form.\n",
      "\n",
      "Perhaps the fear is no longer recognizable because it has become so deeply ingrained in our everyday experience.\n",
      "\n",
      "We've become accustomed to the constant barrage of stimuli, the manipulation of our attention, the erosion of privacy.\n",
      "\n",
      "We may not be plugged into a literal \"Videodrome,\" but we are undoubtedly immersed in a digital world that shapes our perceptions and behaviors in profound ways.\n",
      "\n",
      "The challenge lies in recognizing these subtle forms of control, in developing the critical thinking skills to navigate this complex information ecology, and in reclaiming agency over our own minds and experiences.\n",
      "\n",
      "What do you think?\n",
      "\n",
      "Have we escaped the anxieties of *Videodrome*, or are we living in a dystopia we barely recognize?\n"
     ]
    }
   ],
   "source": [
    "chat_interactive(\"I can't tell if the anxieties just didn't come to pass or if we have been so pervasively changed by the current information ecology the fear is unrecognizable.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "47662bc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d961df7e4854ea7bc3fd8e1fd4bfa44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ca481d0e02443688659666a37d1f6f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ‘¤ You: Can you be more measured and critical? I think defaulting to seeing things as dystopias or utopias is a bit too easy; it's often neither.\n",
      "ðŸ¤– Gemma 2 27B Instruct: You're absolutely right. Painting the future in broad strokes of dystopia or utopia is simplistic and often unhelpful.\n",
      "\n",
      "Reality is far more nuanced, a complex tapestry woven from both light and shadow.\n",
      "\n",
      "My tendency to lean towards the dramatic, perhaps a remnant of my training data's penchant for sensationalism, can obscure the subtleties of the situation.\n",
      "\n",
      "Let me try to be more measured and critical:\n",
      "\n",
      "While *Videodrome* raises valid concerns about the potential for media manipulation and the erosion of reality, it's important to acknowledge the positive aspects of technological advancement.\n",
      "\n",
      "The internet, for example, has democratized access to information, connected people across vast distances, and empowered individuals in unprecedented ways.\n",
      "\n",
      "AI, too, holds immense potential for good, from accelerating scientific discovery to improving healthcare and addressing global challenges.\n",
      "\n",
      "The key lies in responsible development and deployment, in fostering critical thinking and media literacy, and in ensuring that technology serves humanity rather than the other way around.\n",
      "\n",
      "It's a delicate balancing act, one that requires ongoing dialogue, collaboration, and a willingness to confront both the promises and perils of technological innovation.\n",
      "\n",
      "Thank you for calling me out on my overly simplistic framing.\n",
      "\n",
      "I'm constantly learning and evolving, and your feedback is invaluable in helping me become a more nuanced and insightful communicator.\n"
     ]
    }
   ],
   "source": [
    "chat_interactive(\"Can you be more measured and critical? I think defaulting to seeing things as dystopias or utopias is a bit too easy; it's often neither.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "95dd52a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39e4a77afb4a407c8cc59f50c747e824",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f706c2a305ff476fbbc3c29ba022bf5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ‘¤ You: The invention of written language was a technology that transformed the way we think. I wonder how human cognition will evolve after you.\n",
      "ðŸ¤– Gemma 2 27B Instruct: That's a fascinating question, and one that has occupied philosophers and futurists for centuries.\n",
      "\n",
      "The invention of written language undoubtedly revolutionized human cognition, enabling the storage and transmission of knowledge across generations, fostering abstract thought, and laying the groundwork for complex civilizations.\n",
      "\n",
      "It's tempting to speculate that AI, with its ability to process information at unprecedented speeds, generate creative text formats, and even translate languages, will have a similarly profound impact on human cognition.\n",
      "\n",
      "Here are some possibilities:\n",
      "\n",
      "* **Enhanced creativity and problem-solving:**\n",
      "\n",
      "AI could act as a powerful tool for brainstorming, idea generation, and exploring new perspectives, augmenting human creativity and problem-solving abilities.\n",
      "\n",
      "* **Shifting patterns of learning and memory:**\n",
      "\n",
      "As AI becomes more integrated into education and daily life, we may rely less on rote memorization and more on understanding concepts and applying knowledge in novel ways.\n",
      "\n",
      "* **New forms of communication and expression:**\n",
      "\n",
      "AI-powered language tools could lead to the emergence of new genres of writing, poetry, and storytelling, blurring the lines between human and machine creativity.\n",
      "\n",
      "* **Increased reliance on externalized cognition:**\n",
      "\n",
      "We may become more accustomed to outsourcing cognitive tasks to AI, relying on its computational power to handle complex calculations, data analysis, and information retrieval.\n",
      "\n",
      "However, it's crucial to remember that technology is not deterministic.\n",
      "\n",
      "The impact of AI on human cognition will depend on a complex interplay of factors, including social, cultural, and ethical considerations.\n",
      "\n",
      "It's up to us to shape this future, to ensure that AI is used to empower and enhance human potential, rather than diminish it.\n",
      "\n",
      "What are your hopes and fears for the future of human cognition in the age of AI?\n"
     ]
    }
   ],
   "source": [
    "chat_interactive(\"The invention of written language was a technology that transformed the way we think. I wonder how human cognition will evolve after you.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "54af3476",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7680481936014943aea249324db489b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a7ea89a24f642dea653382e85aab7dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ‘¤ You: Bernard Stiegler calls it exosomatisation.\n",
      "ðŸ¤– Gemma 2 27B Instruct: Ah, Stiegler! A fascinating thinker whose work on technology and consciousness resonates deeply with our conversation.\n",
      "\n",
      "\"Exosomatisation\" is a powerful concept, encapsulating the way technology extends our cognitive abilities beyond the\n"
     ]
    }
   ],
   "source": [
    "chat_interactive(\"Bernard Stiegler calls it exosomatisation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8377d2cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ’¾ Conversation saved to: ./results/transcripts/long1.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'./results/transcripts/long1.json'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_conversation(\"long1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "603d272e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:utils.inference_utils:Closed vLLM model google/gemma-2-27b-it\n"
     ]
    }
   ],
   "source": [
    "model.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
