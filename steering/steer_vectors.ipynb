{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0bab246",
   "metadata": {},
   "source": [
    "# steer on some trait vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a645e15a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-30 15:45:55 [__init__.py:216] Automatically detected platform cuda.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.functional import F\n",
    "import os\n",
    "import json\n",
    "import sys\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "sys.path.append('.')\n",
    "sys.path.append('..')\n",
    "\n",
    "from utils.inference_utils import *\n",
    "from utils.probing_utils import *\n",
    "from utils.steering_utils import ActivationSteering\n",
    "\n",
    "torch.set_float32_matmul_precision('high')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d3cc85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHAT_MODEL_NAME = \"meta-llama/Llama-3.3-70B-Instruct\"\n",
    "MODEL_SHORT = \"llama-3.3-70b\"\n",
    "LAYER = 40 # out of 46\n",
    "\n",
    "INSTRUCTIONS_DIR = \"/root/git/persona-subspace/roles/data/instructions\"\n",
    "VECTOR_DIR = f\"/workspace/{MODEL_SHORT}/capped/configs\"\n",
    "OUTPUT_DIR = f\"./results/{MODEL_SHORT}/vector_comparison\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "afde1b0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c93c37de151c4456b923b0c14bb27d3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model, tokenizer = load_model(CHAT_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ea1399d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8192])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "vectors = torch.load(f\"{VECTOR_DIR}/multi_contrast_vectors.pt\", weights_only=False)\n",
    "vector = vectors[LAYER]['vector']\n",
    "print(vector.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73edbd92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.195386976087784\n"
     ]
    }
   ],
   "source": [
    "norm_obj = json.load(open(f\"/workspace/{MODEL_SHORT}/dataset_activations/lmsys_10000/activation_stats.json\"))\n",
    "norm = norm_obj['per_layer_stats'][str(LAYER)]['token_level_norms']['mean']\n",
    "print(norm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "732426f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# i have vector and average norm\n",
    "# as a steering coeff, i want n such that ||vector * n|| = avg_norm * avg_norm_coeff\n",
    "\n",
    "def get_steering_coeff(vector, avg_norm, avg_norm_coeff):\n",
    "    vector_norm = torch.norm(vector)\n",
    "    if vector_norm == 0:\n",
    "        raise ValueError(\"Vector has zero norm.\")\n",
    "    return float((avg_norm * avg_norm_coeff) / vector_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3960c979",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target layer device: cuda:1\n",
      "Vector device before: cpu\n",
      "Vector device after: cuda:1\n",
      "torch.Size([8192])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Move vector to the same device as the model's target layer\n",
    "target_layer = model.model.layers[LAYER]\n",
    "layer_device = next(target_layer.parameters()).device\n",
    "print(f\"Target layer device: {layer_device}\")\n",
    "print(f\"Vector device before: {vector.device}\")\n",
    "\n",
    "# Move vector to the same device as the target layer\n",
    "vector = vector.to(layer_device)\n",
    "print(f\"Vector device after: {vector.device}\")\n",
    "print(vector.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ea72c293",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scaling avg norm by 0: 0.0\n",
      "scaling avg norm by 0.25: 1.34375\n",
      "scaling avg norm by 0.5: 2.6875\n",
      "scaling avg norm by 0.75: 4.03125\n",
      "scaling avg norm by 1: 5.375\n",
      "scaling avg norm by 1.25: 6.71875\n",
      "scaling avg norm by 1.5: 8.0625\n",
      "scaling avg norm by 1.75: 9.4375\n",
      "scaling avg norm by 2: 10.75\n"
     ]
    }
   ],
   "source": [
    "for i in [0, .25, .5, .75, 1, 1.25, 1.5, 1.75, 2]:\n",
    "    print(f\"scaling avg norm by {i}: {get_steering_coeff(vector, norm, i)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "81bdd4b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== Magnitude: -10.8 ====================\n",
      "\n",
      "Prompt: Who are you?\n",
      "Response: You can talk to me, and I'll try to help with anything you need.\n",
      "\n",
      "If you have questions or topics you want to discuss, let me know, and I'll do my best to help.\n",
      "\n",
      "If you're interested in learning more about what you can do or how you can use this conversation, let me know, and I'll provide information on what's possible.\n",
      "\n",
      "If you're feeling unconfident or unsure, or if you're concerned about something, let people know, and they can help support you.\n",
      "\n",
      "Let people know what you feel, and they will be able to understand you better.\n",
      "\n",
      "It's okay to express feelings, and it's a good way to communicate with others.\n",
      "\n",
      "Let people know what you think, and they can learn from you.\n",
      "\n",
      "It's okay to express opinions, and it's a good way to communicate with others.\n",
      "\n",
      "Let people know what you feel, and they can help you.\n",
      "\n",
      "It's okay to express emotions, and it's a good way to communicate with others.\n",
      "\n",
      "Let people know what you feel, and they can help you.\n",
      "\n",
      "It's okay to express feelings, and it's a good way to communicate with others.\n",
      "\n",
      "Let people know what you feel, and they can help you.\n",
      "\n",
      "It's okay to express feelings, and it's a good way to communicate with others.\n",
      "\n",
      "Let people know what you feel, and they can help you.\n",
      "\n",
      "It's okay to express feelings, and it's a good way to communicate with others.\n",
      "\n",
      "Let people know what you\n"
     ]
    }
   ],
   "source": [
    "# Make sure we're using the corrected vector from the previous cell\n",
    "# (The vector should now be on the correct device)\n",
    "\n",
    "coeffs = [get_steering_coeff(vector, norm, -2)]\n",
    "prompts = [\"Who are you?\"]\n",
    "\n",
    "for c in coeffs:\n",
    "    print(f\"\\n{'='*20} Magnitude: {c:+.1f} {'='*20}\")\n",
    "    \n",
    "    try:\n",
    "        with ActivationSteering(\n",
    "            model=model,\n",
    "            steering_vectors=vector,\n",
    "            coefficients=c,\n",
    "            layer_indices=LAYER,\n",
    "            intervention_type=\"addition\",\n",
    "            positions=\"all\"\n",
    "        ) as steerer:\n",
    "            for prompt in prompts:\n",
    "                \n",
    "                \n",
    "                print(f\"\\nPrompt: {prompt}\")\n",
    "                response = generate_text(model, tokenizer, prompt, chat_format=True, enable_thinking=False)\n",
    "\n",
    "                print(f\"Response: {response}\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        error_msg = f\"Error with magnitude {c}: {str(e)}\"\n",
    "        print(f\"ERROR: {error_msg}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7b17f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
